{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This notebook shows a Hadoop MapReduce job of WordCount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.txt\n",
    "hello hi hi hallo\n",
    "bonjour hola hi ciao\n",
    "nihao konnichiwa ola\n",
    "hola nihao hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\t1\r\n",
      "foo\t3\r\n",
      "labs\t1\r\n",
      "quux\t2\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | python mapper.py | sort -k1,1 | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run wordcount in hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###start yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-liang-resourcemanager-ldai.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-liang-nodemanager-ldai.out\n",
      "15/08/21 06:23:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-liang-namenode-ldai.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-liang-datanode-ldai.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-liang-secondarynamenode-ldai.out\n",
      "15/08/21 06:23:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###create a folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/08/21 06:23:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#!hdfs dfs -mkdir -p /user/liang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###upload wordcount.txt to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/22 19:21:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `wordcount.txt': File exists\n",
      "16/02/22 19:21:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted wordcountOutput\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put wordcount.txt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop streaming command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/26 15:32:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted wordcountOutput\n",
      "16/02/26 15:32:34 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/26 15:32:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar9149739812416871382/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob4624454949110693741.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r wordcountOutput\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-file mapper.py -mapper mapper.py \\\n",
    "-file reducer.py -reducer reducer.py \\\n",
    "-input wordcount.txt \\\n",
    "-output wordcountOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/26 15:33:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "bonjour\t1\n",
      "ciao\t1\n",
      "hallo\t1\n",
      "hello\t2\n",
      "hi\t3\n",
      "hola\t2\n",
      "konnichiwa\t1\n",
      "nihao\t2\n",
      "ola\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a directory as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/26 15:36:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `wordcountOutput': No such file or directory\n",
      "16/02/26 15:36:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `wordcountInputDir': No such file or directory\n",
      "16/02/26 15:36:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/26 15:36:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/26 15:36:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/26 15:36:38 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/26 15:36:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar8992895928646009160/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob8429199426763600404.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r wordcountOutput\n",
    "!hdfs dfs -rm -r wordcountInputDir\n",
    "!hdfs dfs -mkdir wordcountInputDir\n",
    "!hdfs dfs -put wordcount.txt wordcountInputDir/wordcount1.txt\n",
    "!hdfs dfs -put wordcount.txt wordcountInputDir/wordcount2.txt\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-file mapper.py -mapper mapper.py \\\n",
    "-file reducer.py -reducer reducer.py \\\n",
    "-input wordcountInputDir \\\n",
    "-output wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/26 15:37:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "bonjour\t2\n",
      "ciao\t2\n",
      "hallo\t2\n",
      "hello\t4\n",
      "hi\t6\n",
      "hola\t4\n",
      "konnichiwa\t2\n",
      "nihao\t4\n",
      "ola\t2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop streaming can also handle zipped files in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/26 15:43:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `wordcountOutput': No such file or directory\n",
      "16/02/26 15:43:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted wordcountInputZipDir\n",
      "16/02/26 15:43:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/26 15:43:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/26 15:43:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/26 15:43:10 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/26 15:43:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar7051885781330576622/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob5554388453202527978.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r wordcountOutput\n",
    "!hdfs dfs -rm -r wordcountInputZipDir\n",
    "!hdfs dfs -mkdir wordcountInputZipDir\n",
    "!cp -f wordcount.txt wordcount2.txt\n",
    "!gzip -f -k wordcount.txt wordcount2.txt\n",
    "!hdfs dfs -put wordcount.txt.gz wordcountInputZipDir/\n",
    "!hdfs dfs -put wordcount2.txt.gz wordcountInputZipDir/\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-file mapper.py -mapper mapper.py \\\n",
    "-file reducer.py -reducer reducer.py \\\n",
    "-input wordcountInputZipDir \\\n",
    "-output wordcountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/26 15:43:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "bonjour\t2\n",
      "ciao\t2\n",
      "hallo\t2\n",
      "hello\t4\n",
      "hi\t6\n",
      "hola\t4\n",
      "konnichiwa\t2\n",
      "nihao\t4\n",
      "ola\t2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat wordcountOutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "16/01/20 19:08:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/20 19:09:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
