{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.1\n",
    "In the database world What is 3NF? Does machine learning use data in 3NF? If so why?   \n",
    "In what form does ML consume data?  \n",
    "Why would one use log files that are denormalized?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. \n",
    "\n",
    "Run your code on the data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file)). \n",
    "\n",
    "In this output please include the webpage URL, webpageID and Visitor ID.:\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "1. Left joining Table Left with Table Right\n",
    "2. Right joining Table Left with Table Right\n",
    "3. Inner joining Table Left with Table Right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJoin_5_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJoin_5_2.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "    urls = {} # key = pageId, value = url\n",
    "    keys_emitted = set() # Set of keys of all emitted urls. Used for left join.\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MRJoin, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--joinType', type='str', default=\"inner\")\n",
    "        \n",
    "    def load_options(self, args):\n",
    "        super(MRJoin, self).load_options(args)\n",
    "        self.joinType = self.options.joinType\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        # Load URL info data file into memory.  \n",
    "        # Line format: \n",
    "        # 1287,/autoroute\n",
    "        with open(\"processed_urls.data\", \"r\") as f:\n",
    "            for fields in csv.reader(f):\n",
    "                self.urls[fields[0]] = fields[1]\n",
    "\n",
    "    def mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # V,1000,1,C,10001\n",
    "        fields = csv.reader([line]).next()\n",
    "        \n",
    "        key = fields[1]\n",
    "        url = None\n",
    "        toEmit = False\n",
    "        \n",
    "        if key in self.urls:\n",
    "            url = self.urls[key]\n",
    "            \n",
    "        if self.joinType == \"right\":\n",
    "            toEmit = True\n",
    "        elif self.joinType == \"left\":\n",
    "            if url is not None:\n",
    "                toEmit = True\n",
    "                self.keys_emitted.add(key) # Remember what we have emitted\n",
    "        else: # inner join\n",
    "            if url is not None:\n",
    "                toEmit = True\n",
    "        \n",
    "        if toEmit:\n",
    "            # Output format\n",
    "            # pageid, url,V,1,C,10001\n",
    "            yield key, (url, fields[0], fields[2], fields[3], fields[4])\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        if self.joinType == \"left\":\n",
    "            # Emit all the remaining urls\n",
    "            remaining = set(self.urls.keys()) - self.keys_emitted\n",
    "            for key in remaining:\n",
    "                yield key, (self.urls[key], None, None, None, None)\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   mapper_final=self.mapper_final)\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join type:left\n",
      "Number of records:98663\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n",
      "Join type:right\n",
      "Number of records:98654\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n",
      "Join type:inner\n",
      "Number of records:98654\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from MRJoin_5_2 import MRJoin\n",
    "\n",
    "for joinType in [\"left\", \"right\", \"inner\"]:\n",
    "    mr_job = MRJoin(args=['processed_anonymous-msweb.data', \n",
    "                        '--file', 'processed_urls.data', # broadcast to every mapper\n",
    "                        \"--strict-protocols\",\n",
    "                        '--joinType', joinType])\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "\n",
    "        lines = []\n",
    "        for line in runner.stream_output():\n",
    "            lines.append(line)\n",
    "            \n",
    "        print \"Join type:\" + joinType\n",
    "        print \"Number of records:\" + str(len(lines))\n",
    "        print \"First 5 lines:\"\n",
    "        for i in range(5):\n",
    "            print lines[i].strip()\n",
    "            \n",
    "        print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW 5.3  EDA of Google n-grams dataset\n",
    "For the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:  \n",
    "``googlebooks-eng-all-5gram-20090715-0-filtered.txt``\n",
    "\n",
    "Finally show your results on the Google n-grams dataset. \n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "``\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)``\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3a\n",
    "\n",
    "Find Longest 5-gram (number of characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrLongest_5_3a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrLongest_5_3a.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Find Longest 5-gram (number of characters)\n",
    "class MrLongest(MRJob):\n",
    "    def mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "        fields = csv.reader([line], delimiter='\\t').next()        \n",
    "        ngram = fields[0]\n",
    "        yield len(ngram), ngram\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.emitted = False\n",
    "        \n",
    "    def reducer(self, length, values):\n",
    "        # We only need to emit the first one, which is the longest for this reducer\n",
    "        if not self.emitted:\n",
    "            self.emitted = True\n",
    "            ngrams = [ngram for ngram in values]\n",
    "            yield length, ngrams\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper,\n",
    "                reducer_init=self.reducer_init,\n",
    "                reducer=self.reducer,\n",
    "                # First key is length; sort it in reverse order\n",
    "                jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"1\",\n",
    "                    \"mapred.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapred.text.key.comparator.options\":\"-k1,1nr\"\n",
    "                          }                \n",
    "                  )\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrLongest.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare test file\n",
    "!head -n 10 googlebooks-eng-all-5gram-20090715-0-filtered.txt > testData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing unitTest_5_3a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unitTest_5_3a.py\n",
    "import unittest\n",
    "from MrLongest_5_3a import MrLongest\n",
    "\n",
    "class UnitTest_5_3(unittest.TestCase):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnitTest_5_3, self).__init__(*args, **kwargs)\n",
    "        with open('testData.txt', 'r') as f:\n",
    "            self.first_line = f.readline()\n",
    "            self.first_ngram = self.first_line.split('\\t')[0]\n",
    "        \n",
    "    def test_MrLongest_mapper(self):\n",
    "        j = MrLongest()\n",
    "        self.assertEqual(j.mapper(None, self.first_line).next(), \n",
    "                         (len(self.first_ngram), self.first_ngram))\n",
    "        \n",
    "    def test_MrLongest_reducer(self):\n",
    "        j = MrLongest()\n",
    "        ngrams = [\"0123456789\", \"A12345678B\"]\n",
    "        length = len(ngrams[0])\n",
    "        \n",
    "        j.reducer_init()\n",
    "        self.assertEqual(j.reducer(length, ngrams).next(), (length, ngrams))\n",
    "\n",
    "        # We only output the first one.\n",
    "        with self.assertRaises(StopIteration):\n",
    "            j.reducer(length, ngrams).next()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 2 tests in 0.004s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python unitTest_5_3a.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fullTest_5_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fullTest_5_3.py\n",
    "import unittest\n",
    "from MrLongest_5_3a import MrLongest\n",
    "\n",
    "class FullTest_5_3a(unittest.TestCase):\n",
    "\n",
    "    def test_full(self):\n",
    "        mr_job = MrLongest(\n",
    "            args=['testData.txt', \n",
    "                  # Have to use Hadoop, otherwise custom sort order won't work.\n",
    "                  '-r', 'hadoop', \n",
    "                  '--strict-protocols',\n",
    "                  # so options from local mrjob.conf don't pollute the test env.\n",
    "                  '--no-conf', \n",
    "                 ])\n",
    "\n",
    "        results = []\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            for line in runner.stream_output():\n",
    "                # Use the job's specified protocol to read the output\n",
    "                key, value = mr_job.parse_output_line(line)\n",
    "                results.append((key, value))\n",
    "\n",
    "        self.assertEqual(len(results), 1)\n",
    "        self.assertEqual(results[0], \n",
    "                (33, ['A Circumstantial Narrative of the', 'A BILL FOR ESTABLISHING RELIGIOUS']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.compat\"\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 37.503s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python fullTest_5_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it on a local hadoop, using just one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.110146.825546\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.110146.825546/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/MrLongest_5_3a.patrickng.20160215.110146.825546/files/\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar963639743509190685/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob3259114538871942951.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/MrLongest_5_3a.patrickng.20160215.110146.825546/output\n",
      "58\t[\"Hydroxytryptamine stimulates inositol phosphate production\", \"Interpersonal Communication Interpersonal communication is\"]\n",
      "STDERR: 16/02/15 19:02:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.110146.825546\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/MrLongest_5_3a.patrickng.20160215.110146.825546 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python MrLongest_5_3a.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "--jobconf mapred.map.tasks=3 --jobconf mapred.reduce.tasks=1 \\\n",
    "-r hadoop \\\n",
    "--strict-protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it for real on EMR, using all data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-2c54d3a4b9812930\n",
      "using s3://mrjob-2c54d3a4b9812930/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.111635.198877\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.111635.198877/b.py\n",
      "Copying non-input files into s3://mrjob-2c54d3a4b9812930/tmp/MrLongest_5_3a.patrickng.20160215.111635.198877/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-2X8QEUZQA1863\n",
      "Setting EMR tags: job=hw5_3a\n",
      "Created new job flow j-2X8QEUZQA1863\n",
      "Job launched 32.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 65.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 97.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 129.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 162.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 194.3s ago, status STARTING: Configuring cluster software\n",
      "Job launched 226.2s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 258.5s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 290.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 323.1s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 355.3s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 387.5s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 419.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 452.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 484.2s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 516.7s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 549.1s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 581.3s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 613.1s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 645.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 678.5s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 710.6s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 742.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 774.8s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 807.2s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 839.8s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 871.7s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 903.5s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 935.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 968.3s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1000.0s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1031.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1064.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1096.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 825.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 331\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 1021526088\n",
      "    FILE_BYTES_WRITTEN: 2048946252\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    S3_BYTES_READ: 2156069116\n",
      "    S3_BYTES_WRITTEN: 331\n",
      "  Job Counters :\n",
      "    Launched map tasks: 196\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 194\n",
      "    SLOTS_MILLIS_MAPS: 4485825\n",
      "    SLOTS_MILLIS_REDUCES: 723518\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 2459090\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 1862272363\n",
      "    Map output materialized bytes: 1022202521\n",
      "    Map output records: 58682266\n",
      "    Physical memory (bytes) snapshot: 126753538048\n",
      "    Reduce input groups: 80\n",
      "    Reduce input records: 58682266\n",
      "    Reduce output records: 1\n",
      "    Reduce shuffle bytes: 1022202521\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 117364532\n",
      "    Total committed heap usage (bytes): 123887681536\n",
      "    Virtual memory (bytes) snapshot: 234537271296\n",
      "Streaming final output from s3://mrjob-2c54d3a4b9812930/tmp/MrLongest_5_3a.patrickng.20160215.111635.198877/output/\n",
      "159\t[\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\", \"ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\"]\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.111635.198877\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/MrLongest_5_3a.patrickng.20160215.111635.198877/\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/logs/j-2X8QEUZQA1863/\n",
      "Terminating job flow: j-2X8QEUZQA1863\n"
     ]
    }
   ],
   "source": [
    "!python MrLongest_5_3a.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "--jobconf mapred.map.tasks=28 --jobconf mapred.reduce.tasks=1 \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.xlarge \\\n",
    "--num-ec2-instances 4 \\\n",
    "--emr-tag job=hw5_3a \\\n",
    "--strict-protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "Longest 5-gram:  \n",
    "Length = 159\t\n",
    "\n",
    "\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"  \n",
    "\n",
    "\"ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3b\n",
    "\n",
    "Top 10 most frequent words (please use the count information), i.e., unigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrFrequent_5_3b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrFrequent_5_3b.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "class MrFrequent(MRJob):\n",
    "    def word_count_mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "        fields = csv.reader([line], delimiter='\\t').next()        \n",
    "        count = int(fields[1])\n",
    "        \n",
    "        for word in fields[0].split():\n",
    "            yield word.lower(), count\n",
    "\n",
    "    def sum_word_count_reducer(self, key, counts):\n",
    "        yield key, sum([count for count in counts])\n",
    "        \n",
    "    def top_words_reducer(self, key, counts):\n",
    "        yield key, counts.next()\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.word_count_mapper,\n",
    "                combiner=self.sum_word_count_reducer,\n",
    "                reducer=self.sum_word_count_reducer,\n",
    "                ),\n",
    "            MRStep(\n",
    "                reducer=self.top_words_reducer,\n",
    "                jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"2\",\n",
    "                    \"mapred.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapred.text.key.comparator.options\":\"-k2,2nr\",\n",
    "                    \"mapred.reduce.tasks\":1\n",
    "                          }                \n",
    "                )\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrFrequent.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting unitTest_5_3b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unitTest_5_3b.py\n",
    "import unittest\n",
    "from MrFrequent_5_3b import MrFrequent\n",
    "\n",
    "class UnitTest_5_3b(unittest.TestCase):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnitTest_5_3b, self).__init__(*args, **kwargs)\n",
    "        with open('testData.txt', 'r') as f:\n",
    "            self.first_line = f.readline()\n",
    "            fields = self.first_line.split('\\t')\n",
    "            self.first_ngram = fields[0].lower()\n",
    "            self.count = int(fields[1])\n",
    "            self.first_words = self.first_ngram.split()\n",
    "        \n",
    "    def test_MrFrequent_mapper1(self):\n",
    "        j = MrFrequent()\n",
    "        self.assertEqual(j.word_count_mapper(None, self.first_line).next(), \n",
    "                         (self.first_words[0], self.count))\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.002s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python unitTest_5_3b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try it on local hadoop, using one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094205.039662\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094205.039662/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/MrFrequent_5_3b.patrickng.20160216.094205.039662/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar5860950533453137455/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob7115778972864884546.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar8853161565966944590/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob6388067108854124405.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/MrFrequent_5_3b.patrickng.20160216.094205.039662/output\n",
      "STDERR: 16/02/16 17:43:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094205.039662\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/MrFrequent_5_3b.patrickng.20160216.094205.039662 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python MrFrequent_5_3b.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "-r hadoop \\\n",
    "> wordsByFreq-hadoop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat wordsByFreq-hadoop.txt | cut -d$'\\t' -f 1 | sed 's/\"//g' > wordsByFreq.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-2c54d3a4b9812930\n",
      "using s3://mrjob-2c54d3a4b9812930/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094632.911086\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094632.911086/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Copying non-input files into s3://mrjob-2c54d3a4b9812930/tmp/MrFrequent_5_3b.patrickng.20160216.094632.911086/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-1L0VLAN20IZB5\n",
      "Setting EMR tags: job=hw5_3b\n",
      "Created new job flow j-1L0VLAN20IZB5\n",
      "Job launched 31.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 64.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 97.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 129.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 160.9s ago, status STARTING: Configuring cluster software\n",
      "Job launched 193.1s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 225.3s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 257.6s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 289.7s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 321.9s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 353.8s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 388.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 420.5s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 453.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 486.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 520.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 552.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 585.0s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 616.8s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 649.3s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 681.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 713.4s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 745.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 778.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 810.3s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 842.5s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 874.7s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 907.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 939.0s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 971.4s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 1003.3s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 1035.8s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 1068.0s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 1100.7s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 2 of 2)\n",
      "Job launched 1132.9s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 2 of 2)\n",
      "Job launched 1165.5s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 2 of 2)\n",
      "Job launched 1198.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 947.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 4158739\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 909800526\n",
      "    FILE_BYTES_WRITTEN: 302222521\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    HDFS_BYTES_WRITTEN: 4158739\n",
      "    S3_BYTES_READ: 2156069116\n",
      "  Job Counters :\n",
      "    Launched map tasks: 199\n",
      "    Launched reduce tasks: 35\n",
      "    Rack-local map tasks: 197\n",
      "    SLOTS_MILLIS_MAPS: 29222320\n",
      "    SLOTS_MILLIS_REDUCES: 9169157\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 7518420\n",
      "    Combine input records: 306120824\n",
      "    Combine output records: 19532239\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 3136729760\n",
      "    Map output materialized bytes: 89920750\n",
      "    Map output records: 293411330\n",
      "    Physical memory (bytes) snapshot: 107896635392\n",
      "    Reduce input groups: 269339\n",
      "    Reduce input records: 6822745\n",
      "    Reduce output records: 269339\n",
      "    Reduce shuffle bytes: 89920750\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 26354984\n",
      "    Total committed heap usage (bytes): 109886570496\n",
      "    Virtual memory (bytes) snapshot: 263895851008\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 4905624\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 4158739\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 3078094\n",
      "    FILE_BYTES_WRITTEN: 8857521\n",
      "    HDFS_BYTES_READ: 4919309\n",
      "    S3_BYTES_WRITTEN: 4158739\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 78\n",
      "    Launched map tasks: 85\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 7\n",
      "    SLOTS_MILLIS_MAPS: 882219\n",
      "    SLOTS_MILLIS_REDUCES: 27633\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 98890\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 4158739\n",
      "    Map input records: 269339\n",
      "    Map output bytes: 4428078\n",
      "    Map output materialized bytes: 3432619\n",
      "    Map output records: 269339\n",
      "    Physical memory (bytes) snapshot: 38913409024\n",
      "    Reduce input groups: 269339\n",
      "    Reduce input records: 269339\n",
      "    Reduce output records: 269339\n",
      "    Reduce shuffle bytes: 3432619\n",
      "    SPLIT_RAW_BYTES: 13685\n",
      "    Spilled Records: 538678\n",
      "    Total committed heap usage (bytes): 44468535296\n",
      "    Virtual memory (bytes) snapshot: 104911396864\n",
      "Streaming final output from s3://mrjob-2c54d3a4b9812930/tmp/MrFrequent_5_3b.patrickng.20160216.094632.911086/output/\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094632.911086\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/MrFrequent_5_3b.patrickng.20160216.094632.911086/\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/logs/j-1L0VLAN20IZB5/\n",
      "Terminating job flow: j-1L0VLAN20IZB5\n"
     ]
    }
   ],
   "source": [
    "!python MrFrequent_5_3b.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.xlarge \\\n",
    "--num-ec2-instances 6 \\\n",
    "--emr-tag job=hw5_3b \\\n",
    "> wordsByFreq-emr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Top 10 frequent words:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\r\n",
      "of\r\n",
      "to\r\n",
      "in\r\n",
      "a\r\n",
      "and\r\n",
      "that\r\n",
      "is\r\n",
      "be\r\n",
      "as\r\n"
     ]
    }
   ],
   "source": [
    "!cat wordsByFreq-emr.txt | cut -d$'\\t' -f 1 | sed 's/\"//g' > wordsByFreq.txt\n",
    "!head -10 wordsByFreq.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3c\n",
    "\n",
    "20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrDensity_5_3c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrDensity_5_3c.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing \n",
    "# order of relative frequency \n",
    "class MrDensity(MRJob):\n",
    "    emitted = 0\n",
    "    \n",
    "    def word_count_mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "        fields = csv.reader([line], delimiter='\\t').next()        \n",
    "        count = int(fields[1])\n",
    "        page_count = int(fields[2])\n",
    "        \n",
    "        for word in fields[0].split():\n",
    "            yield word, (count, page_count)\n",
    "\n",
    "    def get_density_reducer(self, key, values):\n",
    "        # Sum up the count and the page_count\n",
    "        (count_total, page_count_total) = reduce(lambda x,y: (x[0]+y[0], x[1]+y[1]), values)\n",
    "        yield key, count_total/page_count_total # density\n",
    "        \n",
    "    def sort_density_reducer(self, key, values):\n",
    "        yield key, values.next()\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.word_count_mapper,\n",
    "                reducer=self.get_density_reducer,\n",
    "                jobconf={\n",
    "                    \"mapred.job.name\":\"MrDensity-step1\",\n",
    "                    \"mapreduce.job.maps\":35,\n",
    "                    \"mapreduce.job.reduces\":35\n",
    "                    }\n",
    "                ),\n",
    "            \n",
    "            MRStep(\n",
    "                reducer=self.sort_density_reducer,\n",
    "                jobconf={\n",
    "                    \"mapred.job.name\":\"MrDensity-step2\",\n",
    "                    \"stream.num.map.output.key.fields\":\"2\",\n",
    "                    \"mapred.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapred.text.key.comparator.options\":\"-k2,2nr\",\n",
    "                    \"mapreduce.job.maps\":35,\n",
    "                    \"mapreduce.job.reduces\":1\n",
    "                          }                \n",
    "                )\n",
    "\n",
    "\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrDensity.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it on local hadoop using one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.145730.477553\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.145730.477553/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/MrDensity_5_3c.patrickng.20160215.145730.477553/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.job.name: mapreduce.job.name\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar429936120250666729/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob1906914641030899917.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.job.name: mapreduce.job.name\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar2617186584136399451/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob8682889912114133367.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/MrDensity_5_3c.patrickng.20160215.145730.477553/output\n",
      "STDERR: 16/02/15 22:59:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.145730.477553\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/MrDensity_5_3c.patrickng.20160215.145730.477553 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python MrDensity_5_3c.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "-r hadoop \\\n",
    "> hw_5_3c.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"lak\"\t3.072289156626506\r\n",
      "\"Honourable\"\t2.8927536231884057\r\n",
      "\"Expiration\"\t2.510204081632653\r\n",
      "\"operand\"\t2.353448275862069\r\n",
      "\"bust\"\t2.3493975903614457\r\n",
      "\"houseless\"\t2.274891774891775\r\n",
      "\"Gynecological\"\t2.2481536189069424\r\n",
      "\"denatured\"\t2.1864406779661016\r\n",
      "\"Saving\"\t2.1129032258064515\r\n",
      "\"Phe\"\t2.0408163265306123\r\n",
      "\"Pathology\"\t2.021301775147929\r\n",
      "\"Kiowa\"\t2.0\r\n",
      "\"apiece\"\t1.9607843137254901\r\n",
      "\"unreachable\"\t1.9433962264150944\r\n",
      "\"theres\"\t1.9230769230769231\r\n",
      "\"Rumanian\"\t1.904320987654321\r\n",
      "\"traitorously\"\t1.8928571428571428\r\n",
      "\"pilage\"\t1.8333333333333333\r\n",
      "\"Dock\"\t1.8028169014084507\r\n",
      "\"aristocrat\"\t1.7906976744186047\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 hw_5_3c.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Absolutist\"\t1.0\r\n",
      "\"Ability\"\t1.0\r\n",
      "\"Aberdeen\"\t1.0\r\n",
      "\"Abdul\"\t1.0\r\n",
      "\"Abbotsford\"\t1.0\r\n",
      "\"Abbot's\"\t1.0\r\n",
      "\"Abbot\"\t1.0\r\n",
      "\"Aalborg\"\t1.0\r\n",
      "\"AUDACIOUS\"\t1.0\r\n",
      "\"zebra\"\t1.0\r\n",
      "\"zeolite\"\t1.0\r\n",
      "\"zest\"\t1.0\r\n",
      "\"AMERICAN\"\t1.0\r\n",
      "\"AMERICA\"\t1.0\r\n",
      "\"AMAZON\"\t1.0\r\n",
      "\"AE\"\t1.0\r\n",
      "\"zodiac\"\t1.0\r\n",
      "\"nooks\"\t1.0\r\n",
      "\"nontrivial\"\t1.0\r\n",
      "\"nonsuit\"\t1.0\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 20 hw_5_3c.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-2c54d3a4b9812930\n",
      "using s3://mrjob-2c54d3a4b9812930/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.150403.990340\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.150403.990340/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Copying non-input files into s3://mrjob-2c54d3a4b9812930/tmp/MrDensity_5_3c.patrickng.20160215.150403.990340/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-3B6YY76OGMKEH\n",
      "Setting EMR tags: job=hw5_3c\n",
      "Created new job flow j-3B6YY76OGMKEH\n",
      "Detected hadoop configuration property names that do not match hadoop version 1.0.3:\n",
      "The have been translated as follows\n",
      " mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Detected hadoop configuration property names that do not match hadoop version 1.0.3:\n",
      "The have been translated as follows\n",
      " mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Job launched 32.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 65.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 97.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 130.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 162.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 195.5s ago, status STARTING: Configuring cluster software\n",
      "Job launched 227.6s ago, status STARTING: Configuring cluster software\n",
      "Job launched 260.2s ago, status STARTING: Configuring cluster software\n",
      "Job launched 292.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 324.2s ago, status STARTING: Configuring cluster software\n",
      "Job launched 356.1s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 388.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 420.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 453.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 485.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 517.9s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 550.0s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 582.3s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 614.2s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 646.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 678.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 711.3s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 743.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 776.0s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 807.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 840.1s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 872.2s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 905.1s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 937.3s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 969.7s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1001.7s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1034.3s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1066.1s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1098.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1130.2s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1162.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1195.0s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1227.2s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1259.2s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1292.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 2 of 2)\n",
      "Job launched 1324.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 2 of 2)\n",
      "Job launched 1356.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 1028.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from s3://mrjob-2c54d3a4b9812930/tmp/MrDensity_5_3c.patrickng.20160215.150403.990340/output/\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.150403.990340\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/MrDensity_5_3c.patrickng.20160215.150403.990340/\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/logs/j-3B6YY76OGMKEH/\n",
      "Terminating job flow: j-3B6YY76OGMKEH\n"
     ]
    }
   ],
   "source": [
    "!python MrDensity_5_3c.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.xlarge \\\n",
    "--num-ec2-instances 6 \\\n",
    "--emr-tag job=hw5_3c \\\n",
    "> hw_5_3c-emr.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "20 most dense and 20 least dense words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"xxxx\"\t11.557291666666666\r\n",
      "\"NA\"\t10.161726044782885\r\n",
      "\"blah\"\t8.0741599073001158\r\n",
      "\"nnn\"\t7.5333333333333332\r\n",
      "\"nd\"\t6.5611436445056839\r\n",
      "\"ND\"\t5.4073642846747196\r\n",
      "\"oooooooooooooooo\"\t4.921875\r\n",
      "\"PIC\"\t4.7272727272727275\r\n",
      "\"llll\"\t4.5116279069767442\r\n",
      "\"LUTHER\"\t4.3494983277591972\r\n",
      "\"oooooo\"\t4.2072378595731514\r\n",
      "\"NN\"\t4.0908402725208175\r\n",
      "\"ooooo\"\t3.9492846924177396\r\n",
      "\"OOOOOO\"\t3.9313725490196076\r\n",
      "\"IIII\"\t3.7877030162412995\r\n",
      "\"lillelu\"\t3.7624521072796937\r\n",
      "\"OOOOO\"\t3.6570701447431206\r\n",
      "\"Sc\"\t3.6065624999999999\r\n",
      "\"Pfeffermann\"\t3.5769230769230771\r\n",
      "\"Madarassy\"\t3.5769230769230771\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 hw_5_3c-emr.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"AAN\"\t1.0\r\n",
      "\"yallowchy\"\t1.0\r\n",
      "\"yeelding\"\t1.0\r\n",
      "\"yeers\"\t1.0\r\n",
      "\"yelloch\"\t1.0\r\n",
      "\"yemen\"\t1.0\r\n",
      "\"yerne\"\t1.0\r\n",
      "\"yestermorn\"\t1.0\r\n",
      "\"yleisen\"\t1.0\r\n",
      "\"ymounted\"\t1.0\r\n",
      "\"yont\"\t1.0\r\n",
      "\"youngster's\"\t1.0\r\n",
      "\"yproved\"\t1.0\r\n",
      "\"zamarra\"\t1.0\r\n",
      "\"zein\"\t1.0\r\n",
      "\"zeles\"\t1.0\r\n",
      "\"zeugmatographic\"\t1.0\r\n",
      "\"zoosperms\"\t1.0\r\n",
      "\"zuletzt\"\t1.0\r\n",
      "\"GOKHALE\"\t1.0\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 20 hw_5_3c-emr.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3d\n",
    "\n",
    "Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrDistribution_5_3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrDistribution_5_3d.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Distribution of 5-gram sizes (character length). E.g., count (using the count field) \n",
    "# up how many times a 5-gram of 50 characters shows up.\n",
    "class MrDistribution(MRJob):\n",
    "    def length_count_mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "        fields = csv.reader([line], delimiter='\\t').next()        \n",
    "        ngram = fields[0]\n",
    "        yield len(ngram), 1\n",
    "\n",
    "    def length_sum_reducer(self, length, values):\n",
    "        yield length, sum([v for v in values])\n",
    "        \n",
    "    def length_sort_reducer(self, length, values):\n",
    "        yield length, values.next()\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.length_count_mapper,\n",
    "                combiner=self.length_sum_reducer,\n",
    "                reducer=self.length_sum_reducer,\n",
    "                jobconf={\n",
    "                    \"mapred.job.name\":\"MrDistribution-step1\",\n",
    "                    \"mapreduce.job.maps\":21,\n",
    "                    \"mapreduce.job.reduces\":21\n",
    "                    }\n",
    "                  ),\n",
    "            MRStep(\n",
    "                reducer=self.length_sort_reducer,\n",
    "                jobconf={\n",
    "                    \"mapred.job.name\":\"MrDistribution-step2\",\n",
    "                    \"stream.num.map.output.key.fields\":\"1\",\n",
    "                    \"mapred.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapred.text.key.comparator.options\":\"-k1,1n\",\n",
    "                    \"mapreduce.job.maps\":21,\n",
    "                    \"mapreduce.job.reduces\":1\n",
    "                          }                \n",
    "                )\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrDistribution.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it using local hadoop on one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.155756.872061\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.155756.872061/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/MrDistribution_5_3d.patrickng.20160215.155756.872061/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar4375792671918037377/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob464519390787840335.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar7124349300120240179/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob5559430068776049745.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/MrDistribution_5_3d.patrickng.20160215.155756.872061/output\n",
      "9\t1\n",
      "10\t1\n",
      "11\t1\n",
      "12\t3\n",
      "13\t22\n",
      "14\t94\n",
      "15\t309\n",
      "16\t1112\n",
      "17\t2859\n",
      "18\t5853\n",
      "19\t10200\n",
      "20\t15479\n",
      "21\t20108\n",
      "22\t24625\n",
      "23\t27333\n",
      "24\t28052\n",
      "25\t27690\n",
      "26\t25991\n",
      "27\t23405\n",
      "28\t20587\n",
      "29\t17257\n",
      "30\t14428\n",
      "31\t11340\n",
      "32\t9061\n",
      "33\t6950\n",
      "34\t5152\n",
      "35\t3871\n",
      "36\t2868\n",
      "37\t2027\n",
      "38\t1516\n",
      "39\t1027\n",
      "40\t756\n",
      "41\t476\n",
      "42\t337\n",
      "43\t263\n",
      "44\t195\n",
      "45\t113\n",
      "46\t87\n",
      "47\t51\n",
      "48\t33\n",
      "49\t31\n",
      "50\t13\n",
      "51\t10\n",
      "52\t9\n",
      "53\t8\n",
      "54\t2\n",
      "55\t4\n",
      "57\t2\n",
      "58\t2\n",
      "STDERR: 16/02/15 23:59:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.155756.872061\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/MrDistribution_5_3d.patrickng.20160215.155756.872061 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python MrDistribution_5_3d.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "-r hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-2c54d3a4b9812930\n",
      "using s3://mrjob-2c54d3a4b9812930/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.163351.813491\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.163351.813491/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Copying non-input files into s3://mrjob-2c54d3a4b9812930/tmp/MrDistribution_5_3d.patrickng.20160215.163351.813491/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-2PCI3KL7N7Y90\n",
      "Setting EMR tags: job=hw5_3d\n",
      "Created new job flow j-2PCI3KL7N7Y90\n",
      "Detected hadoop configuration property names that do not match hadoop version 1.0.3:\n",
      "The have been translated as follows\n",
      " mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Detected hadoop configuration property names that do not match hadoop version 1.0.3:\n",
      "The have been translated as follows\n",
      " mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Job launched 32.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 64.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 97.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 129.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 161.7s ago, status STARTING: Configuring cluster software\n",
      "Job launched 194.6s ago, status STARTING: Configuring cluster software\n",
      "Job launched 226.8s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 259.1s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 292.3s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 324.3s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 356.2s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 388.3s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 420.4s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 453.6s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 485.9s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 518.2s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 550.0s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 581.9s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 614.1s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 646.1s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 678.7s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 712.5s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 744.2s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 2 of 2)\n",
      "Job launched 777.5s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 519.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from s3://mrjob-2c54d3a4b9812930/tmp/MrDistribution_5_3d.patrickng.20160215.163351.813491/output/\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.163351.813491\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/MrDistribution_5_3d.patrickng.20160215.163351.813491/\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/logs/j-2PCI3KL7N7Y90/\n",
      "Terminating job flow: j-2PCI3KL7N7Y90\n"
     ]
    }
   ],
   "source": [
    "!python MrDistribution_5_3d.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.xlarge \\\n",
    "--num-ec2-instances 4 \\\n",
    "--emr-tag job=hw5_3d \\\n",
    "> hw_5_3d-emr.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOItekhpn0UtS4yx6SWrc/wOzQPMWhpeBoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cfd0490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "data = {}\n",
    "with open(\"hw_5_3d-emr.result\", \"r\") as f:\n",
    "    for fields in csv.reader(f, delimiter=\"\\t\"):\n",
    "        data[int(fields[0])] = int(fields[1])\n",
    "        \n",
    "c,b,ax = plt.hist(data.keys(), weights=data.values(), bins=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4 (1)\n",
    "\n",
    "Build stripes for the most frequent 10,000 words using cooccurrence information based on\n",
    "the words ranked from 9,001-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get 1,001th to 10,000th most frequent words as our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sed -n 9001,10000p wordsByFreq.txt > vocab.txt\n",
    "!head -n 10000 wordsByFreq.txt > topwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrStripes_5_4_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrStripes_5_4_1.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def mergeStripes(x,y):\n",
    "    for k,v in y.items():\n",
    "        x[k] += v            \n",
    "    return x    \n",
    "\n",
    "# Build cooccurrence stripes\n",
    "class MrStripes(MRJob):\n",
    "    vocab = set()\n",
    "    topwords = set()\n",
    "\n",
    "    def checkAndAddToStripes(self, stripes, key, word, count):\n",
    "        # Add word to the stripe of key if word is found in the vocab\n",
    "        if word in self.vocab:\n",
    "            if key not in stripes:\n",
    "                stripes[key] = defaultdict(int)\n",
    "\n",
    "            stripes[key][word] += count\n",
    "\n",
    "    def mapper_init(self):\n",
    "        with open(\"vocab.txt\") as f:\n",
    "            for word in f.readlines():\n",
    "                self.vocab.add(word.strip().lower())\n",
    "                \n",
    "        with open(\"topwords.txt\") as f:\n",
    "            for word in f.readlines():\n",
    "                self.topwords.add(word.strip().lower())\n",
    "                \n",
    "    \n",
    "    def mapper(self, line_no, line):\n",
    "        [ngram,count,pages,books] = re.split(\"\\t\",line.strip())\n",
    "        ngram = ngram.lower() \n",
    "        count = int(count)\n",
    "        \n",
    "        # Build stripes for the most frequent 10,000 words using cooccurrence \n",
    "        # information based on the words ranked from 9,001-10,000 as a basis/vocabulary\n",
    "        \n",
    "        # Will build stripes only for those words found in topwords.txt\n",
    "        words = filter(lambda w: w in self.topwords, re.split(\" \",ngram))  \n",
    "        \n",
    "        # Build cooccurrence stripe for each remaining word\n",
    "        stripes = {} \n",
    "        words = sorted(words) # To help avoid pairing up identical items\n",
    "        for i in range(len(words)-1):\n",
    "            x = words[i]\n",
    "            for j in range(i+1, len(words)):\n",
    "                y = words[j]\n",
    "                if x == y:\n",
    "                    continue # Don't pair up same word\n",
    "                \n",
    "                #print \"Debug:x,y=\",x,y\n",
    "                self.checkAndAddToStripes(stripes, x, y, count)\n",
    "                self.checkAndAddToStripes(stripes, y, x, count)\n",
    "            \n",
    "        for key, stripe in stripes.items():\n",
    "            yield key, stripe   \n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        # merge the stripes\n",
    "        merged = defaultdict(int)\n",
    "        reduce(lambda x,y: mergeStripes(x,y), values, merged)\n",
    "        yield key, merged\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                combiner=self.reducer,\n",
    "                reducer=self.reducer,\n",
    "                ),\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrStripes.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrStripes_unit_test_5_4_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrStripes_unit_test_5_4_1.py\n",
    "import unittest\n",
    "from MrStripes_5_4_1 import MrStripes\n",
    "\n",
    "class MrStripes_5_4_1(unittest.TestCase):\n",
    "\n",
    "    def test_full(self):\n",
    "        mr_job = MrStripes(\n",
    "            args=['testInput_5_4_1.txt', \n",
    "                  '--file', 'vocabTest_5_4_1.txt#vocab.txt', \\\n",
    "                  '--file', 'topwordsTest_5_4_1.txt#topwords.txt', \\\n",
    "                  '-r', 'inline', \n",
    "                  '-q',\n",
    "                  # so options from local mrjob.conf don't pollute the test env.\n",
    "                  '--no-conf', \n",
    "                 ])\n",
    "\n",
    "        results = []\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            for line in runner.stream_output():\n",
    "                results.append(line)\n",
    "\n",
    "        with open(\"testOutput_5_4_1.txt\", \"r\") as f:\n",
    "            for x,y in zip(results, f.readlines()):\n",
    "                self.assertEqual(x.strip(),y.strip())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.runner\"\r\n",
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.034s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python MrStripes_unit_test_5_4_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python MrStripes_5_4_1.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "--file vocab.txt \\\n",
    "--file topwords.txt \\\n",
    "-r inline \\\n",
    "-q | sort \\\n",
    "> test_5_4_2-one-file.inverted.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://patng323-w261-hw541/input/vocab.txt\n",
      "remove_bucket: s3://patng323-w261-hw541/\n",
      "make_bucket: s3://patng323-w261-hw541/\n",
      "upload: ./vocab.txt to s3://patng323-w261-hw541/input/vocab.txt\n",
      "upload: ./topwords.txt to s3://patng323-w261-hw541/input/topwords.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rb --force s3://patng323-w261-hw541\n",
    "!aws s3 mb s3://patng323-w261-hw541\n",
    "!aws s3 cp vocab.txt s3://patng323-w261-hw541/input/vocab.txt\n",
    "!aws s3 cp topwords.txt s3://patng323-w261-hw541/input/topwords.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating new scratch bucket mrjob-ee8f80c0a6aff1b8\n",
      "using s3://mrjob-ee8f80c0a6aff1b8/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrStripes_5_4_1.patrickng.20160216.163959.154235\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrStripes_5_4_1.patrickng.20160216.163959.154235/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "creating S3 bucket 'mrjob-ee8f80c0a6aff1b8' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-ee8f80c0a6aff1b8/tmp/MrStripes_5_4_1.patrickng.20160216.163959.154235/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-3EAVDIZOYEW4M\n",
      "Created new job flow j-3EAVDIZOYEW4M\n",
      "Job launched 32.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 64.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 96.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 128.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 161.0s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 193.5s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 225.6s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160216.163959.154235: Step 1 of 1)\n",
      "Job launched 258.7s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160216.163959.154235: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 66.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 11483605\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 3164\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 2368\n",
      "    FILE_BYTES_WRITTEN: 196608\n",
      "    HDFS_BYTES_READ: 738\n",
      "    S3_BYTES_READ: 11483605\n",
      "    S3_BYTES_WRITTEN: 3164\n",
      "  Job Counters :\n",
      "    Launched map tasks: 6\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 6\n",
      "    SLOTS_MILLIS_MAPS: 69020\n",
      "    SLOTS_MILLIS_REDUCES: 19170\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 6900\n",
      "    Combine input records: 124\n",
      "    Combine output records: 117\n",
      "    Map input bytes: 11444614\n",
      "    Map input records: 311614\n",
      "    Map output bytes: 3434\n",
      "    Map output materialized bytes: 2744\n",
      "    Map output records: 124\n",
      "    Physical memory (bytes) snapshot: 2643976192\n",
      "    Reduce input groups: 103\n",
      "    Reduce input records: 117\n",
      "    Reduce output records: 103\n",
      "    Reduce shuffle bytes: 2744\n",
      "    SPLIT_RAW_BYTES: 738\n",
      "    Spilled Records: 234\n",
      "    Total committed heap usage (bytes): 2714238976\n",
      "    Virtual memory (bytes) snapshot: 9296449536\n",
      "Streaming final output from s3://patng323-w261-hw541/output/\n",
      "\"alternate\"\t{\"viewing\": 82}\n",
      "\"alzheimer's\"\t{\"dementia\": 181}\n",
      "\"amidst\"\t{\"tumult\": 80, \"restless\": 43}\n",
      "\"ammonium\"\t{\"hydroxide\": 78}\n",
      "\"anemia\"\t{\"pernicious\": 58}\n",
      "\"annum\"\t{\"thereon\": 69}\n",
      "\"approximated\"\t{\"subcutaneous\": 120}\n",
      "\"architectural\"\t{\"decoration\": 46}\n",
      "\"articular\"\t{\"cartilage\": 51}\n",
      "\"authoritative\"\t{\"interpreter\": 50}\n",
      "\"balcony\"\t{\"overlooking\": 139}\n",
      "\"bottles\"\t{\"necks\": 65}\n",
      "\"brightest\"\t{\"diamond\": 71}\n",
      "\"canons\"\t{\"commonest\": 43}\n",
      "\"careless\"\t{\"hasty\": 58}\n",
      "\"cartilage\"\t{\"articular\": 51, \"localized\": 51}\n",
      "\"ce\"\t{\"qui\": 48}\n",
      "\"commence\"\t{\"qui\": 63, \"palestinian\": 62}\n",
      "\"commonest\"\t{\"canons\": 43}\n",
      "\"commonplace\"\t{\"feathers\": 84}\n",
      "\"complexion\"\t{\"darker\": 86}\n",
      "\"contradictory\"\t{\"predicate\": 171}\n",
      "\"conveying\"\t{\"pipes\": 155}\n",
      "\"dame\"\t{\"habitation\": 66}\n",
      "\"darker\"\t{\"complexion\": 86}\n",
      "\"darkest\"\t{\"superstition\": 110}\n",
      "\"decoration\"\t{\"architectural\": 46}\n",
      "\"deliberations\"\t{\"trent\": 47}\n",
      "\"dementia\"\t{\"alzheimer's\": 181}\n",
      "\"diamond\"\t{\"brightest\": 71}\n",
      "\"discomfort\"\t{\"localized\": 43}\n",
      "\"dividend\"\t{\"shareholders\": 43}\n",
      "\"dumb\"\t{\"pretending\": 69}\n",
      "\"endowment\"\t{\"unstable\": 84}\n",
      "\"est\"\t{\"qui\": 83}\n",
      "\"establishments\"\t{\"sanitary\": 49}\n",
      "\"feathers\"\t{\"commonplace\": 84}\n",
      "\"flexor\"\t{\"sheath\": 203}\n",
      "\"flocks\"\t{\"herds\": 969}\n",
      "\"flourish\"\t{\"jungle\": 137}\n",
      "\"fossil\"\t{\"shells\": 103}\n",
      "\"habitation\"\t{\"dame\": 66}\n",
      "\"hasty\"\t{\"careless\": 58, \"impatience\": 41}\n",
      "\"herds\"\t{\"flocks\": 969, \"restless\": 43}\n",
      "\"humiliation\"\t{\"intolerable\": 127, \"rejoiced\": 132}\n",
      "\"hydroxide\"\t{\"soda\": 42, \"ammonium\": 78}\n",
      "\"impatience\"\t{\"hasty\": 41}\n",
      "\"indiana\"\t{\"linguistics\": 128}\n",
      "\"inspector\"\t{\"sanitary\": 46}\n",
      "\"interpreter\"\t{\"authoritative\": 50}\n",
      "\"intolerable\"\t{\"humiliation\": 127}\n",
      "\"irresistible\"\t{\"speedily\": 44}\n",
      "\"jones\"\t{\"summon\": 83}\n",
      "\"jungle\"\t{\"flourish\": 137}\n",
      "\"laden\"\t{\"spoils\": 67}\n",
      "\"linguistics\"\t{\"indiana\": 128}\n",
      "\"localized\"\t{\"discomfort\": 43, \"cartilage\": 51}\n",
      "\"magnificence\"\t{\"sketches\": 66}\n",
      "\"matthew\"\t{\"sayings\": 76}\n",
      "\"meridian\"\t{\"zenith\": 141}\n",
      "\"necks\"\t{\"bottles\": 65}\n",
      "\"operative\"\t{\"wholesale\": 43}\n",
      "\"overlooking\"\t{\"balcony\": 139}\n",
      "\"palestinian\"\t{\"commence\": 62}\n",
      "\"peritoneal\"\t{\"sac\": 77}\n",
      "\"pernicious\"\t{\"anemia\": 58}\n",
      "\"pink\"\t{\"replacing\": 47, \"wax\": 411}\n",
      "\"pipes\"\t{\"conveying\": 155}\n",
      "\"pitched\"\t{\"tents\": 123}\n",
      "\"polished\"\t{\"shells\": 59}\n",
      "\"predicate\"\t{\"contradictory\": 171}\n",
      "\"pretending\"\t{\"dumb\": 69}\n",
      "\"qui\"\t{\"est\": 83, \"commence\": 63, \"ce\": 48}\n",
      "\"rejoiced\"\t{\"humiliation\": 132}\n",
      "\"relaxed\"\t{\"vigilance\": 145}\n",
      "\"replacing\"\t{\"pink\": 47}\n",
      "\"resembling\"\t{\"shells\": 107}\n",
      "\"restless\"\t{\"amidst\": 43, \"herds\": 43}\n",
      "\"sac\"\t{\"peritoneal\": 77, \"uterine\": 48}\n",
      "\"sanitary\"\t{\"inspector\": 46, \"establishments\": 49}\n",
      "\"sayings\"\t{\"matthew\": 76}\n",
      "\"shareholders\"\t{\"dividend\": 43}\n",
      "\"sheath\"\t{\"flexor\": 203}\n",
      "\"shells\"\t{\"resembling\": 107, \"polished\": 59, \"fossil\": 103}\n",
      "\"sketches\"\t{\"magnificence\": 66}\n",
      "\"soda\"\t{\"hydroxide\": 42}\n",
      "\"speedily\"\t{\"irresistible\": 44}\n",
      "\"spoils\"\t{\"laden\": 67}\n",
      "\"subcutaneous\"\t{\"approximated\": 120}\n",
      "\"summon\"\t{\"jones\": 83}\n",
      "\"superstition\"\t{\"darkest\": 110}\n",
      "\"telescope\"\t{\"viewing\": 93}\n",
      "\"tents\"\t{\"pitched\": 123}\n",
      "\"thereon\"\t{\"annum\": 69}\n",
      "\"trent\"\t{\"deliberations\": 47}\n",
      "\"tumult\"\t{\"amidst\": 80}\n",
      "\"unstable\"\t{\"endowment\": 84}\n",
      "\"uterine\"\t{\"sac\": 48}\n",
      "\"viewing\"\t{\"alternate\": 82, \"telescope\": 93}\n",
      "\"vigilance\"\t{\"relaxed\": 145}\n",
      "\"wax\"\t{\"pink\": 411}\n",
      "\"wholesale\"\t{\"operative\": 43}\n",
      "\"zenith\"\t{\"meridian\": 141}\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrStripes_5_4_1.patrickng.20160216.163959.154235\n",
      "Removing all files in s3://mrjob-ee8f80c0a6aff1b8/tmp/MrStripes_5_4_1.patrickng.20160216.163959.154235/\n",
      "Removing all files in s3://mrjob-ee8f80c0a6aff1b8/tmp/logs/j-3EAVDIZOYEW4M/\n",
      "Terminating job flow: j-3EAVDIZOYEW4M\n"
     ]
    }
   ],
   "source": [
    "!python MrStripes_5_4_1.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "--file 's3://patng323-w261-hw541/input/vocab.txt#vocab.txt' \\\n",
    "--file 's3://patng323-w261-hw541/input/topwords.txt#topwords.txt' \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.large \\\n",
    "--aws-region us-east-1 \\\n",
    "--output-dir=s3://patng323-w261-hw541/output \\\n",
    "--num-ec2-instances 5 \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4(2)\n",
    "\n",
    "Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrComparison_5_4_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrComparison_5_4_2.py\n",
    "\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class MrComparison(MRJob):\n",
    "    # The output from hw5.4.1 is in JSONProtocol\n",
    "    INPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    \n",
    "    METHOD_JACCARD = \"jaccard\"\n",
    "    METHOD_COSINE_SIM = \"cosine_sim\"\n",
    "    COUNT_MARKER = \"#\"\n",
    "\n",
    "    totalCounts = {}\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(MrComparison, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--compareMethod', type='str', default=self.METHOD_JACCARD)\n",
    "        \n",
    "    def load_options(self, args):\n",
    "        super(MrComparison, self).load_options(args)\n",
    "        self.compareMethod = self.options.compareMethod\n",
    "        \n",
    "    def mapper1(self, term, stripe):\n",
    "        keys = sorted(stripe.keys()) # Have to sort it so that we can get ordered result??\n",
    "        for i in range(len(keys)-1):\n",
    "            word1 = keys[i]\n",
    "            for j in range(i+1, len(keys)):\n",
    "                if self.compareMethod == self.METHOD_JACCARD:\n",
    "                    measure = 1\n",
    "                    \n",
    "                yield (keys[i],keys[j]), measure\n",
    "            \n",
    "        if self.compareMethod == self.METHOD_JACCARD:\n",
    "            # It's the |A| value\n",
    "            yield self.COUNT_MARKER + term, len(stripe)\n",
    "            \n",
    "    def reducer1(self, key, values):\n",
    "        if key[0] == self.COUNT_MARKER:\n",
    "            yield key, values.next()\n",
    "        else:\n",
    "            total = reduce(lambda x,y: x+y, values)\n",
    "            yield key, total  \n",
    "            \n",
    "    def reducer2(self, key, values):\n",
    "        if key[0] == self.COUNT_MARKER:\n",
    "            self.totalCounts[key[1:]] = values.next()\n",
    "        else:\n",
    "            word1 = key[0]\n",
    "            word2 = key[1]\n",
    "            if self.compareMethod == self.METHOD_JACCARD:\n",
    "                cooccurCount = values.next()\n",
    "                # score = |A^B| / (|A|+|B|-|A^B|)\n",
    "                score = cooccurCount / \\\n",
    "                    (self.totalCounts[word1] + self.totalCounts[word2] - cooccurCount)\n",
    "                    \n",
    "                yield key, score\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper1,\n",
    "                combiner=self.reducer1,\n",
    "                reducer=self.reducer1,\n",
    "                ),\n",
    "            MRStep(\n",
    "                reducer=self.reducer2,\n",
    "                jobconf={\n",
    "                    \"mapred.reduce.tasks\":1 \n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrComparison.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A\", \"B\"]\t0.6666666666666666\r\n",
      "[\"A\", \"C\"]\t0.2\r\n",
      "[\"M\", \"N\"]\t1.0\r\n",
      "[\"M\", \"Z\"]\t0.5\r\n",
      "[\"N\", \"Z\"]\t0.5\r\n",
      "[\"X\", \"Y\"]\t1.0\r\n",
      "[\"X\", \"Z\"]\t0.3333333333333333\r\n",
      "[\"Y\", \"Z\"]\t0.3333333333333333\r\n"
     ]
    }
   ],
   "source": [
    "!python MrComparison_5_4_2.py \\\n",
    "-r inline \\\n",
    "test_5_4_2.inverted.index \\\n",
    "-q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--bootstrap 'sudo yum install -y python-pip'\n",
    "--bootstrap 'sudo pip install --upgrade scipy'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
