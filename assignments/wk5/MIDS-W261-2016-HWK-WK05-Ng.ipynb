{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.1\n",
    "In the database world What is 3NF? Does machine learning use data in 3NF? If so why?   \n",
    "In what form does ML consume data?  \n",
    "Why would one use log files that are denormalized?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. \n",
    "\n",
    "Run your code on the data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file)). \n",
    "\n",
    "In this output please include the webpage URL, webpageID and Visitor ID.:\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "1. Left joining Table Left with Table Right\n",
    "2. Right joining Table Left with Table Right\n",
    "3. Inner joining Table Left with Table Right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJoin_5_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJoin_5_2.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "    urls = {} # key = pageId, value = url\n",
    "    keys_emitted = set() # Set of keys of all emitted urls. Used for left join.\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MRJoin, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--joinType', type='str', default=\"inner\")\n",
    "        \n",
    "    def load_options(self, args):\n",
    "        super(MRJoin, self).load_options(args)\n",
    "        self.joinType = self.options.joinType\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        # Load URL info data file into memory.  \n",
    "        # Line format: \n",
    "        # 1287,/autoroute\n",
    "        with open(\"processed_urls.data\", \"r\") as f:\n",
    "            for fields in csv.reader(f):\n",
    "                self.urls[fields[0]] = fields[1]\n",
    "\n",
    "    def mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # V,1000,1,C,10001\n",
    "        fields = csv.reader([line]).next()\n",
    "        \n",
    "        key = fields[1]\n",
    "        url = None\n",
    "        toEmit = False\n",
    "        \n",
    "        if key in self.urls:\n",
    "            url = self.urls[key]\n",
    "            \n",
    "        if self.joinType == \"right\":\n",
    "            toEmit = True\n",
    "        elif self.joinType == \"left\":\n",
    "            if url is not None:\n",
    "                toEmit = True\n",
    "                self.keys_emitted.add(key) # Remember what we have emitted\n",
    "        else: # inner join\n",
    "            if url is not None:\n",
    "                toEmit = True\n",
    "        \n",
    "        if toEmit:\n",
    "            # Output format\n",
    "            # pageid, url,V,1,C,10001\n",
    "            yield key, (url, fields[0], fields[2], fields[3], fields[4])\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        if self.joinType == \"left\":\n",
    "            # Emit all the remaining urls\n",
    "            remaining = set(self.urls.keys()) - self.keys_emitted\n",
    "            for key in remaining:\n",
    "                yield key, (self.urls[key], None, None, None, None)\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   mapper_final=self.mapper_final)\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join type:left\n",
      "Number of records:98663\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n",
      "Join type:right\n",
      "Number of records:98654\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n",
      "Join type:inner\n",
      "Number of records:98654\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from MRJoin_5_2 import MRJoin\n",
    "\n",
    "for joinType in [\"left\", \"right\", \"inner\"]:\n",
    "    mr_job = MRJoin(args=['processed_anonymous-msweb.data', \n",
    "                        '--file', 'processed_urls.data', # broadcast to every mapper\n",
    "                        \"--strict-protocols\",\n",
    "                        '--joinType', joinType])\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "\n",
    "        lines = []\n",
    "        for line in runner.stream_output():\n",
    "            lines.append(line)\n",
    "            \n",
    "        print \"Join type:\" + joinType\n",
    "        print \"Number of records:\" + str(len(lines))\n",
    "        print \"First 5 lines:\"\n",
    "        for i in range(5):\n",
    "            print lines[i].strip()\n",
    "            \n",
    "        print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW 5.3  EDA of Google n-grams dataset\n",
    "For the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:  \n",
    "``googlebooks-eng-all-5gram-20090715-0-filtered.txt``\n",
    "\n",
    "Finally show your results on the Google n-grams dataset. \n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "``\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)``\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrLongest_5_3a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrLongest_5_3a.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Find Longest 5-gram (number of characters)\n",
    "class MrLongest(MRJob):\n",
    "    def mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "        fields = csv.reader([line], delimiter='\\t').next()        \n",
    "        ngram = fields[0]\n",
    "        yield len(ngram), ngram\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.emitted = False\n",
    "        \n",
    "    def reducer(self, length, values):\n",
    "        # We only need to emit the first one, which is the longest for this reducer\n",
    "        if not self.emitted:\n",
    "            self.emitted = True\n",
    "            ngrams = [ngram for ngram in values]\n",
    "            yield length, ngrams\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper,\n",
    "                reducer_init=self.reducer_init,\n",
    "                reducer=self.reducer,\n",
    "                # First key is length; sort it in reverse order\n",
    "                jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"1\",\n",
    "                    \"mapred.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapred.text.key.comparator.options\":\"-k1,1nr\"\n",
    "                          }                \n",
    "                  )\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrLongest.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare test file\n",
    "!head -n 10 googlebooks-eng-all-5gram-20090715-0-filtered.txt > testData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing unitTest_5_3a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unitTest_5_3a.py\n",
    "import unittest\n",
    "from MrLongest_5_3a import MrLongest\n",
    "\n",
    "class UnitTest_5_3(unittest.TestCase):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnitTest_5_3, self).__init__(*args, **kwargs)\n",
    "        with open('testData.txt', 'r') as f:\n",
    "            self.first_line = f.readline()\n",
    "            self.first_ngram = self.first_line.split('\\t')[0]\n",
    "        \n",
    "    def test_MrLongest_mapper(self):\n",
    "        j = MrLongest()\n",
    "        self.assertEqual(j.mapper(None, self.first_line).next(), \n",
    "                         (len(self.first_ngram), self.first_ngram))\n",
    "        \n",
    "    def test_MrLongest_reducer(self):\n",
    "        j = MrLongest()\n",
    "        ngrams = [\"0123456789\", \"A12345678B\"]\n",
    "        length = len(ngrams[0])\n",
    "        \n",
    "        j.reducer_init()\n",
    "        self.assertEqual(j.reducer(length, ngrams).next(), (length, ngrams))\n",
    "\n",
    "        # We only output the first one.\n",
    "        with self.assertRaises(StopIteration):\n",
    "            j.reducer(length, ngrams).next()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 2 tests in 0.004s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python unitTest_5_3a.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fullTest_5_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fullTest_5_3.py\n",
    "import unittest\n",
    "from MrLongest_5_3a import MrLongest\n",
    "\n",
    "class FullTest_5_3a(unittest.TestCase):\n",
    "\n",
    "    def test_full(self):\n",
    "        mr_job = MrLongest(\n",
    "            args=['testData.txt', \n",
    "                  # Have to use Hadoop, otherwise custom sort order won't work.\n",
    "                  '-r', 'hadoop', \n",
    "                  '--strict-protocols',\n",
    "                  # so options from local mrjob.conf don't pollute the test env.\n",
    "                  '--no-conf', \n",
    "                 ])\n",
    "\n",
    "        results = []\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            for line in runner.stream_output():\n",
    "                # Use the job's specified protocol to read the output\n",
    "                key, value = mr_job.parse_output_line(line)\n",
    "                results.append((key, value))\n",
    "\n",
    "        self.assertEqual(len(results), 1)\n",
    "        self.assertEqual(results[0], \n",
    "                (33, ['A Circumstantial Narrative of the', 'A BILL FOR ESTABLISHING RELIGIOUS']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.compat\"\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 37.503s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python fullTest_5_3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
