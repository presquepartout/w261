{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.1\n",
    "In the database world What is 3NF? Does machine learning use data in 3NF? If so why?   \n",
    "In what form does ML consume data?  \n",
    "Why would one use log files that are denormalized?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. \n",
    "\n",
    "Run your code on the data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file)). \n",
    "\n",
    "In this output please include the webpage URL, webpageID and Visitor ID.:\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "1. Left joining Table Left with Table Right\n",
    "2. Right joining Table Left with Table Right\n",
    "3. Inner joining Table Left with Table Right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJoin_5_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJoin_5_2.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "    urls = {} # key = pageId, value = url\n",
    "    keys_emitted = set() # Set of keys of all emitted urls. Used for left join.\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MRJoin, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--joinType', type='str', default=\"inner\")\n",
    "        \n",
    "    def load_options(self, args):\n",
    "        super(MRJoin, self).load_options(args)\n",
    "        self.joinType = self.options.joinType\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        # Load URL info data file into memory.  \n",
    "        # Line format: \n",
    "        # 1287,/autoroute\n",
    "        with open(\"processed_urls.data\", \"r\") as f:\n",
    "            for fields in csv.reader(f):\n",
    "                self.urls[fields[0]] = fields[1]\n",
    "\n",
    "    def mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # V,1000,1,C,10001\n",
    "        fields = csv.reader([line]).next()\n",
    "        \n",
    "        key = fields[1]\n",
    "        url = None\n",
    "        toEmit = False\n",
    "        \n",
    "        if key in self.urls:\n",
    "            url = self.urls[key]\n",
    "            \n",
    "        if self.joinType == \"right\":\n",
    "            toEmit = True\n",
    "        elif self.joinType == \"left\":\n",
    "            if url is not None:\n",
    "                toEmit = True\n",
    "                self.keys_emitted.add(key) # Remember what we have emitted\n",
    "        else: # inner join\n",
    "            if url is not None:\n",
    "                toEmit = True\n",
    "        \n",
    "        if toEmit:\n",
    "            # Output format\n",
    "            # pageid, url,V,1,C,10001\n",
    "            yield key, (url, fields[0], fields[2], fields[3], fields[4])\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        if self.joinType == \"left\":\n",
    "            # Emit all the remaining urls\n",
    "            remaining = set(self.urls.keys()) - self.keys_emitted\n",
    "            for key in remaining:\n",
    "                yield key, (self.urls[key], None, None, None, None)\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   mapper_final=self.mapper_final)\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join type:left\n",
      "Number of records:98663\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n",
      "Join type:right\n",
      "Number of records:98654\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n",
      "Join type:inner\n",
      "Number of records:98654\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from MRJoin_5_2 import MRJoin\n",
    "\n",
    "for joinType in [\"left\", \"right\", \"inner\"]:\n",
    "    mr_job = MRJoin(args=['processed_anonymous-msweb.data', \n",
    "                        '--file', 'processed_urls.data', # broadcast to every mapper\n",
    "                        \"--strict-protocols\",\n",
    "                        '--joinType', joinType])\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "\n",
    "        lines = []\n",
    "        for line in runner.stream_output():\n",
    "            lines.append(line)\n",
    "            \n",
    "        print \"Join type:\" + joinType\n",
    "        print \"Number of records:\" + str(len(lines))\n",
    "        print \"First 5 lines:\"\n",
    "        for i in range(5):\n",
    "            print lines[i].strip()\n",
    "            \n",
    "        print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW 5.3  EDA of Google n-grams dataset\n",
    "For the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:  \n",
    "``googlebooks-eng-all-5gram-20090715-0-filtered.txt``\n",
    "\n",
    "Finally show your results on the Google n-grams dataset. \n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "``\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)``\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3a\n",
    "\n",
    "Find Longest 5-gram (number of characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrLongest_5_3a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrLongest_5_3a.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Find Longest 5-gram (number of characters)\n",
    "class MrLongest(MRJob):\n",
    "    def mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "        fields = csv.reader([line], delimiter='\\t').next()        \n",
    "        ngram = fields[0]\n",
    "        yield len(ngram), ngram\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.emitted = False\n",
    "        \n",
    "    def reducer(self, length, values):\n",
    "        # We only need to emit the first one, which is the longest for this reducer\n",
    "        if not self.emitted:\n",
    "            self.emitted = True\n",
    "            ngrams = [ngram for ngram in values]\n",
    "            yield length, ngrams\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper,\n",
    "                reducer_init=self.reducer_init,\n",
    "                reducer=self.reducer,\n",
    "                # First key is length; sort it in reverse order\n",
    "                jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"1\",\n",
    "                    \"mapred.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapred.text.key.comparator.options\":\"-k1,1nr\"\n",
    "                          }                \n",
    "                  )\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrLongest.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare test file\n",
    "!head -n 10 googlebooks-eng-all-5gram-20090715-0-filtered.txt > testData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing unitTest_5_3a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unitTest_5_3a.py\n",
    "import unittest\n",
    "from MrLongest_5_3a import MrLongest\n",
    "\n",
    "class UnitTest_5_3(unittest.TestCase):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnitTest_5_3, self).__init__(*args, **kwargs)\n",
    "        with open('testData.txt', 'r') as f:\n",
    "            self.first_line = f.readline()\n",
    "            self.first_ngram = self.first_line.split('\\t')[0]\n",
    "        \n",
    "    def test_MrLongest_mapper(self):\n",
    "        j = MrLongest()\n",
    "        self.assertEqual(j.mapper(None, self.first_line).next(), \n",
    "                         (len(self.first_ngram), self.first_ngram))\n",
    "        \n",
    "    def test_MrLongest_reducer(self):\n",
    "        j = MrLongest()\n",
    "        ngrams = [\"0123456789\", \"A12345678B\"]\n",
    "        length = len(ngrams[0])\n",
    "        \n",
    "        j.reducer_init()\n",
    "        self.assertEqual(j.reducer(length, ngrams).next(), (length, ngrams))\n",
    "\n",
    "        # We only output the first one.\n",
    "        with self.assertRaises(StopIteration):\n",
    "            j.reducer(length, ngrams).next()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 2 tests in 0.004s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python unitTest_5_3a.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fullTest_5_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fullTest_5_3.py\n",
    "import unittest\n",
    "from MrLongest_5_3a import MrLongest\n",
    "\n",
    "class FullTest_5_3a(unittest.TestCase):\n",
    "\n",
    "    def test_full(self):\n",
    "        mr_job = MrLongest(\n",
    "            args=['testData.txt', \n",
    "                  # Have to use Hadoop, otherwise custom sort order won't work.\n",
    "                  '-r', 'hadoop', \n",
    "                  '--strict-protocols',\n",
    "                  # so options from local mrjob.conf don't pollute the test env.\n",
    "                  '--no-conf', \n",
    "                 ])\n",
    "\n",
    "        results = []\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            for line in runner.stream_output():\n",
    "                # Use the job's specified protocol to read the output\n",
    "                key, value = mr_job.parse_output_line(line)\n",
    "                results.append((key, value))\n",
    "\n",
    "        self.assertEqual(len(results), 1)\n",
    "        self.assertEqual(results[0], \n",
    "                (33, ['A Circumstantial Narrative of the', 'A BILL FOR ESTABLISHING RELIGIOUS']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.compat\"\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 37.503s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python fullTest_5_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it on a local hadoop, using just one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.110146.825546\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.110146.825546/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/MrLongest_5_3a.patrickng.20160215.110146.825546/files/\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar963639743509190685/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob3259114538871942951.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/MrLongest_5_3a.patrickng.20160215.110146.825546/output\n",
      "58\t[\"Hydroxytryptamine stimulates inositol phosphate production\", \"Interpersonal Communication Interpersonal communication is\"]\n",
      "STDERR: 16/02/15 19:02:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.110146.825546\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/MrLongest_5_3a.patrickng.20160215.110146.825546 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python MrLongest_5_3a.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "--jobconf mapred.map.tasks=3 --jobconf mapred.reduce.tasks=1 \\\n",
    "-r hadoop \\\n",
    "--strict-protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it for real on EMR, using all data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-2c54d3a4b9812930\n",
      "using s3://mrjob-2c54d3a4b9812930/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.111635.198877\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.111635.198877/b.py\n",
      "Copying non-input files into s3://mrjob-2c54d3a4b9812930/tmp/MrLongest_5_3a.patrickng.20160215.111635.198877/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-2X8QEUZQA1863\n",
      "Setting EMR tags: job=hw5_3a\n",
      "Created new job flow j-2X8QEUZQA1863\n",
      "Job launched 32.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 65.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 97.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 129.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 162.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 194.3s ago, status STARTING: Configuring cluster software\n",
      "Job launched 226.2s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 258.5s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 290.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 323.1s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 355.3s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 387.5s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 419.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 452.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 484.2s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 516.7s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 549.1s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 581.3s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 613.1s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 645.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 678.5s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 710.6s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 742.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 774.8s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 807.2s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 839.8s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 871.7s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 903.5s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 935.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 968.3s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1000.0s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1031.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1064.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1096.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 825.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 331\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 1021526088\n",
      "    FILE_BYTES_WRITTEN: 2048946252\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    S3_BYTES_READ: 2156069116\n",
      "    S3_BYTES_WRITTEN: 331\n",
      "  Job Counters :\n",
      "    Launched map tasks: 196\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 194\n",
      "    SLOTS_MILLIS_MAPS: 4485825\n",
      "    SLOTS_MILLIS_REDUCES: 723518\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 2459090\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 1862272363\n",
      "    Map output materialized bytes: 1022202521\n",
      "    Map output records: 58682266\n",
      "    Physical memory (bytes) snapshot: 126753538048\n",
      "    Reduce input groups: 80\n",
      "    Reduce input records: 58682266\n",
      "    Reduce output records: 1\n",
      "    Reduce shuffle bytes: 1022202521\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 117364532\n",
      "    Total committed heap usage (bytes): 123887681536\n",
      "    Virtual memory (bytes) snapshot: 234537271296\n",
      "Streaming final output from s3://mrjob-2c54d3a4b9812930/tmp/MrLongest_5_3a.patrickng.20160215.111635.198877/output/\n",
      "159\t[\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\", \"ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\"]\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.111635.198877\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/MrLongest_5_3a.patrickng.20160215.111635.198877/\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/logs/j-2X8QEUZQA1863/\n",
      "Terminating job flow: j-2X8QEUZQA1863\n"
     ]
    }
   ],
   "source": [
    "!python MrLongest_5_3a.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "--jobconf mapred.map.tasks=28 --jobconf mapred.reduce.tasks=1 \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.xlarge \\\n",
    "--num-ec2-instances 4 \\\n",
    "--emr-tag job=hw5_3a \\\n",
    "--strict-protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "Longest 5-gram:  \n",
    "Length = 159\t\n",
    "\n",
    "\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"  \n",
    "\n",
    "\"ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3b\n",
    "\n",
    "Top 10 most frequent words (please use the count information), i.e., unigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrFrequent_5_3b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrFrequent_5_3b.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "class MrFrequent(MRJob):\n",
    "    def word_count_mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "        fields = csv.reader([line], delimiter='\\t').next()        \n",
    "        count = int(fields[1])\n",
    "        \n",
    "        for word in fields[0].split():\n",
    "            yield word.lower(), count\n",
    "\n",
    "    def sum_word_count_reducer(self, key, counts):\n",
    "        yield key, sum([count for count in counts])\n",
    "        \n",
    "    def top_words_reducer(self, key, counts):\n",
    "        yield key, counts.next()\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.word_count_mapper,\n",
    "                combiner=self.sum_word_count_reducer,\n",
    "                reducer=self.sum_word_count_reducer,\n",
    "                ),\n",
    "            MRStep(\n",
    "                reducer=self.top_words_reducer,\n",
    "                jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"2\",\n",
    "                    \"mapred.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapred.text.key.comparator.options\":\"-k2,2nr\",\n",
    "                    \"mapred.reduce.tasks\":1\n",
    "                          }                \n",
    "                )\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrFrequent.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting unitTest_5_3b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unitTest_5_3b.py\n",
    "import unittest\n",
    "from MrFrequent_5_3b import MrFrequent\n",
    "\n",
    "class UnitTest_5_3b(unittest.TestCase):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnitTest_5_3b, self).__init__(*args, **kwargs)\n",
    "        with open('testData.txt', 'r') as f:\n",
    "            self.first_line = f.readline()\n",
    "            fields = self.first_line.split('\\t')\n",
    "            self.first_ngram = fields[0].lower()\n",
    "            self.count = int(fields[1])\n",
    "            self.first_words = self.first_ngram.split()\n",
    "        \n",
    "    def test_MrFrequent_mapper1(self):\n",
    "        j = MrFrequent()\n",
    "        self.assertEqual(j.word_count_mapper(None, self.first_line).next(), \n",
    "                         (self.first_words[0], self.count))\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.002s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python unitTest_5_3b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try it on local hadoop, using one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094205.039662\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094205.039662/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/MrFrequent_5_3b.patrickng.20160216.094205.039662/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar5860950533453137455/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob7115778972864884546.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar8853161565966944590/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob6388067108854124405.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/MrFrequent_5_3b.patrickng.20160216.094205.039662/output\n",
      "STDERR: 16/02/16 17:43:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094205.039662\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/MrFrequent_5_3b.patrickng.20160216.094205.039662 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python MrFrequent_5_3b.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "-r hadoop \\\n",
    "> topwords-hadoop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat topwords-hadoop.txt | cut -d$'\\t' -f 1 | sed 's/\"//g' > topwords.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-2c54d3a4b9812930\n",
      "using s3://mrjob-2c54d3a4b9812930/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094632.911086\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094632.911086/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Copying non-input files into s3://mrjob-2c54d3a4b9812930/tmp/MrFrequent_5_3b.patrickng.20160216.094632.911086/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-1L0VLAN20IZB5\n",
      "Setting EMR tags: job=hw5_3b\n",
      "Created new job flow j-1L0VLAN20IZB5\n",
      "Job launched 31.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 64.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 97.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 129.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 160.9s ago, status STARTING: Configuring cluster software\n",
      "Job launched 193.1s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 225.3s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 257.6s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 289.7s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 321.9s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 353.8s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 388.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 420.5s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 453.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 486.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 520.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 552.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 585.0s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 616.8s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 649.3s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 681.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 713.4s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 745.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 778.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 810.3s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 842.5s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 874.7s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 907.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 939.0s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 971.4s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 1003.3s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 1035.8s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 1068.0s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 1 of 2)\n",
      "Job launched 1100.7s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 2 of 2)\n",
      "Job launched 1132.9s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 2 of 2)\n",
      "Job launched 1165.5s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 2 of 2)\n",
      "Job launched 1198.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160216.094632.911086: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 947.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 4158739\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 909800526\n",
      "    FILE_BYTES_WRITTEN: 302222521\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    HDFS_BYTES_WRITTEN: 4158739\n",
      "    S3_BYTES_READ: 2156069116\n",
      "  Job Counters :\n",
      "    Launched map tasks: 199\n",
      "    Launched reduce tasks: 35\n",
      "    Rack-local map tasks: 197\n",
      "    SLOTS_MILLIS_MAPS: 29222320\n",
      "    SLOTS_MILLIS_REDUCES: 9169157\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 7518420\n",
      "    Combine input records: 306120824\n",
      "    Combine output records: 19532239\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 3136729760\n",
      "    Map output materialized bytes: 89920750\n",
      "    Map output records: 293411330\n",
      "    Physical memory (bytes) snapshot: 107896635392\n",
      "    Reduce input groups: 269339\n",
      "    Reduce input records: 6822745\n",
      "    Reduce output records: 269339\n",
      "    Reduce shuffle bytes: 89920750\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 26354984\n",
      "    Total committed heap usage (bytes): 109886570496\n",
      "    Virtual memory (bytes) snapshot: 263895851008\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 4905624\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 4158739\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 3078094\n",
      "    FILE_BYTES_WRITTEN: 8857521\n",
      "    HDFS_BYTES_READ: 4919309\n",
      "    S3_BYTES_WRITTEN: 4158739\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 78\n",
      "    Launched map tasks: 85\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 7\n",
      "    SLOTS_MILLIS_MAPS: 882219\n",
      "    SLOTS_MILLIS_REDUCES: 27633\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 98890\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 4158739\n",
      "    Map input records: 269339\n",
      "    Map output bytes: 4428078\n",
      "    Map output materialized bytes: 3432619\n",
      "    Map output records: 269339\n",
      "    Physical memory (bytes) snapshot: 38913409024\n",
      "    Reduce input groups: 269339\n",
      "    Reduce input records: 269339\n",
      "    Reduce output records: 269339\n",
      "    Reduce shuffle bytes: 3432619\n",
      "    SPLIT_RAW_BYTES: 13685\n",
      "    Spilled Records: 538678\n",
      "    Total committed heap usage (bytes): 44468535296\n",
      "    Virtual memory (bytes) snapshot: 104911396864\n",
      "Streaming final output from s3://mrjob-2c54d3a4b9812930/tmp/MrFrequent_5_3b.patrickng.20160216.094632.911086/output/\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160216.094632.911086\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/MrFrequent_5_3b.patrickng.20160216.094632.911086/\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/logs/j-1L0VLAN20IZB5/\n",
      "Terminating job flow: j-1L0VLAN20IZB5\n"
     ]
    }
   ],
   "source": [
    "!python MrFrequent_5_3b.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.xlarge \\\n",
    "--num-ec2-instances 6 \\\n",
    "--emr-tag job=hw5_3b \\\n",
    "> topwords-emr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "Top 10 frequent words:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\r\n",
      "of\r\n",
      "to\r\n",
      "in\r\n",
      "a\r\n",
      "and\r\n",
      "that\r\n",
      "is\r\n",
      "be\r\n",
      "as\r\n"
     ]
    }
   ],
   "source": [
    "!cat topwords-emr.txt | cut -d$'\\t' -f 1 | sed 's/\"//g' > topwords.txt\n",
    "!head -10 topwords.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.3c\n",
    "\n",
    "20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrDensity_5_3c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrDensity_5_3c.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing \n",
    "# order of relative frequency \n",
    "class MrDensity(MRJob):\n",
    "    emitted = 0\n",
    "    \n",
    "    def word_count_mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "        fields = csv.reader([line], delimiter='\\t').next()        \n",
    "        count = int(fields[1])\n",
    "        page_count = int(fields[2])\n",
    "        \n",
    "        for word in fields[0].split():\n",
    "            yield word, (count, page_count)\n",
    "\n",
    "    def get_density_reducer(self, key, values):\n",
    "        # Sum up the count and the page_count\n",
    "        (count_total, page_count_total) = reduce(lambda x,y: (x[0]+y[0], x[1]+y[1]), values)\n",
    "        yield key, count_total/page_count_total # density\n",
    "        \n",
    "    def sort_density_reducer(self, key, values):\n",
    "        yield key, values.next()\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.word_count_mapper,\n",
    "                reducer=self.get_density_reducer,\n",
    "                jobconf={\n",
    "                    \"mapred.job.name\":\"MrDensity-step1\",\n",
    "                    \"mapreduce.job.maps\":35,\n",
    "                    \"mapreduce.job.reduces\":35\n",
    "                    }\n",
    "                ),\n",
    "            \n",
    "            MRStep(\n",
    "                reducer=self.sort_density_reducer,\n",
    "                jobconf={\n",
    "                    \"mapred.job.name\":\"MrDensity-step2\",\n",
    "                    \"stream.num.map.output.key.fields\":\"2\",\n",
    "                    \"mapred.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapred.text.key.comparator.options\":\"-k2,2nr\",\n",
    "                    \"mapreduce.job.maps\":35,\n",
    "                    \"mapreduce.job.reduces\":1\n",
    "                          }                \n",
    "                )\n",
    "\n",
    "\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrDensity.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it on local hadoop using one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.145730.477553\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.145730.477553/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/MrDensity_5_3c.patrickng.20160215.145730.477553/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.job.name: mapreduce.job.name\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar429936120250666729/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob1906914641030899917.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "mapred.job.name: mapreduce.job.name\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar2617186584136399451/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob8682889912114133367.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/MrDensity_5_3c.patrickng.20160215.145730.477553/output\n",
      "STDERR: 16/02/15 22:59:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.145730.477553\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/MrDensity_5_3c.patrickng.20160215.145730.477553 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python MrDensity_5_3c.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "-r hadoop \\\n",
    "> hw_5_3c.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"lak\"\t3.072289156626506\r\n",
      "\"Honourable\"\t2.8927536231884057\r\n",
      "\"Expiration\"\t2.510204081632653\r\n",
      "\"operand\"\t2.353448275862069\r\n",
      "\"bust\"\t2.3493975903614457\r\n",
      "\"houseless\"\t2.274891774891775\r\n",
      "\"Gynecological\"\t2.2481536189069424\r\n",
      "\"denatured\"\t2.1864406779661016\r\n",
      "\"Saving\"\t2.1129032258064515\r\n",
      "\"Phe\"\t2.0408163265306123\r\n",
      "\"Pathology\"\t2.021301775147929\r\n",
      "\"Kiowa\"\t2.0\r\n",
      "\"apiece\"\t1.9607843137254901\r\n",
      "\"unreachable\"\t1.9433962264150944\r\n",
      "\"theres\"\t1.9230769230769231\r\n",
      "\"Rumanian\"\t1.904320987654321\r\n",
      "\"traitorously\"\t1.8928571428571428\r\n",
      "\"pilage\"\t1.8333333333333333\r\n",
      "\"Dock\"\t1.8028169014084507\r\n",
      "\"aristocrat\"\t1.7906976744186047\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 hw_5_3c.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Absolutist\"\t1.0\r\n",
      "\"Ability\"\t1.0\r\n",
      "\"Aberdeen\"\t1.0\r\n",
      "\"Abdul\"\t1.0\r\n",
      "\"Abbotsford\"\t1.0\r\n",
      "\"Abbot's\"\t1.0\r\n",
      "\"Abbot\"\t1.0\r\n",
      "\"Aalborg\"\t1.0\r\n",
      "\"AUDACIOUS\"\t1.0\r\n",
      "\"zebra\"\t1.0\r\n",
      "\"zeolite\"\t1.0\r\n",
      "\"zest\"\t1.0\r\n",
      "\"AMERICAN\"\t1.0\r\n",
      "\"AMERICA\"\t1.0\r\n",
      "\"AMAZON\"\t1.0\r\n",
      "\"AE\"\t1.0\r\n",
      "\"zodiac\"\t1.0\r\n",
      "\"nooks\"\t1.0\r\n",
      "\"nontrivial\"\t1.0\r\n",
      "\"nonsuit\"\t1.0\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 20 hw_5_3c.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-2c54d3a4b9812930\n",
      "using s3://mrjob-2c54d3a4b9812930/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.150403.990340\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.150403.990340/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Copying non-input files into s3://mrjob-2c54d3a4b9812930/tmp/MrDensity_5_3c.patrickng.20160215.150403.990340/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-3B6YY76OGMKEH\n",
      "Setting EMR tags: job=hw5_3c\n",
      "Created new job flow j-3B6YY76OGMKEH\n",
      "Detected hadoop configuration property names that do not match hadoop version 1.0.3:\n",
      "The have been translated as follows\n",
      " mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Detected hadoop configuration property names that do not match hadoop version 1.0.3:\n",
      "The have been translated as follows\n",
      " mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Job launched 32.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 65.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 97.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 130.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 162.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 195.5s ago, status STARTING: Configuring cluster software\n",
      "Job launched 227.6s ago, status STARTING: Configuring cluster software\n",
      "Job launched 260.2s ago, status STARTING: Configuring cluster software\n",
      "Job launched 292.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 324.2s ago, status STARTING: Configuring cluster software\n",
      "Job launched 356.1s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 388.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 420.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 453.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 485.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 517.9s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 550.0s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 582.3s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 614.2s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 646.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 678.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 711.3s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 743.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 776.0s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 807.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 840.1s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 872.2s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 905.1s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 937.3s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 969.7s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1001.7s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1034.3s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1066.1s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1098.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1130.2s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1162.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1195.0s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1227.2s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1259.2s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 1 of 2)\n",
      "Job launched 1292.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 2 of 2)\n",
      "Job launched 1324.4s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 2 of 2)\n",
      "Job launched 1356.8s ago, status RUNNING: Running step (MrDensity_5_3c.patrickng.20160215.150403.990340: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 1028.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from s3://mrjob-2c54d3a4b9812930/tmp/MrDensity_5_3c.patrickng.20160215.150403.990340/output/\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDensity_5_3c.patrickng.20160215.150403.990340\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/MrDensity_5_3c.patrickng.20160215.150403.990340/\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/logs/j-3B6YY76OGMKEH/\n",
      "Terminating job flow: j-3B6YY76OGMKEH\n"
     ]
    }
   ],
   "source": [
    "!python MrDensity_5_3c.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.xlarge \\\n",
    "--num-ec2-instances 6 \\\n",
    "--emr-tag job=hw5_3c \\\n",
    "> hw_5_3c-emr.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "20 most dense and 20 least dense words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"xxxx\"\t11.557291666666666\r\n",
      "\"NA\"\t10.161726044782885\r\n",
      "\"blah\"\t8.0741599073001158\r\n",
      "\"nnn\"\t7.5333333333333332\r\n",
      "\"nd\"\t6.5611436445056839\r\n",
      "\"ND\"\t5.4073642846747196\r\n",
      "\"oooooooooooooooo\"\t4.921875\r\n",
      "\"PIC\"\t4.7272727272727275\r\n",
      "\"llll\"\t4.5116279069767442\r\n",
      "\"LUTHER\"\t4.3494983277591972\r\n",
      "\"oooooo\"\t4.2072378595731514\r\n",
      "\"NN\"\t4.0908402725208175\r\n",
      "\"ooooo\"\t3.9492846924177396\r\n",
      "\"OOOOOO\"\t3.9313725490196076\r\n",
      "\"IIII\"\t3.7877030162412995\r\n",
      "\"lillelu\"\t3.7624521072796937\r\n",
      "\"OOOOO\"\t3.6570701447431206\r\n",
      "\"Sc\"\t3.6065624999999999\r\n",
      "\"Pfeffermann\"\t3.5769230769230771\r\n",
      "\"Madarassy\"\t3.5769230769230771\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 hw_5_3c-emr.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"AAN\"\t1.0\r\n",
      "\"yallowchy\"\t1.0\r\n",
      "\"yeelding\"\t1.0\r\n",
      "\"yeers\"\t1.0\r\n",
      "\"yelloch\"\t1.0\r\n",
      "\"yemen\"\t1.0\r\n",
      "\"yerne\"\t1.0\r\n",
      "\"yestermorn\"\t1.0\r\n",
      "\"yleisen\"\t1.0\r\n",
      "\"ymounted\"\t1.0\r\n",
      "\"yont\"\t1.0\r\n",
      "\"youngster's\"\t1.0\r\n",
      "\"yproved\"\t1.0\r\n",
      "\"zamarra\"\t1.0\r\n",
      "\"zein\"\t1.0\r\n",
      "\"zeles\"\t1.0\r\n",
      "\"zeugmatographic\"\t1.0\r\n",
      "\"zoosperms\"\t1.0\r\n",
      "\"zuletzt\"\t1.0\r\n",
      "\"GOKHALE\"\t1.0\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 20 hw_5_3c-emr.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW5.3d\n",
    "\n",
    "Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrDistribution_5_3d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrDistribution_5_3d.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Distribution of 5-gram sizes (character length). E.g., count (using the count field) \n",
    "# up how many times a 5-gram of 50 characters shows up.\n",
    "class MrDistribution(MRJob):\n",
    "    def length_count_mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "        fields = csv.reader([line], delimiter='\\t').next()        \n",
    "        ngram = fields[0]\n",
    "        yield len(ngram), 1\n",
    "\n",
    "    def length_sum_reducer(self, length, values):\n",
    "        yield length, sum([v for v in values])\n",
    "        \n",
    "    def length_sort_reducer(self, length, values):\n",
    "        yield length, values.next()\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.length_count_mapper,\n",
    "                combiner=self.length_sum_reducer,\n",
    "                reducer=self.length_sum_reducer,\n",
    "                jobconf={\n",
    "                    \"mapred.job.name\":\"MrDistribution-step1\",\n",
    "                    \"mapreduce.job.maps\":21,\n",
    "                    \"mapreduce.job.reduces\":21\n",
    "                    }\n",
    "                  ),\n",
    "            MRStep(\n",
    "                reducer=self.length_sort_reducer,\n",
    "                jobconf={\n",
    "                    \"mapred.job.name\":\"MrDistribution-step2\",\n",
    "                    \"stream.num.map.output.key.fields\":\"1\",\n",
    "                    \"mapred.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapred.text.key.comparator.options\":\"-k1,1n\",\n",
    "                    \"mapreduce.job.maps\":21,\n",
    "                    \"mapreduce.job.reduces\":1\n",
    "                          }                \n",
    "                )\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrDistribution.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it using local hadoop on one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.155756.872061\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.155756.872061/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/MrDistribution_5_3d.patrickng.20160215.155756.872061/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar4375792671918037377/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob464519390787840335.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar7124349300120240179/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob5559430068776049745.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/MrDistribution_5_3d.patrickng.20160215.155756.872061/output\n",
      "9\t1\n",
      "10\t1\n",
      "11\t1\n",
      "12\t3\n",
      "13\t22\n",
      "14\t94\n",
      "15\t309\n",
      "16\t1112\n",
      "17\t2859\n",
      "18\t5853\n",
      "19\t10200\n",
      "20\t15479\n",
      "21\t20108\n",
      "22\t24625\n",
      "23\t27333\n",
      "24\t28052\n",
      "25\t27690\n",
      "26\t25991\n",
      "27\t23405\n",
      "28\t20587\n",
      "29\t17257\n",
      "30\t14428\n",
      "31\t11340\n",
      "32\t9061\n",
      "33\t6950\n",
      "34\t5152\n",
      "35\t3871\n",
      "36\t2868\n",
      "37\t2027\n",
      "38\t1516\n",
      "39\t1027\n",
      "40\t756\n",
      "41\t476\n",
      "42\t337\n",
      "43\t263\n",
      "44\t195\n",
      "45\t113\n",
      "46\t87\n",
      "47\t51\n",
      "48\t33\n",
      "49\t31\n",
      "50\t13\n",
      "51\t10\n",
      "52\t9\n",
      "53\t8\n",
      "54\t2\n",
      "55\t4\n",
      "57\t2\n",
      "58\t2\n",
      "STDERR: 16/02/15 23:59:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.155756.872061\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/MrDistribution_5_3d.patrickng.20160215.155756.872061 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python MrDistribution_5_3d.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "-r hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-2c54d3a4b9812930\n",
      "using s3://mrjob-2c54d3a4b9812930/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.163351.813491\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.163351.813491/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Copying non-input files into s3://mrjob-2c54d3a4b9812930/tmp/MrDistribution_5_3d.patrickng.20160215.163351.813491/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-2PCI3KL7N7Y90\n",
      "Setting EMR tags: job=hw5_3d\n",
      "Created new job flow j-2PCI3KL7N7Y90\n",
      "Detected hadoop configuration property names that do not match hadoop version 1.0.3:\n",
      "The have been translated as follows\n",
      " mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Detected hadoop configuration property names that do not match hadoop version 1.0.3:\n",
      "The have been translated as follows\n",
      " mapreduce.job.maps: mapred.map.tasks\n",
      "mapreduce.job.reduces: mapred.reduce.tasks\n",
      "Job launched 32.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 64.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 97.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 129.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 161.7s ago, status STARTING: Configuring cluster software\n",
      "Job launched 194.6s ago, status STARTING: Configuring cluster software\n",
      "Job launched 226.8s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 259.1s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 292.3s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 324.3s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 356.2s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 388.3s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 420.4s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 453.6s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 485.9s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 518.2s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 550.0s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 581.9s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 614.1s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 646.1s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 678.7s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 712.5s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 1 of 2)\n",
      "Job launched 744.2s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 2 of 2)\n",
      "Job launched 777.5s ago, status RUNNING: Running step (MrDistribution_5_3d.patrickng.20160215.163351.813491: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 519.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from s3://mrjob-2c54d3a4b9812930/tmp/MrDistribution_5_3d.patrickng.20160215.163351.813491/output/\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrDistribution_5_3d.patrickng.20160215.163351.813491\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/MrDistribution_5_3d.patrickng.20160215.163351.813491/\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/logs/j-2PCI3KL7N7Y90/\n",
      "Terminating job flow: j-2PCI3KL7N7Y90\n"
     ]
    }
   ],
   "source": [
    "!python MrDistribution_5_3d.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.xlarge \\\n",
    "--num-ec2-instances 4 \\\n",
    "--emr-tag job=hw5_3d \\\n",
    "> hw_5_3d-emr.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEKCAYAAAAcgp5RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSeo4g16SOs6gl6SOM+glqeMMeknquP8Pg1WW5ZarDssAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c20b7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "data = {}\n",
    "with open(\"hw_5_3d-emr.result\", \"r\") as f:\n",
    "    for fields in csv.reader(f, delimiter=\"\\t\"):\n",
    "        data[int(fields[0])] = int(fields[1])\n",
    "        \n",
    "plt.hist(data.keys(), weights=data.values(), bins=80)\n",
    "plt.title(\"Histogram of 5-gram sizes\")\n",
    "plt.show()\n",
    "\n",
    "plt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4 (1)\n",
    "\n",
    "Build stripes for the most frequent 10,000 words using cooccurrence information based on\n",
    "the words ranked from 1,001-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get 9,001th to 10,000th most frequent words as our vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9000 vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "#!sed -n 9001,10000p topwords.txt > vocab.txt\n",
    "!sed -n 1001,10000p topwords.txt > vocab.txt\n",
    "!wc -l vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrStripes_5_4_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrStripes_5_4_1.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def mergeStripes(x,y):\n",
    "    for k,v in y.items():\n",
    "        x[k] += v            \n",
    "    return x    \n",
    "\n",
    "def addToStripes(stripes,key,word,count):\n",
    "    if key not in stripes:\n",
    "        stripes[key] = defaultdict(int)\n",
    "        \n",
    "    stripes[key][word] += count\n",
    "\n",
    "# Build cooccurrence stripes\n",
    "class MrStripes(MRJob):\n",
    "    vocab = set()\n",
    "\n",
    "    def mapper_init(self):\n",
    "        with open(\"vocab.txt\") as f:\n",
    "            for word in f.readlines():\n",
    "                self.vocab.add(word.strip().lower())\n",
    "    \n",
    "    def mapper(self, line_no, line):\n",
    "        [ngram,count,pages,books] = re.split(\"\\t\",line.strip())\n",
    "        ngram = ngram.lower() # Should we????\n",
    "        count = int(count)\n",
    "        \n",
    "        # keep only those words found in vocab.txt\n",
    "        words = filter(lambda w: w in self.vocab, re.split(\" \",ngram))  \n",
    "        \n",
    "        # Build cooccurrence stripe for each remaining word\n",
    "        stripes = {} \n",
    "        words = sorted(words) # To help avoid pairing up identical items\n",
    "        for i in range(len(words)-1):\n",
    "            word1 = words[i]\n",
    "            for j in range(i+1, len(words)):\n",
    "                word2 = words[j]\n",
    "                if word1 == word2:\n",
    "                    continue\n",
    "                addToStripes(stripes, word1, word2, count)\n",
    "                addToStripes(stripes, word2, word1, count)\n",
    "            \n",
    "        for key, stripe in stripes.items():\n",
    "            yield key, stripe   \n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        # merge the stripes\n",
    "        merged = defaultdict(int)\n",
    "        reduce(lambda x,y: mergeStripes(x,y), values, merged)\n",
    "        yield key, merged\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                mapper=self.mapper,\n",
    "                combiner=self.reducer,\n",
    "                reducer=self.reducer,\n",
    "                ),\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrStripes.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrStripes_unit_test_5_4_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrStripes_unit_test_5_4_1.py\n",
    "import unittest\n",
    "from MrStripes_5_4_1 import MrStripes\n",
    "\n",
    "class MrStripes_5_4_1(unittest.TestCase):\n",
    "\n",
    "    def test_full(self):\n",
    "        mr_job = MrStripes(\n",
    "            args=['testInput_5_4_1.txt', \n",
    "                  '--file', 'vocabTest_5_4_1.txt#vocab.txt', \\\n",
    "                  '-r', 'inline', \n",
    "                  '-q',\n",
    "                  # so options from local mrjob.conf don't pollute the test env.\n",
    "                  '--no-conf', \n",
    "                 ])\n",
    "\n",
    "        results = []\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            for line in runner.stream_output():\n",
    "                results.append(line)\n",
    "\n",
    "        with open(\"testOutput_5_4_1.txt\", \"r\") as f:\n",
    "            for x,y in zip(results, f.readlines()):\n",
    "                self.assertEqual(x.strip(),y.strip())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.runner\"\r\n",
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.033s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python MrStripes_unit_test_5_4_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try on local machine using one input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python MrStripes_5_4_1.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "--file vocab.txt \\\n",
    "-r local \\\n",
    "-q | sort > result_stripes_5_4_1-local.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it on EMR and save to output to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://patng323-w261-hw541/input/vocab.txt\n",
      "delete: s3://patng323-w261-hw541/output/part-00003\n",
      "delete: s3://patng323-w261-hw541/output/part-00005\n",
      "delete: s3://patng323-w261-hw541/output/part-00006\n",
      "delete: s3://patng323-w261-hw541/output/part-00000\n",
      "delete: s3://patng323-w261-hw541/output/part-00001\n",
      "delete: s3://patng323-w261-hw541/output/_SUCCESS \n",
      "delete: s3://patng323-w261-hw541/output/part-00004\n",
      "delete: s3://patng323-w261-hw541/output/part-00002\n",
      "remove_bucket: s3://patng323-w261-hw541/\n",
      "make_bucket: s3://patng323-w261-hw541/\n",
      "upload: ./vocab.txt to s3://patng323-w261-hw541/input/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rb --force s3://patng323-w261-hw541\n",
    "!aws s3 mb s3://patng323-w261-hw541\n",
    "!aws s3 cp vocab.txt s3://patng323-w261-hw541/input/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating new scratch bucket mrjob-cf3b90d8845c25a1\n",
      "using s3://mrjob-cf3b90d8845c25a1/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrStripes_5_4_1.patrickng.20160218.132711.773804\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrStripes_5_4_1.patrickng.20160218.132711.773804/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "creating S3 bucket 'mrjob-cf3b90d8845c25a1' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-cf3b90d8845c25a1/tmp/MrStripes_5_4_1.patrickng.20160218.132711.773804/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-DDQZ0F5H1ACZ\n",
      "Created new job flow j-DDQZ0F5H1ACZ\n",
      "Job launched 32.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 64.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 96.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 128.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 160.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 193.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 225.2s ago, status STARTING: Configuring cluster software\n",
      "Job launched 258.1s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 290.0s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 322.5s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 354.3s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 386.4s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 418.3s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 450.6s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 482.5s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 514.5s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 546.3s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 578.6s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 610.7s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 643.5s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 675.5s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 707.8s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 739.9s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 772.0s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 803.9s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job launched 836.4s ago, status RUNNING: Running step (MrStripes_5_4_1.patrickng.20160218.132711.773804: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 540.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 131055963\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 223285550\n",
      "    FILE_BYTES_WRITTEN: 501125669\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    S3_BYTES_READ: 2156069116\n",
      "    S3_BYTES_WRITTEN: 131055963\n",
      "  Job Counters :\n",
      "    Launched map tasks: 194\n",
      "    Launched reduce tasks: 8\n",
      "    Rack-local map tasks: 192\n",
      "    SLOTS_MILLIS_MAPS: 4872016\n",
      "    SLOTS_MILLIS_REDUCES: 1751423\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 2134240\n",
      "    Combine input records: 25375566\n",
      "    Combine output records: 1673267\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 744168803\n",
      "    Map output materialized bytes: 272446958\n",
      "    Map output records: 25375566\n",
      "    Physical memory (bytes) snapshot: 126367186944\n",
      "    Reduce input groups: 9000\n",
      "    Reduce input records: 1673267\n",
      "    Reduce output records: 9000\n",
      "    Reduce shuffle bytes: 272446958\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 3346534\n",
      "    Total committed heap usage (bytes): 120134828032\n",
      "    Virtual memory (bytes) snapshot: 259752976384\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrStripes_5_4_1.patrickng.20160218.132711.773804\n",
      "Removing all files in s3://mrjob-cf3b90d8845c25a1/tmp/MrStripes_5_4_1.patrickng.20160218.132711.773804/\n",
      "Removing all files in s3://mrjob-cf3b90d8845c25a1/tmp/logs/j-DDQZ0F5H1ACZ/\n",
      "Terminating job flow: j-DDQZ0F5H1ACZ\n"
     ]
    }
   ],
   "source": [
    "!python MrStripes_5_4_1.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "--file 's3://patng323-w261-hw541/input/vocab.txt#vocab.txt' \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.large \\\n",
    "--num-ec2-instances 5 \\\n",
    "--aws-region us-east-1 \\\n",
    "--no-output \\\n",
    "--output-dir=s3://patng323-w261-hw541/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.4(2)\n",
    "\n",
    "Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrComparison_5_4_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrComparison_5_4_2.py\n",
    "\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class MrComparison(MRJob):\n",
    "    # The output from hw5.4.1 is in JSONProtocol\n",
    "    INPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    \n",
    "    METHOD_JACCARD = \"jaccard\"\n",
    "    METHOD_COSINE_SIM = \"cosine_sim\"\n",
    "    COUNT_MARKER = \"*\"\n",
    "\n",
    "    totalCounts = {}\n",
    "\n",
    "    def configure_options(self):\n",
    "        super(MrComparison, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--compareMethod', type='str', default=self.METHOD_JACCARD)\n",
    "        \n",
    "    def load_options(self, args):\n",
    "        super(MrComparison, self).load_options(args)\n",
    "        self.compareMethod = self.options.compareMethod\n",
    "\n",
    "    def step1MapSingleEmit(self, val):\n",
    "        if self.compareMethod == self.METHOD_JACCARD:\n",
    "            return 1\n",
    "        elif self.compareMethod == self.METHOD_COSINE_SIM:\n",
    "            return int(val)**2\n",
    "        else:\n",
    "            raise Exception(\"Unknown compare method:\", self.compareMethod)\n",
    "        \n",
    "    def step1MapPairEmit(self, xVal, yVal):\n",
    "        if self.compareMethod == self.METHOD_JACCARD:\n",
    "            return 1\n",
    "        elif self.compareMethod == self.METHOD_COSINE_SIM:\n",
    "            return int(xVal) * int(yVal)\n",
    "        \n",
    "    def step2Score(self, x, y, val):\n",
    "        if self.compareMethod == self.METHOD_JACCARD:\n",
    "            # score = |A^B| / (|A|+|B|-|A^B|)\n",
    "            # The parameter val is |A^B|\n",
    "            return val / (self.totalCounts[x] + self.totalCounts[y] - val) \n",
    "        elif self.compareMethod == self.METHOD_COSINE_SIM:\n",
    "            # score = A.B / ||A||.||B||\n",
    "            return val / (math.sqrt(self.totalCounts[x])*math.sqrt(self.totalCounts[y]))\n",
    "    \n",
    "    # For each term in the stripe, emit a value, and also a value for each of its pair.\n",
    "    # E.g. for Jaccard, if stripe contains (A:2,B:1,C:2), we'll emit\n",
    "    # *A1, AB1, AC1, *B1, BC1, *C1\n",
    "    def mapper1(self, term, stripe):\n",
    "        # Have to sort it so that we won't get (a,b) and (b,a) at the same time\n",
    "        # otherwise reducer won't work.  They expect both to arrive as (a,b)\n",
    "        keys = sorted(stripe.keys()) \n",
    "        \n",
    "        for i in range(len(keys)):\n",
    "            x = keys[i]\n",
    "            yield self.COUNT_MARKER + x, self.step1MapSingleEmit(stripe[x])\n",
    "            \n",
    "            for j in range(i+1, len(keys)):\n",
    "                y = keys[j]\n",
    "                yield (x, y), self.step1MapPairEmit(stripe[x], stripe[y])\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        if key[0] == self.COUNT_MARKER:\n",
    "            yield key, reduce(lambda x,y: x + y, values) \n",
    "        else:\n",
    "            yield key, reduce(lambda x,y: x + y, values)  \n",
    "    \n",
    "    # Generate the score for each pair of \"doc\"\n",
    "    def reducer2(self, key, values):\n",
    "        if key[0] == self.COUNT_MARKER:\n",
    "            self.totalCounts[key[1:]] = reduce(lambda x,y: x + y, values) \n",
    "        else:\n",
    "            yield key, self.step2Score(key[0], key[1], values.next())\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper1,\n",
    "                combiner=self.reducer1,\n",
    "                reducer=self.reducer1,\n",
    "                ),\n",
    "            MRStep(\n",
    "                reducer=self.reducer2,\n",
    "                jobconf={\n",
    "                    \"mapred.reduce.tasks\":1 \n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrComparison.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Stripe of stripe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrComparison_5_4_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrComparison_5_4_2.py\n",
    "\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def mergeStripes(x,y):\n",
    "    for k,v in y.items():\n",
    "        if k in x:\n",
    "            x[k] += v\n",
    "        else:\n",
    "            x[k] = v\n",
    "    return x    \n",
    "\n",
    "class MrComparison(MRJob):\n",
    "    # The output from hw5.4.1 is in JSONProtocol\n",
    "    INPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    \n",
    "    METHOD_JACCARD = \"jaccard\"\n",
    "    METHOD_COSINE_SIM = \"cosine_sim\"\n",
    "    COUNT_MARKER = \"*\"\n",
    "\n",
    "    totalCounts = {}\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MrComparison, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--compareMethod', type='str', default=self.METHOD_JACCARD)\n",
    "        \n",
    "    def load_options(self, args):\n",
    "        super(MrComparison, self).load_options(args)\n",
    "        self.compareMethod = self.options.compareMethod\n",
    "\n",
    "    def step1MapSingleEmit(self, val):\n",
    "        if self.compareMethod == self.METHOD_JACCARD:\n",
    "            return 1\n",
    "        elif self.compareMethod == self.METHOD_COSINE_SIM:\n",
    "            return int(val)**2\n",
    "        else:\n",
    "            raise Exception(\"Unknown compare method:\", self.compareMethod)\n",
    "        \n",
    "    def step1MapPairUpdateStripe(self, stripe, x, y, xVal, yVal):\n",
    "        if self.compareMethod == self.METHOD_JACCARD:\n",
    "            val = 1\n",
    "        elif self.compareMethod == self.METHOD_COSINE_SIM:\n",
    "            val = int(xVal) * int(yVal)\n",
    "        \n",
    "        assert y not in stripe[x], \"Don't expect y to appear twice under x\"\n",
    "        stripe[x][y] = val\n",
    "        \n",
    "    def step2Score(self, x, y, val):\n",
    "        if self.compareMethod == self.METHOD_JACCARD:\n",
    "            # score = |A^B| / (|A|+|B|-|A^B|)\n",
    "            # The parameter val is |A^B|\n",
    "            return val / (self.totalCounts[x] + self.totalCounts[y] - val) \n",
    "        elif self.compareMethod == self.METHOD_COSINE_SIM:\n",
    "            # score = A.B / ||A||.||B||\n",
    "            return val / (math.sqrt(self.totalCounts[x])*math.sqrt(self.totalCounts[y]))\n",
    "        \n",
    "    # For each term in the stripe, emit a value, and also a value for each of its pair.\n",
    "    # E.g. for Jaccard, if stripe contains (A:2,B:1,C:2), we'll emit\n",
    "    # *A1, AB1, AC1, *B1, BC1, *C1\n",
    "    def mapper1(self, term, postings):\n",
    "        # Have to sort it so that we won't get (a,b) and (b,a) at the same time\n",
    "        # otherwise reducer won't work.  They expect both to arrive as (a,b)\n",
    "        keys = sorted(postings.keys()) \n",
    "        stripe = defaultdict(dict)\n",
    "        \n",
    "        for i in range(len(keys)):\n",
    "            x = keys[i]\n",
    "            yield self.COUNT_MARKER + x, self.step1MapSingleEmit(postings[x])\n",
    "            \n",
    "            for j in range(i+1, len(keys)):\n",
    "                y = keys[j]\n",
    "                self.step1MapPairUpdateStripe(stripe, x, y, postings[x], postings[y])\n",
    "                \n",
    "        # Emit all the stripes\n",
    "        for k, v in stripe.items():\n",
    "            yield k, v\n",
    "\n",
    "    def reducer1(self, key, values):\n",
    "        if key[0] == self.COUNT_MARKER:\n",
    "            yield key, reduce(lambda x,y: x + y, values) \n",
    "        else:\n",
    "            yield key, reduce(lambda x,y: mergeStripes(x,y), values)  \n",
    "    \n",
    "    # Generate the score for each pair of \"doc\"\n",
    "    def reducer2(self, key, values):\n",
    "        if key[0] == self.COUNT_MARKER:\n",
    "            self.totalCounts[key[1:]] = reduce(lambda x,y: x + y, values) \n",
    "        else:\n",
    "            stripe = values.next()\n",
    "            for word, val in stripe.items():\n",
    "                yield (key, word), self.step2Score(key, word, val)\n",
    "                \n",
    "    def reducer3(self, key, values):\n",
    "        for v in values:\n",
    "            yield v, key\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper1,\n",
    "                combiner=self.reducer1,\n",
    "                reducer=self.reducer1,\n",
    "                ),\n",
    "            MRStep(\n",
    "                reducer=self.reducer2,\n",
    "                jobconf={\n",
    "                    \"mapred.reduce.tasks\":1 \n",
    "                    }\n",
    "                ),\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrComparison.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A\", \"C\"]\t0.4\r\n",
      "[\"A\", \"B\"]\t0.6666666666666666\r\n",
      "[\"B\", \"C\"]\t0.2\r\n"
     ]
    }
   ],
   "source": [
    "!python MrComparison_5_4_2.py \\\n",
    "-r inline \\\n",
    "test_5_4_2.inverted.index \\\n",
    "--compareMethod 'jaccard' \\\n",
    "-q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"A\", \"C\"]\t0.5184476595525866\r\n",
      "[\"A\", \"B\"]\t0.7004041959724749\r\n",
      "[\"B\", \"C\"]\t0.9026976808426486\r\n"
     ]
    }
   ],
   "source": [
    "!python MrComparison_5_4_2.py \\\n",
    "-r inline \\\n",
    "test_5_4_2.inverted.index \\\n",
    "--compareMethod 'cosine_sim' \\\n",
    "-q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrComparison_unit_test_5_4_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrComparison_unit_test_5_4_2.py\n",
    "import unittest\n",
    "from MrComparison_5_4_2 import MrComparison\n",
    "\n",
    "class MrComparison_5_4_2(unittest.TestCase):\n",
    "\n",
    "    def test_full(self):\n",
    "        methods = ['jaccard', \"cosine-sim\"]\n",
    "        for method in methods:\n",
    "            print \"Testing method:\", method\n",
    "            results = []\n",
    "            \n",
    "            mr_job = MrComparison(\n",
    "                args=['test_5_4_2.inverted.index', \n",
    "                      '-r', 'inline', \n",
    "                      '--compareMethod', method,\n",
    "                      '-q',\n",
    "                      # so options from local mrjob.conf don't pollute the test env.\n",
    "                      '--no-conf', \n",
    "                     ])\n",
    "\n",
    "            with mr_job.make_runner() as runner:\n",
    "                runner.run()\n",
    "                for line in runner.stream_output():\n",
    "                    results.append(line)\n",
    "\n",
    "            with open(\"testOutput_5_4_2.\" + method + \".txt\", \"r\") as f:\n",
    "                for x,y in zip(results, f.readlines()):\n",
    "                    self.assertEqual(x.strip(),y.strip())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing method: jaccard\n",
      "No handlers could be found for logger \"mrjob.runner\"\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.049s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python MrComparison_unit_test_5_4_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test comparisons with a small index on local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head:\n",
      "[\"alternate\", \"telescope\"]\t1.0\n",
      "[\"amidst\", \"herds\"]\t0.3333333333333333\n",
      "[\"ammonium\", \"soda\"]\t1.0\n",
      "[\"articular\", \"localized\"]\t0.5\n",
      "[\"careless\", \"impatience\"]\t1.0\n",
      "[\"cartilage\", \"discomfort\"]\t0.5\n",
      "[\"ce\", \"est\"]\t1.0\n",
      "[\"ce\", \"commence\"]\t0.5\n",
      "[\"commence\", \"est\"]\t0.5\n",
      "[\"establishments\", \"inspector\"]\t1.0\n",
      "\n",
      "Tail:\n",
      "[\"establishments\", \"inspector\"]\t1.0\n",
      "[\"flocks\", \"restless\"]\t0.5\n",
      "[\"fossil\", \"resembling\"]\t1.0\n",
      "[\"fossil\", \"polished\"]\t1.0\n",
      "[\"intolerable\", \"rejoiced\"]\t1.0\n",
      "[\"palestinian\", \"qui\"]\t0.3333333333333333\n",
      "[\"peritoneal\", \"uterine\"]\t1.0\n",
      "[\"polished\", \"resembling\"]\t1.0\n",
      "[\"replacing\", \"wax\"]\t1.0\n",
      "[\"restless\", \"tumult\"]\t0.5\n"
     ]
    }
   ],
   "source": [
    "!python MrComparison_5_4_2.py \\\n",
    "test_5_4_2-small.inverted.index \\\n",
    "--compareMethod 'jaccard' \\\n",
    "-r local \\\n",
    "-q \\\n",
    "> output_5_4_2-jaccard.txt\n",
    "\n",
    "!echo \"Head:\"\n",
    "!head output_5_4_2-jaccard.txt\n",
    "\n",
    "!echo\n",
    "!echo \"Tail:\"\n",
    "!tail output_5_4_2-jaccard.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head:\n",
      "[\"amidst\", \"herds\"]\t0.020988688248806096\n",
      "[\"palestinian\", \"qui\"]\t0.5491356084120524\n",
      "[\"restless\", \"tumult\"]\t0.7071067811865475\n",
      "[\"cartilage\", \"discomfort\"]\t0.7071067811865476\n",
      "[\"flocks\", \"restless\"]\t0.7071067811865476\n",
      "[\"ce\", \"commence\"]\t0.7127408280944525\n",
      "[\"commence\", \"est\"]\t0.7127408280944525\n",
      "[\"articular\", \"localized\"]\t0.7645223227757786\n",
      "[\"alternate\", \"telescope\"]\t1.0\n",
      "[\"ammonium\", \"soda\"]\t1.0\n",
      "\n",
      "Tail:\n",
      "[\"ammonium\", \"soda\"]\t1.0\n",
      "[\"careless\", \"impatience\"]\t1.0\n",
      "[\"ce\", \"est\"]\t1.0\n",
      "[\"establishments\", \"inspector\"]\t1.0\n",
      "[\"fossil\", \"polished\"]\t1.0\n",
      "[\"fossil\", \"resembling\"]\t1.0\n",
      "[\"intolerable\", \"rejoiced\"]\t1.0\n",
      "[\"peritoneal\", \"uterine\"]\t1.0\n",
      "[\"polished\", \"resembling\"]\t1.0\n",
      "[\"replacing\", \"wax\"]\t1.0\n"
     ]
    }
   ],
   "source": [
    "!python MrComparison_5_4_2.py \\\n",
    "test_5_4_2-small.inverted.index \\\n",
    "--compareMethod 'cosine_sim' \\\n",
    "-r local \\\n",
    "-q \\\n",
    "> output_5_4_2-cosine_sim.txt\n",
    "\n",
    "!echo \"Head:\"\n",
    "!head output_5_4_2-cosine_sim.txt\n",
    "\n",
    "!echo\n",
    "!echo \"Tail:\"\n",
    "!tail output_5_4_2-cosine_sim.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it in EMR and output the result to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_bucket: s3://patng323-w261-hw542-jaccard/\n",
      "make_bucket: s3://patng323-w261-hw542-jaccard/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rb --force s3://patng323-w261-hw542-jaccard\n",
    "!aws s3 mb s3://patng323-w261-hw542-jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating new scratch bucket mrjob-47453eb3eb0cb961\n",
      "using s3://mrjob-47453eb3eb0cb961/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrComparison_5_4_2.patrickng.20160219.180217.803509\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrComparison_5_4_2.patrickng.20160219.180217.803509/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "creating S3 bucket 'mrjob-47453eb3eb0cb961' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-47453eb3eb0cb961/tmp/MrComparison_5_4_2.patrickng.20160219.180217.803509/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-3AGH8HFUJVOOJ\n",
      "Setting EMR tags: job=HW542_Jaccard\n",
      "Created new job flow j-3AGH8HFUJVOOJ\n",
      "Job launched 31.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 69.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 102.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 134.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 166.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 199.0s ago, status STARTING: Configuring cluster software\n",
      "Job launched 230.8s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job completed.\n",
      "Running time was 5361.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 131469979\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 639901323\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 33815568966\n",
      "    FILE_BYTES_WRITTEN: 41722857980\n",
      "    HDFS_BYTES_READ: 2660\n",
      "    HDFS_BYTES_WRITTEN: 639901323\n",
      "    S3_BYTES_READ: 131469979\n",
      "  Job Counters :\n",
      "    Launched map tasks: 29\n",
      "    Launched reduce tasks: 8\n",
      "    Rack-local map tasks: 29\n",
      "    SLOTS_MILLIS_MAPS: 43491591\n",
      "    SLOTS_MILLIS_REDUCES: 9316409\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 19525150\n",
      "    Combine input records: 23745683\n",
      "    Combine output records: 7380300\n",
      "    Map input bytes: 131055963\n",
      "    Map input records: 9000\n",
      "    Map output bytes: 82915019160\n",
      "    Map output materialized bytes: 8011701404\n",
      "    Map output records: 16493808\n",
      "    Physical memory (bytes) snapshot: 11821281280\n",
      "    Reduce input groups: 17999\n",
      "    Reduce input records: 128425\n",
      "    Reduce output records: 17999\n",
      "    Reduce shuffle bytes: 8011701404\n",
      "    SPLIT_RAW_BYTES: 2660\n",
      "    Spilled Records: 7421236\n",
      "    Total committed heap usage (bytes): 8279031808\n",
      "    Virtual memory (bytes) snapshot: 46577238016\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 641565659\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 1805085243\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 384572340\n",
      "    FILE_BYTES_WRITTEN: 769931207\n",
      "    HDFS_BYTES_READ: 641570223\n",
      "    S3_BYTES_WRITTEN: 1805085243\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 28\n",
      "    Launched map tasks: 28\n",
      "    Launched reduce tasks: 1\n",
      "    SLOTS_MILLIS_MAPS: 230416\n",
      "    SLOTS_MILLIS_REDUCES: 1126200\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 1325590\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 639901323\n",
      "    Map input records: 17999\n",
      "    Map output bytes: 639924136\n",
      "    Map output materialized bytes: 384586288\n",
      "    Map output records: 17999\n",
      "    Physical memory (bytes) snapshot: 12869812224\n",
      "    Reduce input groups: 17999\n",
      "    Reduce input records: 17999\n",
      "    Reduce output records: 40483491\n",
      "    Reduce shuffle bytes: 384586288\n",
      "    SPLIT_RAW_BYTES: 4564\n",
      "    Spilled Records: 35998\n",
      "    Total committed heap usage (bytes): 12909019136\n",
      "    Virtual memory (bytes) snapshot: 38260236288\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrComparison_5_4_2.patrickng.20160219.180217.803509\n",
      "Removing all files in s3://mrjob-47453eb3eb0cb961/tmp/MrComparison_5_4_2.patrickng.20160219.180217.803509/\n",
      "Removing all files in s3://mrjob-47453eb3eb0cb961/tmp/logs/j-3AGH8HFUJVOOJ/\n",
      "Terminating job flow: j-3AGH8HFUJVOOJ\n"
     ]
    }
   ],
   "source": [
    "!python MrComparison_5_4_2.py \\\n",
    "s3://patng323-w261-hw541/output/ \\\n",
    "-r emr \\\n",
    "--compareMethod 'jaccard' \\\n",
    "--ec2-instance-type m1.large \\\n",
    "--num-ec2-instances 5 \\\n",
    "--aws-region us-east-1 \\\n",
    "--ec2-key-pair w261 \\\n",
    "--ec2-key-pair-file /Users/patrickng/.ssh/w261.pem \\\n",
    "--output-dir=s3://patng323-w261-hw542-jaccard/output/ \\\n",
    "--emr-tag job='HW542_Jaccard' \\\n",
    "--no-output \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_bucket: s3://patng323-w261-hw542-cosine/\n",
      "make_bucket: s3://patng323-w261-hw542-cosine/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rb --force s3://patng323-w261-hw542-cosine\n",
    "!aws s3 mb s3://patng323-w261-hw542-cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating new scratch bucket mrjob-33908752ab4786c3\n",
      "using s3://mrjob-33908752ab4786c3/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrComparison_5_4_2.patrickng.20160220.005132.391897\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrComparison_5_4_2.patrickng.20160220.005132.391897/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "creating S3 bucket 'mrjob-33908752ab4786c3' to use as scratch space\n",
      "Copying non-input files into s3://mrjob-33908752ab4786c3/tmp/MrComparison_5_4_2.patrickng.20160220.005132.391897/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-3MIQ6JEF9ABSB\n",
      "Setting EMR tags: job=HW542_Cosine\n",
      "Created new job flow j-3MIQ6JEF9ABSB\n",
      "Job launched 49.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 82.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 114.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 146.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 178.6s ago, status STARTING: Configuring cluster software\n",
      "Job launched 285.2s ago, status STARTING: Configuring cluster software\n",
      "Job launched 317.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 349.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 380.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 413.2s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 478.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 586.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 618.2s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 685.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 717.4s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 749.6s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 798.8s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 831.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 863.5s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 898.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 931.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 987.2s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1019.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1051.4s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1084.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1115.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1148.6s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1181.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1214.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1245.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1278.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1310.5s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1343.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1375.6s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1411.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1443.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1475.7s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1516.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1548.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1599.2s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1631.7s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1663.7s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1697.2s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1729.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1836.8s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1869.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1901.8s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1934.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1966.8s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 1999.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2032.5s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2064.5s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2097.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2129.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2161.2s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2228.6s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2260.8s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2293.8s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2358.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2391.5s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2423.4s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2459.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2492.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2599.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2631.2s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2697.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2738.2s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2770.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2820.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2852.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2884.6s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2916.6s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 2949.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3052.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3104.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3137.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3243.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3275.2s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3307.5s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3339.4s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3372.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3403.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3537.8s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3569.6s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 1 of 2)\n",
      "Job launched 3612.4s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 3644.5s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 3677.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 3709.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 3816.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 3849.6s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 3882.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 3945.4s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4010.7s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4053.5s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4158.6s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4190.5s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4222.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4255.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4292.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4324.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4356.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4387.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4424.5s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4456.3s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4536.7s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4576.9s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4609.6s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4642.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4674.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4706.2s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4739.2s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4771.7s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4804.0s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job launched 4840.1s ago, status RUNNING: Running step (MrComparison_5_4_2.patrickng.20160220.005132.391897: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 4551.0s (not counting time spent waiting for the EC2 instances)\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Terminating job flow: j-3MIQ6JEF9ABSB\n",
      "Traceback (most recent call last):\n",
      "  File \"MrComparison_5_4_2.py\", line 119, in <module>\n",
      "    MrComparison.run()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/job.py\", line 461, in run\n",
      "    mr_job.execute()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/job.py\", line 479, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/launch.py\", line 153, in execute\n",
      "    self.run_job()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/launch.py\", line 216, in run_job\n",
      "    runner.run()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/runner.py\", line 470, in run\n",
      "    self._run()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/emr.py\", line 883, in _run\n",
      "    self._wait_for_job_to_complete()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/emr.py\", line 1728, in _wait_for_job_to_complete\n",
      "    self._fetch_counters(step_nums, lg_step_num_mapping)\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/emr.py\", line 1944, in _fetch_counters\n",
      "    lg_step_nums, skip_s3_wait)\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/emr.py\", line 1973, in _fetch_counters_s3\n",
      "    self.get_hadoop_version())\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/logparsers.py\", line 224, in scan_for_counters_in_files\n",
      "    for log_file_uri in log_file_uris:\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/emr.py\", line 1794, in _enforce_path_regexp\n",
      "    for path in paths:\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/fs/s3.py\", line 142, in ls\n",
      "    for uri in self._s3_ls(base_uri):\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/fs/s3.py\", line 157, in _s3_ls\n",
      "    for key in bucket.list(key_name):\n",
      "  File \"/Library/Python/2.7/site-packages/boto/s3/bucketlistresultset.py\", line 34, in bucket_lister\n",
      "    encoding_type=encoding_type)\n",
      "  File \"/Library/Python/2.7/site-packages/boto/s3/bucket.py\", line 473, in get_all_keys\n",
      "    '', headers, **params)\n",
      "  File \"/Library/Python/2.7/site-packages/boto/s3/bucket.py\", line 400, in _get_all\n",
      "    body = response.read()\n",
      "  File \"/Library/Python/2.7/site-packages/boto/connection.py\", line 410, in read\n",
      "    self._cached_response = http_client.HTTPResponse.read(self)\n",
      "  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py\", line 588, in read\n",
      "    return self._read_chunked(amt)\n",
      "  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py\", line 630, in _read_chunked\n",
      "    line = self.fp.readline(_MAXLINE + 1)\n",
      "  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.py\", line 480, in readline\n",
      "    data = self._sock.recv(self._rbufsize)\n",
      "  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ssl.py\", line 734, in recv\n",
      "    return self.read(buflen)\n",
      "  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ssl.py\", line 621, in read\n",
      "    v = self._sslobj.read(len or 1024)\n",
      "ssl.SSLError: ('The read operation timed out',)\n"
     ]
    }
   ],
   "source": [
    "!python MrComparison_5_4_2.py \\\n",
    "s3://patng323-w261-hw541/output/ \\\n",
    "-r emr \\\n",
    "--compareMethod 'cosine_sim' \\\n",
    "--ec2-instance-type m1.large \\\n",
    "--num-ec2-instances 9 \\\n",
    "--aws-region us-east-1 \\\n",
    "--output-dir=s3://patng323-w261-hw542-cosine/output/ \\\n",
    "--ec2-key-pair w261 \\\n",
    "--ec2-key-pair-file /Users/patrickng/.ssh/w261.pem \\\n",
    "--no-output \\\n",
    "--emr-tag job='HW542_Cosine' \\\n",
    "    \n",
    "#!aws s3 cp s3://patng323-w261-hw542-test/output/part-00000 output_5_4_2-jaccard-emr.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download results from both runs\n",
    "!aws s3 cp s3://patng323-w261-hw542-jaccard/output/part-00000 result-5_4_2-jaccard.txt\n",
    "!aws s3 cp s3://patng323-w261-hw542-cosine/output/part-00000 result-5_4_2-cosine.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5 Evaluation of synonyms that your discovered\n",
    "```\n",
    "In this part of the assignment you will evaluate the success of you synonym detector.\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined\n",
    "by your measure in (2), and use the synonyms function in the accompanying\n",
    "python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw_5_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw_5_5.py\n",
    "#!/usr/bin/python2.7\n",
    "from __future__ import division\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "''' pass a string to this funciton ( eg 'car') and it will give you a list of\n",
    "words which is related to cat, called lemma of CAT. '''\n",
    "#print all the synset element of an element\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "\n",
    "# In the top 1000 records, for each word, build up its own synonym list by\n",
    "# including all the words which appear as similar.\n",
    "m = re.compile(r\"\\[\\\"(\\S+)\\\"\\,\\s*\\\"(\\S+)\\\"\\].+\")\n",
    "n = 1000\n",
    "simResults = defaultdict(list)\n",
    "vocab = set()\n",
    "\n",
    "simResultFile = sys.argv[1]\n",
    "vocabFile = sys.argv[2]\n",
    "\n",
    "with open(simResultFile, \"r\") as f:\n",
    "    for i in range(n):\n",
    "        line = f.readline()\n",
    "        matched = m.match(line)\n",
    "        word1 = matched.group(1)\n",
    "        word2 = matched.group(2)\n",
    "\n",
    "        # Store the synonyms of word1, and vice-versa for word2\n",
    "        simResults[word1].append(word2)\n",
    "        simResults[word2].append(word1)\n",
    "\n",
    "with open(vocabFile, \"r\") as f:\n",
    "    for word in f.readlines():\n",
    "        vocab.add(word.strip())\n",
    "    \n",
    "# Now we have the list of top words and the synonyms of each word.\n",
    "\n",
    "# Calculate the precision, recall and f1 of each word\n",
    "def calStats(word, similarWords):\n",
    "    # We will use only those synonyms listed in our vocab.\n",
    "    syns = set(synonyms(word)) & vocab\n",
    "    \n",
    "    correct = 0\n",
    "    for word in similarWords:\n",
    "        if word in syns:\n",
    "            correct += 1\n",
    "\n",
    "    precision = correct / len(similarWords)\n",
    "    recall = 0 if len(syns) == 0 else correct / len(syns)\n",
    "    if (precision + recall) == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "precisionTotal = 0\n",
    "recallTotal = 0\n",
    "f1Total = 0\n",
    "\n",
    "print \"From:\", simResultFile\n",
    "print \"-----------------------------\"\n",
    "print \"Word, precision, recall, F1\"\n",
    "for word, similarWords in simResults.items():\n",
    "    precision, recall, f1 = calStats(word, similarWords)\n",
    "    precisionTotal += precision\n",
    "    recallTotal += recall\n",
    "    f1Total += f1\n",
    "    print \"Word:\", word, \"\\t\\tStats:\", (precision, recall, f1)\n",
    "\n",
    "print\n",
    "print \"Summary:\"\n",
    "print \"-----------------------------\"\n",
    "print \"Precision average:\", precisionTotal / len(simResults)\n",
    "print \"Recall average:\", recallTotal / len(simResults)\n",
    "print \"F1 average:\", f1Total / len(simResults)\n",
    "\n",
    "print \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it for the Jaccard result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the result from HW5.4(2)\n",
    "!sort -t$'\\t' -k2,2gr result-5_4_2-jaccard.txt > result-5_4_2-jaccard-sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python hw_5_5.py result-5_4_2-jaccard-sorted.txt vocab.txt > result-hw_5_5-jaccard.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run it for the Cosine result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the result from HW5.4(2)\n",
    "!sort -t$'\\t' -k2,2gr result-5_4_2-cosine.txt > result-5_4_2-jaccard-cosine.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python hw_5_5.py result-5_4_2-cosine-sorted.txt vocab.txt > result-hw_5_5-cosine.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
