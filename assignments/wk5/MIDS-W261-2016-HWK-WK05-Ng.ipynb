{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.1\n",
    "In the database world What is 3NF? Does machine learning use data in 3NF? If so why?   \n",
    "In what form does ML consume data?  \n",
    "Why would one use log files that are denormalized?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. \n",
    "\n",
    "Run your code on the data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file)). \n",
    "\n",
    "In this output please include the webpage URL, webpageID and Visitor ID.:\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "1. Left joining Table Left with Table Right\n",
    "2. Right joining Table Left with Table Right\n",
    "3. Inner joining Table Left with Table Right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MRJoin_5_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MRJoin_5_2.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "    urls = {} # key = pageId, value = url\n",
    "    keys_emitted = set() # Set of keys of all emitted urls. Used for left join.\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MRJoin, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--joinType', type='str', default=\"inner\")\n",
    "        \n",
    "    def load_options(self, args):\n",
    "        super(MRJoin, self).load_options(args)\n",
    "        self.joinType = self.options.joinType\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        # Load URL info data file into memory.  \n",
    "        # Line format: \n",
    "        # 1287,/autoroute\n",
    "        with open(\"processed_urls.data\", \"r\") as f:\n",
    "            for fields in csv.reader(f):\n",
    "                self.urls[fields[0]] = fields[1]\n",
    "\n",
    "    def mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # V,1000,1,C,10001\n",
    "        fields = csv.reader([line]).next()\n",
    "        \n",
    "        key = fields[1]\n",
    "        url = None\n",
    "        toEmit = False\n",
    "        \n",
    "        if key in self.urls:\n",
    "            url = self.urls[key]\n",
    "            \n",
    "        if self.joinType == \"right\":\n",
    "            toEmit = True\n",
    "        elif self.joinType == \"left\":\n",
    "            if url is not None:\n",
    "                toEmit = True\n",
    "                self.keys_emitted.add(key) # Remember what we have emitted\n",
    "        else: # inner join\n",
    "            if url is not None:\n",
    "                toEmit = True\n",
    "        \n",
    "        if toEmit:\n",
    "            # Output format\n",
    "            # pageid, url,V,1,C,10001\n",
    "            yield key, (url, fields[0], fields[2], fields[3], fields[4])\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        if self.joinType == \"left\":\n",
    "            # Emit all the remaining urls\n",
    "            remaining = set(self.urls.keys()) - self.keys_emitted\n",
    "            for key in remaining:\n",
    "                yield key, (self.urls[key], None, None, None, None)\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   mapper_final=self.mapper_final)\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join type:left\n",
      "Number of records:98663\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n",
      "Join type:right\n",
      "Number of records:98654\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n",
      "Join type:inner\n",
      "Number of records:98654\n",
      "First 5 lines:\n",
      "\"1000\"\t[\"/regwiz\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1002\"\t[\"/athome\", \"V\", \"1\", \"C\", \"10001\"]\n",
      "\"1001\"\t[\"/support\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\"1003\"\t[\"/kb\", \"V\", \"1\", \"C\", \"10002\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from MRJoin_5_2 import MRJoin\n",
    "\n",
    "for joinType in [\"left\", \"right\", \"inner\"]:\n",
    "    mr_job = MRJoin(args=['processed_anonymous-msweb.data', \n",
    "                        '--file', 'processed_urls.data', # broadcast to every mapper\n",
    "                        \"--strict-protocols\",\n",
    "                        '--joinType', joinType])\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "\n",
    "        lines = []\n",
    "        for line in runner.stream_output():\n",
    "            lines.append(line)\n",
    "            \n",
    "        print \"Join type:\" + joinType\n",
    "        print \"Number of records:\" + str(len(lines))\n",
    "        print \"First 5 lines:\"\n",
    "        for i in range(5):\n",
    "            print lines[i].strip()\n",
    "            \n",
    "        print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW 5.3  EDA of Google n-grams dataset\n",
    "For the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:  \n",
    "``googlebooks-eng-all-5gram-20090715-0-filtered.txt``\n",
    "\n",
    "Finally show your results on the Google n-grams dataset. \n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "``\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)``\n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrLongest_5_3a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrLongest_5_3a.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Find Longest 5-gram (number of characters)\n",
    "class MrLongest(MRJob):\n",
    "    def mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "        fields = csv.reader([line], delimiter='\\t').next()        \n",
    "        ngram = fields[0]\n",
    "        yield len(ngram), ngram\n",
    "\n",
    "    def reducer_init(self):\n",
    "        self.emitted = False\n",
    "        \n",
    "    def reducer(self, length, values):\n",
    "        # We only need to emit the first one, which is the longest for this reducer\n",
    "        if not self.emitted:\n",
    "            self.emitted = True\n",
    "            ngrams = [ngram for ngram in values]\n",
    "            yield length, ngrams\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper,\n",
    "                reducer_init=self.reducer_init,\n",
    "                reducer=self.reducer,\n",
    "                # First key is length; sort it in reverse order\n",
    "                jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"1\",\n",
    "                    \"mapred.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapred.text.key.comparator.options\":\"-k1,1nr\"\n",
    "                          }                \n",
    "                  )\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrLongest.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare test file\n",
    "!head -n 10 googlebooks-eng-all-5gram-20090715-0-filtered.txt > testData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing unitTest_5_3a.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unitTest_5_3a.py\n",
    "import unittest\n",
    "from MrLongest_5_3a import MrLongest\n",
    "\n",
    "class UnitTest_5_3(unittest.TestCase):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnitTest_5_3, self).__init__(*args, **kwargs)\n",
    "        with open('testData.txt', 'r') as f:\n",
    "            self.first_line = f.readline()\n",
    "            self.first_ngram = self.first_line.split('\\t')[0]\n",
    "        \n",
    "    def test_MrLongest_mapper(self):\n",
    "        j = MrLongest()\n",
    "        self.assertEqual(j.mapper(None, self.first_line).next(), \n",
    "                         (len(self.first_ngram), self.first_ngram))\n",
    "        \n",
    "    def test_MrLongest_reducer(self):\n",
    "        j = MrLongest()\n",
    "        ngrams = [\"0123456789\", \"A12345678B\"]\n",
    "        length = len(ngrams[0])\n",
    "        \n",
    "        j.reducer_init()\n",
    "        self.assertEqual(j.reducer(length, ngrams).next(), (length, ngrams))\n",
    "\n",
    "        # We only output the first one.\n",
    "        with self.assertRaises(StopIteration):\n",
    "            j.reducer(length, ngrams).next()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 2 tests in 0.004s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python unitTest_5_3a.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fullTest_5_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fullTest_5_3.py\n",
    "import unittest\n",
    "from MrLongest_5_3a import MrLongest\n",
    "\n",
    "class FullTest_5_3a(unittest.TestCase):\n",
    "\n",
    "    def test_full(self):\n",
    "        mr_job = MrLongest(\n",
    "            args=['testData.txt', \n",
    "                  # Have to use Hadoop, otherwise custom sort order won't work.\n",
    "                  '-r', 'hadoop', \n",
    "                  '--strict-protocols',\n",
    "                  # so options from local mrjob.conf don't pollute the test env.\n",
    "                  '--no-conf', \n",
    "                 ])\n",
    "\n",
    "        results = []\n",
    "        with mr_job.make_runner() as runner:\n",
    "            runner.run()\n",
    "            for line in runner.stream_output():\n",
    "                # Use the job's specified protocol to read the output\n",
    "                key, value = mr_job.parse_output_line(line)\n",
    "                results.append((key, value))\n",
    "\n",
    "        self.assertEqual(len(results), 1)\n",
    "        self.assertEqual(results[0], \n",
    "                (33, ['A Circumstantial Narrative of the', 'A BILL FOR ESTABLISHING RELIGIOUS']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.compat\"\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 37.503s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "!python fullTest_5_3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it on a local hadoop, using just one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.110146.825546\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.110146.825546/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/MrLongest_5_3a.patrickng.20160215.110146.825546/files/\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar963639743509190685/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob3259114538871942951.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/MrLongest_5_3a.patrickng.20160215.110146.825546/output\n",
      "58\t[\"Hydroxytryptamine stimulates inositol phosphate production\", \"Interpersonal Communication Interpersonal communication is\"]\n",
      "STDERR: 16/02/15 19:02:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.110146.825546\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/MrLongest_5_3a.patrickng.20160215.110146.825546 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python MrLongest_5_3a.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "--jobconf mapred.map.tasks=3 --jobconf mapred.reduce.tasks=1 \\\n",
    "-r hadoop \\\n",
    "--strict-protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it for real on EMR, using all data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-2c54d3a4b9812930\n",
      "using s3://mrjob-2c54d3a4b9812930/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.111635.198877\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.111635.198877/b.py\n",
      "Copying non-input files into s3://mrjob-2c54d3a4b9812930/tmp/MrLongest_5_3a.patrickng.20160215.111635.198877/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-2X8QEUZQA1863\n",
      "Setting EMR tags: job=hw5_3a\n",
      "Created new job flow j-2X8QEUZQA1863\n",
      "Job launched 32.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 65.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 97.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 129.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 162.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 194.3s ago, status STARTING: Configuring cluster software\n",
      "Job launched 226.2s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 258.5s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 290.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 323.1s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 355.3s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 387.5s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 419.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 452.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 484.2s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 516.7s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 549.1s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 581.3s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 613.1s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 645.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 678.5s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 710.6s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 742.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 774.8s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 807.2s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 839.8s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 871.7s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 903.5s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 935.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 968.3s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1000.0s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1031.9s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1064.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job launched 1096.4s ago, status RUNNING: Running step (MrLongest_5_3a.patrickng.20160215.111635.198877: Step 1 of 1)\n",
      "Job completed.\n",
      "Running time was 825.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 331\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 1021526088\n",
      "    FILE_BYTES_WRITTEN: 2048946252\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    S3_BYTES_READ: 2156069116\n",
      "    S3_BYTES_WRITTEN: 331\n",
      "  Job Counters :\n",
      "    Launched map tasks: 196\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 194\n",
      "    SLOTS_MILLIS_MAPS: 4485825\n",
      "    SLOTS_MILLIS_REDUCES: 723518\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 2459090\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 1862272363\n",
      "    Map output materialized bytes: 1022202521\n",
      "    Map output records: 58682266\n",
      "    Physical memory (bytes) snapshot: 126753538048\n",
      "    Reduce input groups: 80\n",
      "    Reduce input records: 58682266\n",
      "    Reduce output records: 1\n",
      "    Reduce shuffle bytes: 1022202521\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 117364532\n",
      "    Total committed heap usage (bytes): 123887681536\n",
      "    Virtual memory (bytes) snapshot: 234537271296\n",
      "Streaming final output from s3://mrjob-2c54d3a4b9812930/tmp/MrLongest_5_3a.patrickng.20160215.111635.198877/output/\n",
      "159\t[\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\", \"ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\"]\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrLongest_5_3a.patrickng.20160215.111635.198877\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/MrLongest_5_3a.patrickng.20160215.111635.198877/\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/logs/j-2X8QEUZQA1863/\n",
      "Terminating job flow: j-2X8QEUZQA1863\n"
     ]
    }
   ],
   "source": [
    "!python MrLongest_5_3a.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "--jobconf mapred.map.tasks=28 --jobconf mapred.reduce.tasks=1 \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.xlarge \\\n",
    "--num-ec2-instances 4 \\\n",
    "--emr-tag job=hw5_3a \\\n",
    "--strict-protocols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "Longest 5-gram:  \n",
    "Length = 159\t\n",
    "\n",
    "\"AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR\"  \n",
    "\n",
    "\"ROPLEZIMPREDASTRODONBRASLPKLSON YHROACLMPARCHEYXMMIOUDAVESAURUS PIOFPILOCOWERSURUASOGETSESNEGCP TYRAVOPSIFENGOQUAPIALLOBOSKENUO OWINFUYAIOKENECKSASXHYILPOYNUAT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrFrequent_5_3b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrFrequent_5_3b.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "class MrFrequent(MRJob):\n",
    "    emitted = 0\n",
    "    \n",
    "    def word_count_mapper(self, line_no, line):\n",
    "        # Line format: \n",
    "        # (ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "        fields = csv.reader([line], delimiter='\\t').next()        \n",
    "        count = int(fields[1])\n",
    "        \n",
    "        for word in fields[0].split():\n",
    "            yield word, count\n",
    "\n",
    "    def sum_word_count_reducer(self, key, counts):\n",
    "        yield key, sum([count for count in counts])\n",
    "        \n",
    "    def mapper(self, key, line):\n",
    "        yield key, line\n",
    "    \n",
    "    def top10_words_reducer(self, key, counts):\n",
    "        if self.emitted < 10:\n",
    "            self.emitted += 1\n",
    "            yield key, counts.next()\n",
    "                                \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.word_count_mapper,\n",
    "                combiner=self.sum_word_count_reducer,\n",
    "                reducer=self.sum_word_count_reducer,\n",
    "                jobconf={\n",
    "                    \"mapred.map.tasks\":28,\n",
    "                    \"mapred.reduce.tasks\":28\n",
    "                    }\n",
    "                ),\n",
    "            MRStep(\n",
    "                reducer=self.top10_words_reducer,\n",
    "                jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"2\",\n",
    "                    \"mapred.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapred.text.key.comparator.options\":\"-k2,2nr\",\n",
    "                    \"mapred.map.tasks\":28,\n",
    "                    \"mapred.reduce.tasks\":1\n",
    "                          }                \n",
    "                )\n",
    "            ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrFrequent.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting unitTest_5_3b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile unitTest_5_3b.py\n",
    "import unittest\n",
    "from MrFrequent_5_3b import MrFrequent\n",
    "\n",
    "class UnitTest_5_3b(unittest.TestCase):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(UnitTest_5_3b, self).__init__(*args, **kwargs)\n",
    "        with open('testData.txt', 'r') as f:\n",
    "            self.first_line = f.readline()\n",
    "            fields = self.first_line.split('\\t')\n",
    "            self.first_ngram = fields[0]\n",
    "            self.count = int(fields[1])\n",
    "            self.first_words = self.first_ngram.split()\n",
    "        \n",
    "    def test_MrFrequent_mapper1(self):\n",
    "        j = MrFrequent()\n",
    "        self.assertEqual(j.word_count_mapper(None, self.first_line).next(), \n",
    "                         (self.first_words[0], self.count))\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.002s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python unitTest_5_3b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try it on local hadoop, using one data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160215.122152.986329\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160215.122152.986329/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/MrFrequent_5_3b.patrickng.20160215.122152.986329/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar878521631538201285/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob6367045769796499102.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.7.1:\n",
      "The have been translated as follows\n",
      " mapred.map.tasks: mapreduce.job.maps\n",
      "mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar237924707015525458/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob1791979672381209302.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/MrFrequent_5_3b.patrickng.20160215.122152.986329/output\n",
      "\"the\"\t27050866\n",
      "\"of\"\t18521702\n",
      "\"to\"\t11575700\n",
      "\"in\"\t7342789\n",
      "\"a\"\t6810203\n",
      "\"and\"\t6049329\n",
      "\"is\"\t4067187\n",
      "\"that\"\t4052631\n",
      "\"be\"\t3716938\n",
      "\"was\"\t2490434\n",
      "STDERR: 16/02/15 20:28:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160215.122152.986329\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/MrFrequent_5_3b.patrickng.20160215.122152.986329 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python MrFrequent_5_3b.py \\\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "-r hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "using existing scratch bucket mrjob-2c54d3a4b9812930\n",
      "using s3://mrjob-2c54d3a4b9812930/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160215.122952.205418\n",
      "writing master bootstrap script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160215.122952.205418/b.py\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "Copying non-input files into s3://mrjob-2c54d3a4b9812930/tmp/MrFrequent_5_3b.patrickng.20160215.122952.205418/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Can't access IAM API, trying default instance profile: EMR_EC2_DefaultRole\n",
      "Can't access IAM API, trying default service role: EMR_DefaultRole\n",
      "Job flow created with ID: j-Q2HU92K7LDQN\n",
      "Setting EMR tags: job=hw5_3a\n",
      "Created new job flow j-Q2HU92K7LDQN\n",
      "Job launched 31.8s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 64.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 95.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 128.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 160.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 192.5s ago, status STARTING: Configuring cluster software\n",
      "Job launched 224.3s ago, status STARTING: Configuring cluster software\n",
      "Job launched 257.4s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 289.7s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 322.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 354.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 386.9s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 419.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 451.9s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 484.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 516.6s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 549.4s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 581.6s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 616.9s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 649.0s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 681.8s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 714.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 747.0s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 780.4s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 812.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 844.4s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 876.3s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 909.0s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 940.8s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 973.1s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 1005.2s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 1037.9s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 1069.9s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 1102.3s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 1134.3s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 1166.8s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 1198.7s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 1230.9s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 1262.7s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 1295.5s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 1 of 2)\n",
      "Job launched 1327.3s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 2 of 2)\n",
      "Job launched 1359.5s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 2 of 2)\n",
      "Job launched 1391.7s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 2 of 2)\n",
      "Job launched 1424.6s ago, status RUNNING: Running step (MrFrequent_5_3b.patrickng.20160215.122952.205418: Step 2 of 2)\n",
      "Job completed.\n",
      "Running time was 1120.0s (not counting time spent waiting for the EC2 instances)\n",
      "ec2_key_pair_file not specified, going to S3\n",
      "Fetching counters from S3...\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Counters from step 1:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 2156069116\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 5251252\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 995254603\n",
      "    FILE_BYTES_WRITTEN: 342232694\n",
      "    HDFS_BYTES_READ: 23640\n",
      "    HDFS_BYTES_WRITTEN: 5251252\n",
      "    S3_BYTES_READ: 2156069116\n",
      "  Job Counters :\n",
      "    Launched map tasks: 193\n",
      "    Launched reduce tasks: 35\n",
      "    Rack-local map tasks: 191\n",
      "    SLOTS_MILLIS_MAPS: 28825035\n",
      "    SLOTS_MILLIS_REDUCES: 9442477\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 7249400\n",
      "    Combine input records: 307583853\n",
      "    Combine output records: 22094566\n",
      "    Map input bytes: 2156069116\n",
      "    Map input records: 58682266\n",
      "    Map output bytes: 3136729760\n",
      "    Map output materialized bytes: 104167724\n",
      "    Map output records: 293411330\n",
      "    Physical memory (bytes) snapshot: 115724509184\n",
      "    Reduce input groups: 343019\n",
      "    Reduce input records: 7922043\n",
      "    Reduce output records: 343019\n",
      "    Reduce shuffle bytes: 104167724\n",
      "    SPLIT_RAW_BYTES: 23640\n",
      "    Spilled Records: 30016609\n",
      "    Total committed heap usage (bytes): 115589775360\n",
      "    Virtual memory (bytes) snapshot: 269438418944\n",
      "Counters from step 2:\n",
      "  File Input Format Counters :\n",
      "    Bytes Read: 5251252\n",
      "  File Output Format Counters :\n",
      "    Bytes Written: 159\n",
      "  FileSystemCounters:\n",
      "    FILE_BYTES_READ: 3940081\n",
      "    FILE_BYTES_WRITTEN: 9053329\n",
      "    HDFS_BYTES_READ: 5255788\n",
      "    S3_BYTES_WRITTEN: 159\n",
      "  Job Counters :\n",
      "    Data-local map tasks: 26\n",
      "    Launched map tasks: 28\n",
      "    Launched reduce tasks: 1\n",
      "    Rack-local map tasks: 2\n",
      "    SLOTS_MILLIS_MAPS: 479360\n",
      "    SLOTS_MILLIS_REDUCES: 20569\n",
      "    Total time spent by all maps waiting after reserving slots (ms): 0\n",
      "    Total time spent by all reduces waiting after reserving slots (ms): 0\n",
      "  Map-Reduce Framework:\n",
      "    CPU time spent (ms): 69710\n",
      "    Combine input records: 0\n",
      "    Combine output records: 0\n",
      "    Map input bytes: 5251252\n",
      "    Map input records: 343019\n",
      "    Map output bytes: 5594271\n",
      "    Map output materialized bytes: 4321713\n",
      "    Map output records: 343019\n",
      "    Physical memory (bytes) snapshot: 10226733056\n",
      "    Reduce input groups: 343019\n",
      "    Reduce input records: 343019\n",
      "    Reduce output records: 10\n",
      "    Reduce shuffle bytes: 4321713\n",
      "    SPLIT_RAW_BYTES: 4536\n",
      "    Spilled Records: 686038\n",
      "    Total committed heap usage (bytes): 12622233600\n",
      "    Virtual memory (bytes) snapshot: 35388805120\n",
      "Streaming final output from s3://mrjob-2c54d3a4b9812930/tmp/MrFrequent_5_3b.patrickng.20160215.122952.205418/output/\n",
      "\"the\"\t5375699242\n",
      "\"of\"\t3691308874\n",
      "\"to\"\t2221164346\n",
      "\"in\"\t1387638591\n",
      "\"a\"\t1342195425\n",
      "\"and\"\t1135779433\n",
      "\"that\"\t798553959\n",
      "\"is\"\t756296656\n",
      "\"be\"\t688053106\n",
      "\"as\"\t481373389\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/MrFrequent_5_3b.patrickng.20160215.122952.205418\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/MrFrequent_5_3b.patrickng.20160215.122952.205418/\n",
      "Removing all files in s3://mrjob-2c54d3a4b9812930/tmp/logs/j-Q2HU92K7LDQN/\n",
      "Terminating job flow: j-Q2HU92K7LDQN\n"
     ]
    }
   ],
   "source": [
    "!python MrFrequent_5_3b.py \\\n",
    "s3://filtered-5grams/ \\\n",
    "-r emr \\\n",
    "--ec2-instance-type m1.xlarge \\\n",
    "--num-ec2-instances 5 \\\n",
    "--emr-tag job=hw5_3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
