{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Class: W261-2  \n",
    "Date: Mar 19, 2016  \n",
    "HW09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 9.0: Short answer questions\n",
    "\n",
    "What is PageRank and what is it used for in the context of web search?  \n",
    "What modifications have to be made to the webgraph in order to leverage the machinery of Markov Chains to \n",
    "compute the steady state distibuton?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is PageRank and what is it used for in the context of web search?  \n",
    "PageRank is a link analysis algorithm and it assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web, with the purpose of \"measuring\" its relative importance within the set.  In the context of web search, it is used for ranking the documents in the posting list of the inverted index, so that relatively more important documents will be put at the beginning of the posting list.  \n",
    "  \n",
    "#### What modifications have to be made to the webgraph in order to leverage the machinery of Markov Chains to compute the steady state distibuton?  \n",
    "In order to leverage the machinery of Markov Chains to compute the steady state distibuton, we need to make two adjustments to the webgraph (i.e. the transition matrix H, where each row represents a node):\n",
    "+ **Stochasticity adjustment** to resolve dangling nodes (nodes with no outbound links) problem:\n",
    "    + For each zero row we replace each element with 1/n, where n is the number of nodes in the wegraph.\n",
    "+ **Primitivity adjustment** to guarantee convergence.\n",
    "    + We need to define a teleport probability $\\alpha$ and define the transition matrix as:  \n",
    "<br/>\n",
    "\\begin{equation} \n",
    "P = (1-\\alpha)H + {\\alpha}I(1/n)  \n",
    "\\end{equation}  \n",
    "where:  \n",
    "        - _H_ is the hyperlink matrix\n",
    "        - n is the number of nodes in the webgraph\n",
    "        - I is the identity matrix\n",
    "        \n",
    "#### OPTIONAL: In topic-specific pagerank, how can we insure that the irreducible property is satified? (HINT: see HW9.4)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 9.1: MRJob implementation of basic PageRank\n",
    "\n",
    "Write a basic MRJob implementation of the iterative PageRank algorithm\n",
    "that takes sparse adjacency lists as input (as explored in HW 7).\n",
    "Make sure that you implementation utilizes teleportation (1-damping/the number of nodes in the network), \n",
    "and further, distributes the mass of dangling nodes with each iteration\n",
    "so that the output of each iteration is correctly normalized (sums to 1).  \n",
    "\n",
    "NOTE: \n",
    "+ The PageRank algorithm assumes that a random surfer (walker), starting from a random web page,\n",
    "chooses the next page to which it will move by clicking at random, with probability d,\n",
    "one of the hyperlinks in the current page. This probability is represented by a so-called\n",
    "‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer\n",
    "jumps to any web page in the network. If a page is a dangling end, meaning it has no\n",
    "outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform\n",
    "distribution and “teleports” to that page]\n",
    "\n",
    "\n",
    "As you build your code, use the test data\n",
    "\n",
    "s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name.  \n",
    "(On Dropbox https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0)\n",
    "\n",
    "with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck\n",
    "your work with the true result, displayed in the first image\n",
    "in the Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "```\n",
    "A,0.033\n",
    "B,0.384\n",
    "C,0.343\n",
    "D,0.039\n",
    "E,0.081\n",
    "F,0.039\n",
    "G,0.016\n",
    "H,0.016\n",
    "I,0.016\n",
    "J,0.016\n",
    "K,0.016\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the graph structure based on the adjaceny list file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrInitGraph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrInitGraph.py\n",
    "import re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import json\n",
    "import sys\n",
    "\n",
    "class MrInitGraph(MRJob):\n",
    "\n",
    "    SORT_VALUES = True  # Need 2nd sort\n",
    "   \n",
    "    def mapper(self, _, line):\n",
    "        # input format:\n",
    "        # Json object:\n",
    "        # 1       {'2': 1, '6': 1}\n",
    "        \n",
    "        fields = line.strip().split('\\t')\n",
    "        node = fields[0]\n",
    "\n",
    "        # JSON uses double quote for string\n",
    "        adjList = json.loads(fields[1].replace(\"'\", '\"'))\n",
    "        \n",
    "        # Output for the node, which is an outbound node\n",
    "        yield node, [0, adjList.keys()] # 0 is used for 2nd sorting\n",
    "        \n",
    "        # Also need to output an empty entry for every nodes in the adjList.\n",
    "        # It is needed for directed graph, so that in the reducer we can generate \n",
    "        # an entry for nodes which don't have an outbound link.\n",
    "        for key in adjList.keys():\n",
    "            yield key, [1]  # 1 is used for 2nd sorting\n",
    "            \n",
    "    def reducer(self, node, values):\n",
    "        value = values.next()\n",
    "        \n",
    "        # An outbound node will be seen first because we use 2nd sorting\n",
    "        if value[0] == 0:\n",
    "            # It's an outbound record.  \n",
    "            # Just yield the node without processing the rest of values.\n",
    "            yield node, [value[1], 0]\n",
    "        else:\n",
    "            # In a directed graph, if a node has no outbound link, generate an entry for it.\n",
    "            # And ignore the rest of values\n",
    "            yield node, [[], 0]\n",
    "                                                              \n",
    "if __name__ == '__main__':\n",
    "    MrInitGraph.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrFindNumberOfNodes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrFindNumberOfNodes.py\n",
    "import re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import json\n",
    "import sys\n",
    "\n",
    "class MrFindNumberOfNodes(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.mapper_init, mapper=self.mapper, mapper_final = self.mapper_final,\n",
    "                   combiner = self.reducer,\n",
    "                   reducer=self.reducer\n",
    "            )]\n",
    "\n",
    "    def mapper_init(self):\n",
    "        self.partial_size = 0\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        self.partial_size += 1\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        yield None, self.partial_size\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        yield None, sum(values)\n",
    "                                                              \n",
    "if __name__ == '__main__':\n",
    "    MrFindNumberOfNodes.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A PageRank iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrPageRankJob01.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrPageRankJob01.py\n",
    "from __future__ import division\n",
    "import re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import json\n",
    "import sys\n",
    "\n",
    "class MrPageRankJob01(MRJob):\n",
    "\n",
    "    INPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    \n",
    "    Type_Graph = 0\n",
    "    Type_PR = 1\n",
    "    \n",
    "    Key_LostPR = \"#\"\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MrPageRankJob01, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--initWithGraphSize', type='int', default=None)\n",
    "\n",
    "    def mapper_init(self):\n",
    "        self.lostPR = 0\n",
    "        \n",
    "    def mapper(self, node, data):\n",
    "        adjList, pageRank = data\n",
    "        \n",
    "        # We're at the first iteration, and PageRank value hasn't been initialized yet.\n",
    "        if self.options.initWithGraphSize is not None:\n",
    "            pageRank = 1 / self.options.initWithGraphSize\n",
    "\n",
    "        # Pass along the graph structure\n",
    "        yield node, [self.Type_Graph, adjList]\n",
    "        \n",
    "        if len(adjList) > 0:\n",
    "            # The PR juice we will send out\n",
    "            p = pageRank / len(adjList)\n",
    "            for link in adjList:\n",
    "                yield link, [self.Type_PR, p] # 1 means it's a PR contribution\n",
    "        else:\n",
    "            self.lostPR += pageRank\n",
    "            \n",
    "    def mapper_final(self):\n",
    "        yield self.Key_LostPR, self.lostPR\n",
    "\n",
    "    def reducer(self, node, values):\n",
    "        pageRank = 0\n",
    "        if node == self.Key_LostPR:\n",
    "            yield self.Key_LostPR, sum(values)\n",
    "        else:\n",
    "            for value in values:\n",
    "                valueType, data = value\n",
    "                if valueType == self.Type_Graph:\n",
    "                    adjList = data\n",
    "                else:\n",
    "                    pageRank += float(data)\n",
    "\n",
    "            yield node, [adjList, pageRank]\n",
    "            \n",
    "                                                              \n",
    "if __name__ == '__main__':\n",
    "    MrPageRankJob01.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrPageRankJob02.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrPageRankJob02.py\n",
    "from __future__ import division\n",
    "import re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import json\n",
    "import sys\n",
    "\n",
    "class MrPageRankJob02(MRJob):\n",
    "\n",
    "    INPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    \n",
    "    Key_LostPR = \"#\"\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MrPageRankJob02, self).configure_options()\n",
    "        self.add_passthrough_option('--graphSize', type=int, default=None)\n",
    "        self.add_passthrough_option('--lostPR', type=float, default=None)\n",
    "        self.add_passthrough_option('--teleportRate', type=float, default=None)        \n",
    "\n",
    "    def mapper(self, node, data):\n",
    "        if node == self.Key_LostPR:\n",
    "            return\n",
    "        \n",
    "        adjList, pageRank = data\n",
    "\n",
    "        pageRank = self.options.teleportRate / self.options.graphSize + \\\n",
    "                   (1-self.options.teleportRate) * (self.options.lostPR / self.options.graphSize + pageRank)\n",
    "\n",
    "        # Pass along the graph structure\n",
    "        yield node, [adjList, pageRank]                        \n",
    "                                                              \n",
    "if __name__ == '__main__':\n",
    "    MrPageRankJob02.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver for HW91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Driver_Hw91.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Driver_Hw91.py\n",
    "from __future__ import division\n",
    "from MrFindNumberOfNodes import MrFindNumberOfNodes\n",
    "from MrInitGraph import MrInitGraph\n",
    "from MrPageRankJob01 import MrPageRankJob01\n",
    "from MrPageRankJob02 import MrPageRankJob02\n",
    "import time\n",
    "import boto\n",
    "from boto.s3.key import Key\n",
    "import mrjob.parse\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--inputFile\", type=str)\n",
    "parser.add_argument(\"--initGraphPath\", type=str, default=None)\n",
    "parser.add_argument(\"--outputPath\", type=str, default=None)\n",
    "parser.add_argument(\"--num-ec2-instances\", type=str, default=None)\n",
    "parser.add_argument(\"--ec2-instance-type\", type=str, default=None)\n",
    "parser.add_argument(\"--teleportRate\", type=float, default=0.15)\n",
    "parser.add_argument(\"--skipInit\", action=\"store_true\", help=\"Don't run initGraph job\")\n",
    "parser.add_argument(\"--printResult\", action=\"store_true\")\n",
    "parser.add_argument(\"--nodeCount\", type=int, help=\"Specify nodeCount; for debug purpose\")\n",
    "parser.add_argument(\"--iteration\", type=int, default=5)\n",
    "parser.add_argument(\"-r\", \"--run\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "jobArgsBase = ['-r', args.run,\n",
    "             '--no-strict-protocols']\n",
    "\n",
    "if args.run == \"emr\":\n",
    "    jobArgsBase += [\"--pool-emr-job-flows\"]\n",
    "    \n",
    "    if args.ec2_instance_type is not None:\n",
    "        jobArgsBase += [\"--ec2-instance-type\", args.ec2_instance_type]\n",
    "        \n",
    "    if args.num_ec2_instances is not None:\n",
    "        jobArgsBase += [\"--num-ec2-instances\", args.num_ec2_instances]\n",
    "\n",
    "i = 1\n",
    "startTime = time.time()\n",
    "\n",
    "initGraphPath = None\n",
    "nodeCount = None\n",
    "outputPath02 = None\n",
    "\n",
    "def ensureFolderPath(path):\n",
    "    return path if path.endswith(\"/\") else path + \"/\"\n",
    "\n",
    "def initGraphStructure():\n",
    "    global initGraphPath\n",
    "    \n",
    "    # Initialize the graph structure\n",
    "    jobArgs = [args.inputFile] + jobArgsBase\n",
    "\n",
    "    if args.run == \"inline\" or args.run == \"local\":\n",
    "        initGraphPath = \"initGraph.txt\"\n",
    "        writeOutput = True\n",
    "    else:\n",
    "        initGraphPath = ensureFolderPath(args.initGraphPath)\n",
    "        jobArgs += [\"--no-output\", \"--output-dir\", initGraphPath]\n",
    "        writeOutput = False\n",
    "\n",
    "    if not args.skipInit:\n",
    "        mr_jobInitGraph = MrInitGraph(args=jobArgs)\n",
    "        with mr_jobInitGraph.make_runner() as runner: \n",
    "            runner.run()\n",
    "\n",
    "            if writeOutput:\n",
    "                # Generate the initGraph file\n",
    "                with open(initGraphPath, 'w') as f:            \n",
    "                    for line in runner.stream_output():\n",
    "                        f.write(\"%s\" % (line))\n",
    "    \n",
    "def findGraphSize():\n",
    "    global nodeCount\n",
    "    \n",
    "    if args.nodeCount is not None:\n",
    "        # nodeCount already provided.  For debug purpose.\n",
    "        nodeCount = args.nodeCount\n",
    "    else:\n",
    "        # Find the number of nodes\n",
    "        mr_jobNumberOfNodes = MrFindNumberOfNodes(args=[initGraphPath] + jobArgsBase)\n",
    "        with mr_jobNumberOfNodes.make_runner() as runner: \n",
    "            runner.run()\n",
    "\n",
    "            for line in runner.stream_output():\n",
    "                key, value =  mr_jobNumberOfNodes.parse_output_line(line)\n",
    "                nodeCount = int(value)\n",
    "\n",
    "    print \"Number of nodes = %d\\n\" % (nodeCount)\n",
    "\n",
    "def readLostPRFromS3(outputPath):\n",
    "    # Read the lost PR from one of the output file.  We will read the beginning portion of each file\n",
    "    # and see if we can find that special lost PR key.\n",
    "    assert mrjob.parse.is_s3_uri(outputPath)\n",
    "    bucket, key = mrjob.parse.parse_s3_uri(outputPath)\n",
    "    bucket = s3_connection.get_bucket(bucket)\n",
    "    pattern = key + 'part-'\n",
    "\n",
    "    for key in s3_connection.get_bucket(bucket):\n",
    "        #print 'key.name=', key.name\n",
    "\n",
    "        # ignore the file we don't care\n",
    "        if not key.name.startswith(pattern):\n",
    "            continue\n",
    "\n",
    "        content = key.get_contents_as_string(headers={'Range' : 'bytes=0-100'})\n",
    "        content = content.partition(\"\\n\")[0] # Get the first line\n",
    "\n",
    "        fields = content.split('\\t')\n",
    "        if fields[0] == '\"' + MrPageRankJob01.Key_LostPR + '\"':\n",
    "            assert len(fields) == 2\n",
    "            lostPR = float(fields[1])\n",
    "            return lostPR\n",
    "    \n",
    "    assert False, \"lostPR isn't found\"\n",
    "\n",
    "def runPageRank(iterations):\n",
    "    global outputPath02\n",
    "    graphInited = False\n",
    "\n",
    "    for i in range(1, iterations+1):\n",
    "        # Run MrPageRankJob01\n",
    "        print \"Iteration:\", i\n",
    "\n",
    "        if i == 1:\n",
    "            inputPath = initGraphPath\n",
    "        else:\n",
    "            inputPath = outputPath02 # from job02 in last iteration\n",
    "\n",
    "        jobArgs = [inputPath] + jobArgsBase\n",
    "        if args.run == \"inline\" or args.run == \"local\":\n",
    "            outputPath01 = \"output-%0d-1.txt\" % i\n",
    "            writeOutput = True\n",
    "        else:\n",
    "            outputPath01 = ensureFolderPath(args.outputPath) + \"iter\" + str(i) + \"-1/\"\n",
    "            writeOutput = False\n",
    "\n",
    "            jobArgs += [\"--output-dir\", outputPath01] \n",
    "\n",
    "            # For this HW we'll use stream-output for Hadoop\n",
    "            if args.run == \"emr\":\n",
    "                jobArgs += [\"--no-output\"]\n",
    "\n",
    "        if args.run == \"emr\":\n",
    "            # Have to access S3 to get partial output\n",
    "            s3_connection = boto.connect_s3()\n",
    "\n",
    "        if not graphInited:\n",
    "            jobArgs += [\"--initWithGraphSize\", str(nodeCount)]\n",
    "            graphInited = True\n",
    "\n",
    "        # Run Job 01 - each node distribute its PageRank to all its neighbors\n",
    "        mr_jobPageRankJob01 = MrPageRankJob01(args=jobArgs)\n",
    "        with mr_jobPageRankJob01.make_runner() as runner: \n",
    "            runner.run()\n",
    "\n",
    "            if args.run == \"inline\" or args.run == \"local\" or args.run == \"hadoop\":\n",
    "                if writeOutput:\n",
    "                    f = open(outputPath01, 'w')\n",
    "\n",
    "                for line in runner.stream_output():\n",
    "                    key, value =  mr_jobPageRankJob01.parse_output_line(line)\n",
    "                    if key == MrPageRankJob01.Key_LostPR:\n",
    "                        lostPR = float(value)\n",
    "                    else:\n",
    "                        if writeOutput:\n",
    "                            f.write(line)\n",
    "                            \n",
    "                        #if args.printResult:\n",
    "                        #    print line.strip()\n",
    "\n",
    "\n",
    "                if writeOutput:\n",
    "                    f.close()\n",
    "\n",
    "            else:\n",
    "                time.sleep(5) # Sleep a bit for S3 consistency\n",
    "                lostPR = readLostPRFromS3(outputPath01)\n",
    "\n",
    "\n",
    "        # Run Job 02 - Distribute the lost PR and also the teleported PageRank\n",
    "        jobArgs = [outputPath01] + jobArgsBase\n",
    "\n",
    "        if args.run == \"inline\" or args.run == \"local\":\n",
    "            outputPath02 = \"output-%0d-2.txt\" % i\n",
    "            writeOutput = True\n",
    "        else:\n",
    "            outputPath02 = ensureFolderPath(args.outputPath) + \"iter\" + str(i) + \"-2/\"\n",
    "            writeOutput = False\n",
    "\n",
    "            jobArgs += [\"--output-dir\", outputPath02] \n",
    "\n",
    "            # For this HW we'll use stream-output for Hadoop\n",
    "            if args.run == \"emr\":\n",
    "                jobArgs += [\"--no-output\"]\n",
    "\n",
    "        jobArgs += ['--graphSize', str(nodeCount), '--lostPR', str(lostPR), \n",
    "                    '--teleportRate', str(args.teleportRate)]\n",
    "\n",
    "        mr_jobPageRankJob02 = MrPageRankJob02(args=jobArgs)\n",
    "        with mr_jobPageRankJob02.make_runner() as runner: \n",
    "            totalPR = 0\n",
    "            runner.run()\n",
    "            if args.run == \"inline\" or args.run == \"local\" or args.run == \"hadoop\":\n",
    "                if writeOutput:\n",
    "                    f = open(outputPath02, 'w')\n",
    "\n",
    "                for line in runner.stream_output():\n",
    "                    if writeOutput:\n",
    "                        f.write(line)\n",
    "\n",
    "                    key, value =  mr_jobPageRankJob01.parse_output_line(line)\n",
    "                    totalPR += float(value[1])\n",
    "\n",
    "                    if args.printResult:\n",
    "                        print line.strip()\n",
    "\n",
    "                if writeOutput:\n",
    "                    f.close()\n",
    "\n",
    "        if args.printResult:\n",
    "            print\n",
    "\n",
    "# The flow\n",
    "initGraphStructure()\n",
    "findGraphSize()\n",
    "runPageRank(args.iteration)\n",
    "\n",
    "print \"Output location=\", outputPath02\n",
    "\n",
    "print\n",
    "print \"Total time: %d sec\" % (time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"mrjob.sim\"\n",
      "Number of nodes = 11\n",
      "\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Output location= output-5-2.txt\n",
      "\n",
      "Total time: 0 sec\n"
     ]
    }
   ],
   "source": [
    "!rm -f output.txt\n",
    "!rm -f initGraph.txt\n",
    "\n",
    "!python Driver_Hw91.py -r inline \\\n",
    "--inputFile PageRank-test.txt \\\n",
    "--iteration 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A\"\t[[], 0.038674936390505975]\r\n",
      "\"B\"\t[[\"C\"], 0.41918431938831874]\r\n",
      "\"C\"\t[[\"B\"], 0.28315371580151916]\r\n",
      "\"D\"\t[[\"A\", \"B\"], 0.04115963362308002]\r\n",
      "\"E\"\t[[\"B\", \"D\", \"F\"], 0.09396022932089781]\r\n",
      "\"F\"\t[[\"B\", \"E\"], 0.04115963362308002]\r\n",
      "\"G\"\t[[\"B\", \"E\"], 0.016541506370530455]\r\n",
      "\"H\"\t[[\"B\", \"E\"], 0.016541506370530455]\r\n",
      "\"I\"\t[[\"B\", \"E\"], 0.016541506370530455]\r\n",
      "\"J\"\t[[\"E\"], 0.016541506370530455]\r\n",
      "\"K\"\t[[\"E\"], 0.016541506370530455]\r\n"
     ]
    }
   ],
   "source": [
    "!cat output-5-2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it on Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/13 17:12:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw91\n",
      "16/03/13 17:12:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "No handlers could be found for logger \"mrjob.compat\"\n",
      "Number of nodes = 11\n",
      "\n",
      "Iteration: 1\n",
      "Iteration: 2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw91\n",
    "!hdfs dfs -mkdir hw91\n",
    "\n",
    "!python Driver_Hw91.py -r hadoop \\\n",
    "--inputFile PageRank-test.txt \\\n",
    "--initGraphPath hdfs://127.0.0.1/user/patrickng/hw91/init/ \\\n",
    "--outputPath hdfs://127.0.0.1/user/patrickng/hw91/output/ \\\n",
    "--iteration 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: s3://patng323-w261-hw91/\n",
      "upload: ./PageRank-test.txt to s3://patng323-w261-hw91/input/PageRank-test.txt\n",
      "2016-03-13 16:04:41        166 PageRank-test.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 mb s3://patng323-w261-hw91\n",
    "!aws s3 cp PageRank-test.txt s3://patng323-w261-hw91/input/\n",
    "!aws s3 ls s3://patng323-w261-hw91/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive s3://patng323-w261-hw91/init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://patng323-w261-hw91/output/_SUCCESS\n",
      "delete: s3://patng323-w261-hw91/output/part-00000\n",
      "Wait 5.0s sec for S3 eventual consistency\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive s3://patng323-w261-hw91/output\n",
    "!echo \"Wait 5.0s sec for S3 eventual consistency\"\n",
    "!sleep 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes = 11\n",
      "No handlers could be found for logger \"mrjob.conf\"\n",
      "outputPath= s3://patng323-w261-hw91/output/\n",
      "key.name= init/_SUCCESS\n",
      "key.name= init/part-00000\n",
      "key.name= input/PageRank-test.txt\n",
      "key.name= output/_SUCCESS\n",
      "key.name= output/part-00000\n",
      "content= \"#\"\t0.090909090909090912\n",
      "lostPR = 0.0909090909091\n",
      "\n",
      "Total time: 175 sec\n"
     ]
    }
   ],
   "source": [
    "!python Driver_Hw91.py -r emr \\\n",
    "--inputFile PageRank-test.txt \\\n",
    "--initGraphPath s3://patng323-w261-hw91/init \\\n",
    "--outputPath s3://patng323-w261-hw91/output \\\n",
    "--num-ec2-instances 2 \\\n",
    "--ec2-instance-type m1.medium \\\n",
    "--skipInit \\\n",
    "--nodeCount 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/13 00:37:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   1 patrickng supergroup          0 2016-03-13 00:34 hw91/init/_SUCCESS\n",
      "-rw-r--r--   1 patrickng supergroup        197 2016-03-13 00:34 hw91/init/part-00000\n",
      "16/03/13 00:37:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "-rw-r--r--   1 patrickng supergroup          0 2016-03-13 00:35 hw91/output/_SUCCESS\n",
      "-rw-r--r--   1 patrickng supergroup        331 2016-03-13 00:35 hw91/output/part-00000\n",
      "16/03/13 00:37:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"#\"\t0.09090909090909091\n",
      "\"A\"\t[[], 0.045454545454545456]\n",
      "\"B\"\t[[\"C\"], 0.3484848484848485]\n",
      "\"C\"\t[[\"B\"], 0.09090909090909091]\n",
      "\"D\"\t[[\"A\", \"B\"], 0.030303030303030304]\n",
      "\"E\"\t[[\"B\", \"D\", \"F\"], 0.36363636363636365]\n",
      "\"F\"\t[[\"B\", \"E\"], 0.030303030303030304]\n",
      "\"G\"\t[[\"B\", \"E\"], 0]\n",
      "\"H\"\t[[\"B\", \"E\"], 0]\n",
      "\"I\"\t[[\"B\", \"E\"], 0]\n",
      "\"J\"\t[[\"E\"], 0]\n",
      "\"K\"\t[[\"E\"], 0]\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls hw91/init\n",
    "!hdfs dfs -ls hw91/output\n",
    "!hdfs dfs -cat hw91/output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derby.log\n",
      "Tue Oct 20 06:46:22 HKT 2015 Thread[main,5,main] Ignored duplicate property derby.module.dataDictionary in jar:file:/Users/patrickng/Programs/apache-hive-1.2.1-bin/lib/hive-jdbc-1.2.1-standalone.jar!/org/apache/derby/modules.properties\n",
      "\n",
      "hello1.txt\n",
      "Jennifer is my wife\n",
      "\n",
      "weka.log\n",
      "2015-01-16 17:48:51 weka.gui.GUIChooser main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto\n",
    "from boto.s3.key import Key\n",
    "\n",
    "s3_connection = boto.connect_s3()\n",
    "bucket = s3_connection.get_bucket('patng323-w261-hw9')\n",
    "for key in bucket:\n",
    "    print(key.name)\n",
    "    s = key.get_contents_as_string(headers={'Range' : 'bytes=0-500'})\n",
    "    print s.partition(\"\\n\")[0]\n",
    "    print\n",
    "key = bucket.new_key('hello1.txt')  \n",
    "key.set_contents_from_string(\"Jennifer is my wife\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Test.py\n",
    "import re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import json\n",
    "import sys\n",
    "#import boto\n",
    "#from boto.s3.key import Key\n",
    "\n",
    "class Test(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        yield None, line\n",
    "        self.increment_counter(\"patng\", \"hello\", 3)\n",
    "        \n",
    "    def mapper_final_ignore(self):\n",
    "        s3 = boto.connect_s3()\n",
    "        bucket = s3.lookup('patng323-w261-hw9')\n",
    "        key = bucket.new_key('hello.txt')  \n",
    "        key.set_contents_from_string(\"Sor Mui is my wife\")   \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Test.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"#\"\t0.09090909090909091\n",
      "\"A\"\t[[], 0.045454545454545456]\n",
      "\"B\"\t[[\"C\"], 0.3484848484848485]\n",
      "\"C\"\t[[\"B\"], 0.09090909090909091]\n",
      "\"D\"\t[[\"A\", \"B\"], 0.030303030303030304]\n",
      "\"E\"\t[[\"B\", \"D\", \"F\"], 0.36363636363636365]\n",
      "\"F\"\t[[\"B\", \"E\"], 0.030303030303030304]\n",
      "\"G\"\t[[\"B\", \"E\"], 0]\n",
      "\"H\"\t[[\"B\", \"E\"], 0]\n",
      "\"I\"\t[[\"B\", \"E\"], 0]\n",
      "\"J\"\t[[\"E\"], 0]\n",
      "\"K\"\t[[\"E\"], 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from snakebite.client import Client\n",
    "client = Client(\"127.0.0.1\", port=8020)\n",
    "result = client.cat(['/user/patrickng/hw91/output/part-00000'])\n",
    "for c in result:\n",
    "    for s in c:\n",
    "        print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patng323-w261-hw91\n",
      "output/\n"
     ]
    }
   ],
   "source": [
    "import mrjob.parse\n",
    "\n",
    "outputPath = 's3://patng323-w261-hw91/output/'\n",
    "\n",
    "assert mrjob.parse.is_s3_uri(outputPath)\n",
    "bucket, key = mrjob.parse.parse_s3_uri(outputPath)\n",
    "print bucket\n",
    "print key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
