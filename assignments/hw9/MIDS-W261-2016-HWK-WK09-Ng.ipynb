{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Class: W261-2  \n",
    "Date: Mar 19, 2016  \n",
    "HW09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 9.0: Short answer questions\n",
    "\n",
    "What is PageRank and what is it used for in the context of web search?  \n",
    "What modifications have to be made to the webgraph in order to leverage the machinery of Markov Chains to \n",
    "compute the steady state distibuton?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is PageRank and what is it used for in the context of web search?  \n",
    "PageRank is a link analysis algorithm and it assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web, with the purpose of \"measuring\" its relative importance within the set.  In the context of web search, it is used for ranking the documents in the posting list of the inverted index, so that relatively more important documents will be put at the beginning of the posting list.  \n",
    "  \n",
    "#### What modifications have to be made to the webgraph in order to leverage the machinery of Markov Chains to compute the steady state distibuton?  \n",
    "In order to leverage the machinery of Markov Chains to compute the steady state distibuton, we need to make two adjustments to the webgraph (i.e. the transition matrix H, where each row represents a node):\n",
    "+ **Stochasticity adjustment** to resolve dangling nodes (nodes with no outbound links) problem:\n",
    "    + For each zero row we replace each element with 1/n, where n is the number of nodes in the wegraph.\n",
    "+ **Primitivity adjustment** to guarantee convergence.\n",
    "    + We need to define a teleport probability $\\alpha$ and define the transition matrix as:  \n",
    "<br/>\n",
    "\\begin{equation} \n",
    "P = (1-\\alpha)H + {\\alpha}I(1/n)  \n",
    "\\end{equation}  \n",
    "where:  \n",
    "        - _H_ is the hyperlink matrix\n",
    "        - n is the number of nodes in the webgraph\n",
    "        - I is the identity matrix\n",
    "        \n",
    "#### OPTIONAL: In topic-specific pagerank, how can we insure that the irreducible property is satified? (HINT: see HW9.4)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 9.1: MRJob implementation of basic PageRank\n",
    "\n",
    "Write a basic MRJob implementation of the iterative PageRank algorithm\n",
    "that takes sparse adjacency lists as input (as explored in HW 7).\n",
    "Make sure that you implementation utilizes teleportation (1-damping/the number of nodes in the network), \n",
    "and further, distributes the mass of dangling nodes with each iteration\n",
    "so that the output of each iteration is correctly normalized (sums to 1).  \n",
    "\n",
    "NOTE: \n",
    "+ The PageRank algorithm assumes that a random surfer (walker), starting from a random web page,\n",
    "chooses the next page to which it will move by clicking at random, with probability d,\n",
    "one of the hyperlinks in the current page. This probability is represented by a so-called\n",
    "‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer\n",
    "jumps to any web page in the network. If a page is a dangling end, meaning it has no\n",
    "outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform\n",
    "distribution and “teleports” to that page]\n",
    "\n",
    "\n",
    "As you build your code, use the test data\n",
    "\n",
    "s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name.  \n",
    "(On Dropbox https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0)\n",
    "\n",
    "with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck\n",
    "your work with the true result, displayed in the first image\n",
    "in the Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "```\n",
    "A,0.033\n",
    "B,0.384\n",
    "C,0.343\n",
    "D,0.039\n",
    "E,0.081\n",
    "F,0.039\n",
    "G,0.016\n",
    "H,0.016\n",
    "I,0.016\n",
    "J,0.016\n",
    "K,0.016\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the graph structure based on the adjaceny list file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrInitGraph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrInitGraph.py\n",
    "import re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import json\n",
    "import sys\n",
    "\n",
    "class MrInitGraph(MRJob):\n",
    "\n",
    "    SORT_VALUES = True  # Need 2nd sort\n",
    "   \n",
    "    def mapper(self, _, line):\n",
    "        # input format:\n",
    "        # Json object:\n",
    "        # 1       {'2': 1, '6': 1}\n",
    "        \n",
    "        fields = line.strip().split('\\t')\n",
    "        node = fields[0]\n",
    "\n",
    "        # JSON uses double quote for string\n",
    "        adjList = json.loads(fields[1].replace(\"'\", '\"'))\n",
    "        \n",
    "        # Output for the node, which is an outbound node\n",
    "        yield node, [0, adjList.keys()] # 0 is used for 2nd sorting\n",
    "        \n",
    "        # Also need to output an empty entry for every nodes in the adjList.\n",
    "        # It is needed for directed graph, so that in the reducer we can generate \n",
    "        # an entry for nodes which don't have an outbound link.\n",
    "        for key in adjList.keys():\n",
    "            yield key, [1]  # 1 is used for 2nd sorting\n",
    "            \n",
    "    def reducer(self, node, values):\n",
    "        value = values.next()\n",
    "        \n",
    "        # An outbound node will be seen first because we use 2nd sorting\n",
    "        if value[0] == 0:\n",
    "            # It's an outbound record.  \n",
    "            # Just yield the node without processing the rest of values.\n",
    "            yield node, [value[1], 0]\n",
    "        else:\n",
    "            # In a directed graph, if a node has no outbound link, generate an entry for it.\n",
    "            # And ignore the rest of values\n",
    "            yield node, [[], 0]\n",
    "                                                              \n",
    "if __name__ == '__main__':\n",
    "    MrInitGraph.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrFindNumberOfNodes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrFindNumberOfNodes.py\n",
    "import re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import json\n",
    "import sys\n",
    "\n",
    "class MrFindNumberOfNodes(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.mapper_init, mapper=self.mapper, mapper_final = self.mapper_final,\n",
    "                   combiner = self.reducer,\n",
    "                   reducer=self.reducer\n",
    "            )]\n",
    "\n",
    "    def mapper_init(self):\n",
    "        self.partial_size = 0\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        self.partial_size += 1\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        yield None, self.partial_size\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        yield None, sum(values)\n",
    "                                                              \n",
    "if __name__ == '__main__':\n",
    "    MrFindNumberOfNodes.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A PageRank iteration - Job 01 and Job 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 01 - Each node distributes its PageRank evenly to all its neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrPageRankJob01.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrPageRankJob01.py\n",
    "from __future__ import division\n",
    "import re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import json\n",
    "import sys\n",
    "\n",
    "class MrPageRankJob01(MRJob):\n",
    "\n",
    "    INPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    \n",
    "    Type_Graph = 0\n",
    "    Type_PR = 1\n",
    "    \n",
    "    Key_LostPR = \"#\"\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MrPageRankJob01, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--initWithGraphSize', type='int', default=None)\n",
    "\n",
    "    def mapper_init(self):\n",
    "        self.lostPR = 0\n",
    "        \n",
    "    def mapper(self, node, data):\n",
    "        adjList, pageRank = data\n",
    "        \n",
    "        # We're at the first iteration, and PageRank value hasn't been initialized yet.\n",
    "        if self.options.initWithGraphSize is not None:\n",
    "            pageRank = 1 / self.options.initWithGraphSize\n",
    "\n",
    "        # Pass along the graph structure\n",
    "        yield node, [self.Type_Graph, adjList]\n",
    "        \n",
    "        if len(adjList) > 0:\n",
    "            # The PR juice we will send out\n",
    "            p = pageRank / len(adjList)\n",
    "            for link in adjList:\n",
    "                yield link, [self.Type_PR, p] # 1 means it's a PR contribution\n",
    "        else:\n",
    "            self.lostPR += pageRank\n",
    "            \n",
    "    def mapper_final(self):\n",
    "        yield self.Key_LostPR, self.lostPR\n",
    "\n",
    "    def reducer(self, node, values):\n",
    "        pageRank = 0\n",
    "        if node == self.Key_LostPR:\n",
    "            yield self.Key_LostPR, sum(values)\n",
    "        else:\n",
    "            for value in values:\n",
    "                valueType, data = value\n",
    "                if valueType == self.Type_Graph:\n",
    "                    adjList = data\n",
    "                else:\n",
    "                    pageRank += float(data)\n",
    "\n",
    "            yield node, [adjList, pageRank]\n",
    "            \n",
    "                                                              \n",
    "if __name__ == '__main__':\n",
    "    MrPageRankJob01.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 02 - Update each node's PageRank usng the \"lost PR\" from last job, and also the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrPageRankJob02.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrPageRankJob02.py\n",
    "from __future__ import division\n",
    "import re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import json\n",
    "import sys\n",
    "\n",
    "class MrPageRankJob02(MRJob):\n",
    "\n",
    "    INPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    \n",
    "    Key_LostPR = \"#\"\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MrPageRankJob02, self).configure_options()\n",
    "        self.add_passthrough_option('--graphSize', type=int, default=None)\n",
    "        self.add_passthrough_option('--lostPR', type=float, default=None)\n",
    "        self.add_passthrough_option('--teleportRate', type=float, default=None)        \n",
    "\n",
    "    def mapper(self, node, data):\n",
    "        if node == self.Key_LostPR:\n",
    "            return\n",
    "        \n",
    "        adjList, pageRank = data\n",
    "\n",
    "        pageRank = self.options.teleportRate / self.options.graphSize + \\\n",
    "                   (1-self.options.teleportRate) * (self.options.lostPR / self.options.graphSize + pageRank)\n",
    "\n",
    "        # Pass along the graph structure\n",
    "        yield node, [adjList, pageRank]                        \n",
    "                                                              \n",
    "if __name__ == '__main__':\n",
    "    MrPageRankJob02.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort result according to PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MrSortPageRank.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MrSortPageRank.py\n",
    "from __future__ import division\n",
    "import re\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import json\n",
    "import sys\n",
    "\n",
    "def toStringKey(n):\n",
    "    n = int(n)\n",
    "    digits = len(str(sys.maxint))\n",
    "    minInt = -sys.maxint - 1\n",
    "\n",
    "    if n < 0:\n",
    "        key = \"-\" + str(abs(minInt-n)).zfill(digits)\n",
    "    else:\n",
    "        key = str(n).zfill(digits)\n",
    "\n",
    "    return key\n",
    "\n",
    "class MrSortPageRank(MRJob):\n",
    "\n",
    "    INPUT_PROTOCOL = mrjob.protocol.JSONProtocol\n",
    "    \n",
    "    def configure_options(self):\n",
    "        super(MrSortPageRank, self).configure_options()\n",
    "        self.add_passthrough_option('--count', type=int, default=None)\n",
    "        \n",
    "    def mapper(self, node, data):\n",
    "        adjList, pageRank = data\n",
    "        # Want to sort in descending order.  So use (1-pageRank)\n",
    "        yield toStringKey((1-pageRank) * 1000000000), [node, pageRank]\n",
    "        \n",
    "    def reducer_init(self):\n",
    "        self.emitted = 0\n",
    "            \n",
    "    def reducer(self, _, values):\n",
    "        \n",
    "        for value in values:\n",
    "            if self.emitted == self.options.count:\n",
    "                return\n",
    "            \n",
    "            self.emitted += 1\n",
    "            yield value[0], value[1]\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   reducer=self.reducer,\n",
    "                   reducer_init = self.reducer_init,\n",
    "                    jobconf={\n",
    "                    \"numReduceTasks\":\"1\"}\n",
    "                  )\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MrSortPageRank.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver for HW91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Driver_Hw91.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Driver_Hw91.py\n",
    "from __future__ import division\n",
    "from MrFindNumberOfNodes import MrFindNumberOfNodes\n",
    "from MrInitGraph import MrInitGraph\n",
    "from MrPageRankJob01 import MrPageRankJob01\n",
    "from MrPageRankJob02 import MrPageRankJob02\n",
    "from MrSortPageRank import MrSortPageRank\n",
    "import time\n",
    "import boto\n",
    "from boto.s3.key import Key\n",
    "import mrjob.parse\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--inputFile\", type=str)\n",
    "parser.add_argument(\"--initGraphPath\", type=str, default=None)\n",
    "parser.add_argument(\"--outputPath\", type=str, default=None)\n",
    "parser.add_argument(\"--num-ec2-instances\", type=str, default=None)\n",
    "parser.add_argument(\"--ec2-instance-type\", type=str, default=None)\n",
    "parser.add_argument(\"--teleportRate\", type=float, default=0.15)\n",
    "parser.add_argument(\"--skipInit\", action=\"store_true\", help=\"Don't run initGraph job\")\n",
    "parser.add_argument(\"--printResult\", action=\"store_true\")\n",
    "parser.add_argument(\"--nodeCount\", type=int, help=\"Specify nodeCount; for debug purpose\")\n",
    "parser.add_argument(\"--iteration\", type=int, default=5)\n",
    "parser.add_argument(\"--resultCount\", type=int, default=20, help=\"Size of sorted result\")\n",
    "parser.add_argument(\"-r\", \"--run\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "jobArgsBase = ['-r', args.run,\n",
    "             '--no-strict-protocols']\n",
    "\n",
    "if args.run == \"emr\":\n",
    "    jobArgsBase += [\"--pool-emr-job-flows\"]\n",
    "    \n",
    "    if args.ec2_instance_type is not None:\n",
    "        jobArgsBase += [\"--ec2-instance-type\", args.ec2_instance_type]\n",
    "        \n",
    "    if args.num_ec2_instances is not None:\n",
    "        jobArgsBase += [\"--num-ec2-instances\", args.num_ec2_instances]\n",
    "\n",
    "i = 1\n",
    "startTime = time.time()\n",
    "\n",
    "initGraphPath = None\n",
    "nodeCount = None\n",
    "outputPath02 = None\n",
    "\n",
    "def ensureFolderPath(path):\n",
    "    return path if path.endswith(\"/\") else path + \"/\"\n",
    "\n",
    "def printMilestone(s):\n",
    "    print s + \"  (%.2fs from start)\" % (time.time() - startTime)\n",
    "    \n",
    "def initGraphStructure():\n",
    "    global initGraphPath\n",
    "    \n",
    "    printMilestone(\"Init Graph Structure\")\n",
    "    \n",
    "    # Initialize the graph structure\n",
    "    jobArgs = [args.inputFile] + jobArgsBase\n",
    "\n",
    "    if args.run == \"inline\" or args.run == \"local\":\n",
    "        initGraphPath = \"initGraph.txt\"\n",
    "        writeOutput = True\n",
    "    else:\n",
    "        initGraphPath = ensureFolderPath(args.initGraphPath)\n",
    "        jobArgs += [\"--no-output\", \"--output-dir\", initGraphPath]\n",
    "        writeOutput = False\n",
    "\n",
    "    if not args.skipInit:\n",
    "        mr_jobInitGraph = MrInitGraph(args=jobArgs)\n",
    "        with mr_jobInitGraph.make_runner() as runner: \n",
    "            runner.run()\n",
    "\n",
    "            if writeOutput:\n",
    "                # Generate the initGraph file\n",
    "                with open(initGraphPath, 'w') as f:            \n",
    "                    for line in runner.stream_output():\n",
    "                        f.write(\"%s\" % (line))\n",
    "\n",
    "def findGraphSize():\n",
    "    global nodeCount\n",
    "    \n",
    "    printMilestone(\"Find Graph Size\")\n",
    "    \n",
    "    if args.nodeCount is not None:\n",
    "        # nodeCount already provided.  For debug purpose.\n",
    "        nodeCount = args.nodeCount\n",
    "    else:\n",
    "        # Find the number of nodes\n",
    "        mr_jobNumberOfNodes = MrFindNumberOfNodes(args=[initGraphPath] + jobArgsBase)\n",
    "        with mr_jobNumberOfNodes.make_runner() as runner: \n",
    "            runner.run()\n",
    "\n",
    "            for line in runner.stream_output():\n",
    "                key, value =  mr_jobNumberOfNodes.parse_output_line(line)\n",
    "                nodeCount = int(value)\n",
    "\n",
    "    print \"Number of nodes = %d\\n\" % (nodeCount)\n",
    "\n",
    "def readLostPRFromS3(outputPath):\n",
    "    # Read the lost PR from one of the output file.  We will read the beginning portion of each file\n",
    "    # and see if we can find that special lost PR key.\n",
    "\n",
    "    s3_connection = boto.connect_s3()\n",
    "\n",
    "    assert mrjob.parse.is_s3_uri(outputPath)\n",
    "    bucket, key = mrjob.parse.parse_s3_uri(outputPath)\n",
    "    bucket = s3_connection.get_bucket(bucket)\n",
    "    pattern = key + 'part-'\n",
    "\n",
    "    for key in s3_connection.get_bucket(bucket):\n",
    "        #print 'key.name=', key.name\n",
    "\n",
    "        # ignore the file we don't care\n",
    "        if not key.name.startswith(pattern):\n",
    "            continue\n",
    "\n",
    "        content = key.get_contents_as_string(headers={'Range' : 'bytes=0-100'})\n",
    "        content = content.partition(\"\\n\")[0] # Get the first line\n",
    "\n",
    "        fields = content.split('\\t')\n",
    "        if fields[0] == '\"' + MrPageRankJob01.Key_LostPR + '\"':\n",
    "            assert len(fields) == 2\n",
    "            lostPR = float(fields[1])\n",
    "            return lostPR\n",
    "    \n",
    "    assert False, \"lostPR isn't found\"\n",
    "\n",
    "def runPageRank(iterations):\n",
    "    global outputPath02\n",
    "    graphInited = False\n",
    "\n",
    "    for i in range(1, iterations+1):\n",
    "        # Run MrPageRankJob01\n",
    "        printMilestone(\"Iteration %02d-1\" % i)\n",
    "\n",
    "        if i == 1:\n",
    "            inputPath = initGraphPath\n",
    "        else:\n",
    "            inputPath = outputPath02 # from job02 in last iteration\n",
    "\n",
    "        jobArgs = [inputPath] + jobArgsBase\n",
    "        if args.run == \"inline\" or args.run == \"local\":\n",
    "            outputPath01 = \"output-%02d-1.txt\" % i\n",
    "            writeOutput = True\n",
    "        else:\n",
    "            outputPath01 = ensureFolderPath(args.outputPath) + \"iter\" + str(i) + \"-1/\"\n",
    "            writeOutput = False\n",
    "\n",
    "            jobArgs += [\"--output-dir\", outputPath01] \n",
    "\n",
    "            # For this HW we'll use stream-output for Hadoop\n",
    "            if args.run == \"emr\":\n",
    "                jobArgs += [\"--no-output\"]\n",
    "\n",
    "        if not graphInited:\n",
    "            jobArgs += [\"--initWithGraphSize\", str(nodeCount)]\n",
    "            graphInited = True\n",
    "\n",
    "        # Run Job 01 - each node distribute its PageRank to all its neighbors\n",
    "        mr_jobPageRankJob01 = MrPageRankJob01(args=jobArgs)\n",
    "        with mr_jobPageRankJob01.make_runner() as runner: \n",
    "            runner.run()\n",
    "\n",
    "            if args.run == \"inline\" or args.run == \"local\" or args.run == \"hadoop\":\n",
    "                if writeOutput:\n",
    "                    f = open(outputPath01, 'w')\n",
    "\n",
    "                for line in runner.stream_output():\n",
    "                    key, value =  mr_jobPageRankJob01.parse_output_line(line)\n",
    "                    if key == MrPageRankJob01.Key_LostPR:\n",
    "                        lostPR = float(value)\n",
    "                    else:\n",
    "                        if writeOutput:\n",
    "                            f.write(line)\n",
    "                            \n",
    "                        #if args.printResult:\n",
    "                        #    print line.strip()\n",
    "\n",
    "\n",
    "                if writeOutput:\n",
    "                    f.close()\n",
    "\n",
    "            else:\n",
    "                time.sleep(5) # Sleep a bit for S3 consistency\n",
    "                lostPR = readLostPRFromS3(outputPath01)\n",
    "\n",
    "\n",
    "        # Run Job 02 - Distribute the lost PR and also the teleported PageRank\n",
    "        jobArgs = [outputPath01] + jobArgsBase\n",
    "\n",
    "        if args.run == \"inline\" or args.run == \"local\":\n",
    "            outputPath02 = \"output-%02d-2.txt\" % i\n",
    "            writeOutput = True\n",
    "        else:\n",
    "            outputPath02 = ensureFolderPath(args.outputPath) + \"iter\" + str(i) + \"-2/\"\n",
    "            writeOutput = False\n",
    "\n",
    "            jobArgs += [\"--output-dir\", outputPath02] \n",
    "\n",
    "            # For this HW we'll use stream-output for Hadoop\n",
    "            if args.run == \"emr\":\n",
    "                jobArgs += [\"--no-output\"]\n",
    "\n",
    "        jobArgs += ['--graphSize', str(nodeCount), '--lostPR', str(lostPR), \n",
    "                    '--teleportRate', str(args.teleportRate)]\n",
    "\n",
    "        printMilestone(\"Iteration: %02d-2\" % i)\n",
    "        mr_jobPageRankJob02 = MrPageRankJob02(args=jobArgs)\n",
    "        with mr_jobPageRankJob02.make_runner() as runner: \n",
    "            totalPR = 0\n",
    "            runner.run()\n",
    "            if args.run == \"inline\" or args.run == \"local\" or args.run == \"hadoop\":\n",
    "                if writeOutput:\n",
    "                    f = open(outputPath02, 'w')\n",
    "\n",
    "                for line in runner.stream_output():\n",
    "                    if writeOutput:\n",
    "                        f.write(line)\n",
    "\n",
    "                    key, value =  mr_jobPageRankJob01.parse_output_line(line)\n",
    "                    totalPR += float(value[1])\n",
    "\n",
    "                    if args.printResult:\n",
    "                        print line.strip()\n",
    "\n",
    "                if writeOutput:\n",
    "                    f.close()\n",
    "\n",
    "        if args.printResult:\n",
    "            print\n",
    "\n",
    "def outputSortedResult(resultCount, resultPath):\n",
    "\n",
    "    # Run MrPageRankJob01\n",
    "    printMilestone(\"Output Sorted Result:\")\n",
    "    print\n",
    "\n",
    "    jobArgs = [resultPath] + jobArgsBase\n",
    "\n",
    "    # Run Job 01 - each node distribute its PageRank to all its neighbors\n",
    "    mr_job = MrSortPageRank(args=jobArgs)\n",
    "    resultFile = \"result-%s.txt\" % args.run\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "\n",
    "        with open(resultFile, \"w\") as f:\n",
    "            for line in runner.stream_output():\n",
    "                f.write(line)\n",
    "                \n",
    "    return resultFile\n",
    "\n",
    "# The flow\n",
    "\n",
    "initGraphStructure()\n",
    "findGraphSize()\n",
    "runPageRank(args.iteration)\n",
    "fn = outputSortedResult(args.resultCount, outputPath02)\n",
    "\n",
    "print\n",
    "print \"Total result location:\", outputPath02\n",
    "print \"Sorted result location:\", fn\n",
    "print\n",
    "print \"Total time: %.3fs\" % (time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Graph Structure  (0.00s from start)\n",
      "No handlers could be found for logger \"mrjob.sim\"\n",
      "Find Graph Size  (0.03s from start)\n",
      "Number of nodes = 11\n",
      "\n",
      "Iteration 01-1  (0.06s from start)\n",
      "Iteration: 01-2  (0.08s from start)\n",
      "Iteration 02-1  (0.10s from start)\n",
      "Iteration: 02-2  (0.12s from start)\n",
      "Iteration 03-1  (0.14s from start)\n",
      "Iteration: 03-2  (0.16s from start)\n",
      "Output Sorted Result:  (0.18s from start)\n",
      "\n",
      "\n",
      "Total result location: output-03-2.txt\n",
      "Sorted result location: result-inline.txt\n",
      "\n",
      "Total time: 0.204s\n"
     ]
    }
   ],
   "source": [
    "!rm -f output.txt\n",
    "!rm -f initGraph.txt\n",
    "\n",
    "!python Driver_Hw91.py -r inline \\\n",
    "--inputFile PageRank-test.txt \\\n",
    "--iteration 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"B\"\t0.40729180730104886\r\n",
      "\"C\"\t0.23815584801812978\r\n",
      "\"E\"\t0.11821894280902044\r\n",
      "\"A\"\t0.0640190695933832\r\n",
      "\"D\"\t0.0447357013030419\r\n",
      "\"F\"\t0.0447357013030419\r\n",
      "\"G\"\t0.016568585934478637\r\n",
      "\"H\"\t0.016568585934478637\r\n",
      "\"I\"\t0.016568585934478637\r\n",
      "\"J\"\t0.016568585934478637\r\n",
      "\"K\"\t0.016568585934478637\r\n"
     ]
    }
   ],
   "source": [
    "!cat result-inline.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it on Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/13 19:25:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw91\n",
      "16/03/13 19:25:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Init Graph Structure  (0.00s from start)\n",
      "No handlers could be found for logger \"mrjob.compat\"\n",
      "Find Graph Size  (42.91s from start)\n",
      "Number of nodes = 11\n",
      "\n",
      "Iteration 01-1  (86.91s from start)\n",
      "Iteration: 01-2  (132.21s from start)\n",
      "Iteration 02-1  (168.93s from start)\n",
      "Iteration: 02-2  (212.81s from start)\n",
      "Iteration 03-1  (248.57s from start)\n",
      "Iteration: 03-2  (290.08s from start)\n",
      "Output Sorted Result:  (327.64s from start)\n",
      "\n",
      "\n",
      "Total result location: hdfs://127.0.0.1/user/patrickng/hw91/output/iter3-2/\n",
      "Sorted result location: result-hadoop.txt\n",
      "\n",
      "Total time: 368.489s\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw91\n",
    "!hdfs dfs -mkdir hw91\n",
    "\n",
    "!python Driver_Hw91.py -r hadoop \\\n",
    "--inputFile PageRank-test.txt \\\n",
    "--initGraphPath hdfs://127.0.0.1/user/patrickng/hw91/init/ \\\n",
    "--outputPath hdfs://127.0.0.1/user/patrickng/hw91/output/ \\\n",
    "--iteration 3 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"B\"\t0.40729180730104886\r\n",
      "\"C\"\t0.23815584801812972\r\n",
      "\"E\"\t0.11821894280902044\r\n",
      "\"A\"\t0.0640190695933832\r\n",
      "\"D\"\t0.0447357013030419\r\n",
      "\"F\"\t0.0447357013030419\r\n",
      "\"K\"\t0.016568585934478637\r\n",
      "\"J\"\t0.016568585934478637\r\n",
      "\"I\"\t0.016568585934478637\r\n",
      "\"H\"\t0.016568585934478637\r\n",
      "\"G\"\t0.016568585934478637\r\n"
     ]
    }
   ],
   "source": [
    "!cat result-hadoop.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it on EMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: s3://patng323-w261-hw91/\n",
      "upload: ./PageRank-test.txt to s3://patng323-w261-hw91/input/PageRank-test.txt\n",
      "2016-03-13 17:18:04        166 PageRank-test.txt\n"
     ]
    }
   ],
   "source": [
    "!aws s3 mb s3://patng323-w261-hw91\n",
    "!aws s3 cp PageRank-test.txt s3://patng323-w261-hw91/input/\n",
    "!aws s3 ls s3://patng323-w261-hw91/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://patng323-w261-hw91/init/_SUCCESS\n",
      "delete: s3://patng323-w261-hw91/init/part-00000  \n",
      "delete: s3://patng323-w261-hw91/output/iter1-1/_SUCCESS\n",
      "delete: s3://patng323-w261-hw91/output/iter1-1/part-00000\n",
      "Wait 5.0s sec for S3 eventual consistency\n",
      "No handlers could be found for logger \"mrjob.conf\"\n",
      "Number of nodes = 11\n",
      "\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Output location= s3://patng323-w261-hw91/output/iter3-2/\n",
      "\n",
      "Total time: 1379 sec\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm --recursive s3://patng323-w261-hw91/init\n",
    "!aws s3 rm --recursive s3://patng323-w261-hw91/output\n",
    "!echo \"Wait 5.0s sec for S3 eventual consistency\"\n",
    "!sleep 5\n",
    "\n",
    "!python Driver_Hw91.py -r emr \\\n",
    "--inputFile PageRank-test.txt \\\n",
    "--initGraphPath s3://patng323-w261-hw91/init \\\n",
    "--outputPath s3://patng323-w261-hw91/output \\\n",
    "--num-ec2-instances 2 \\\n",
    "--ec2-instance-type m1.medium \\\n",
    "--iteration 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-13 17:57:16          0 _SUCCESS\r\n",
      "2016-03-13 17:57:08         63 part-00000\r\n",
      "2016-03-13 17:57:02        113 part-00001\r\n",
      "2016-03-13 17:57:09         76 part-00002\r\n",
      "2016-03-13 17:57:10        146 part-00003\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://patng323-w261-hw91/output/iter3-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]\r\n",
      "To see help text, you can run:\r\n",
      "\r\n",
      "  aws help\r\n",
      "  aws <command> help\r\n",
      "  aws <command> <subcommand> help\r\n",
      "aws: error: argument subcommand: Invalid choice, valid choices are:\r\n",
      "\r\n",
      "ls                                       | website                                 \r\n",
      "cp                                       | mv                                      \r\n",
      "rm                                       | sync                                    \r\n",
      "mb                                       | rb                                      \r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://patng323-w261-hw91/output/iter3-2/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
