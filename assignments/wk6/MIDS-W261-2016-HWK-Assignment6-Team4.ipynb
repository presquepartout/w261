{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #6=====\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hetal Chandaria, Patrick Ng, Marjorie Sayer\n",
    "\n",
    "W261 - 2 , ASSIGNMENT #6\n",
    "\n",
    "Submission Date : Feb 27, 2016\n",
    "\n",
    "Group : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW6.0. \n",
    "In mathematics, computer science, economics, or management science what is mathematical optimization? Give an example of a optimization problem that you have worked with directly or that your organization has worked on. Please describe the objective function and the decision variables. Was the project successful (deployed in the real world)? Describe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\"> Answer </span>\n",
    "\n",
    "Mathematical optimization is the selection of a best element (with regard to some criteria) from some set of available alternatives. In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW6.1 \n",
    "Optimization theory: \n",
    "For unconstrained univariate optimization what are the first order  Necessary Conditions for Optimality (FOC).  What are the second order optimality conditions (SOC)? Give a mathematical defintion. Also in python, plot the univartiate function \n",
    "X^3 -12x^2-6 defined over the real  domain -6 to +6. \n",
    "\n",
    "Also plot its corresponding first and second derivative functions. Eyeballing these graphs, identify candidate optimal points and then classify them as local minimums or maximums. Highlight and label these points in your graphs. Justify your responses using the FOC and SOC.\n",
    "\n",
    "For unconstrained multi-variate optimization what are the first order  Necessary Conditions for Optimality (FOC).  What are the second order optimality conditions (SOC)? Give a mathematical defintion. What is the Hessian matrix in this context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\"> Answer </span>\n",
    "\n",
    "#### Unconstrained univariate optimization\n",
    "\n",
    "1.  FOC : The first-order derivatives must equal zero when evaluated at the same point, called a critical point\n",
    "\n",
    "2. Take the first derivative of a function and find the function for the slope.\n",
    "\n",
    "3. Set dy/dx equal to zero, and solve for x to get the critical point or points.\n",
    "\n",
    "4.  Take the second derivative of the original function.\n",
    "\n",
    "5. SOC: The second-order direct derivatives must have the same sign when evaluated at the critical point(s).\n",
    "\n",
    "    Substitute the x from step 3 into the second derivative and solve, paying particular attention to the sign of the second derivative. \n",
    "\n",
    "6.  Use the following characteristics to determine whether the function evaluated at the critical point or points is a relative maximum or minimum:\n",
    "\n",
    "| Relative Maximum            | Relative Minimum            |\n",
    "|-----------------------------|-----------------------------|\n",
    "| f'(x=a) =0                  | f'(x=a) = 0                 |\n",
    "| f\"(x=a) <0                  | f\"(x=a) > 0                 |\n",
    "\n",
    "Refrence : http://www.columbia.edu/itc/sipa/math/calc_econ_interp_u.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy import *\n",
    "\n",
    "\n",
    "x,f = symbols('x,f')\n",
    "expr = x**3-12*(x**2)-6\n",
    "# display(expr)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(131)\n",
    "xvals = np.arange(-6, 6, 0.01)\n",
    "yvals = (xvals**3)-(12*(xvals**2))-6\n",
    "plt.plot(xvals, yvals)\n",
    "plt.annotate('max', xy=(0, 0), xytext=(0, -100),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            )\n",
    "plt.axvline(x=0,color='green',linestyle='dashed')\n",
    "plt.title('f(x):Optimiation function')\n",
    "\n",
    "\n",
    "\n",
    "expr = 3*x**2 - 24*x\n",
    "# display(expr)\n",
    "plt.subplot(132)\n",
    "markerson =[0]\n",
    "yvals = (3*xvals**2)-(24*xvals)\n",
    "plt.plot(xvals, yvals)\n",
    "plt.plot([0], [0], 'ro')\n",
    "plt.axhline(y=0,color='green',linestyle='dashed')\n",
    "plt.title('FOC')\n",
    "\n",
    "\n",
    "expr = 6*x - 24\n",
    "# display(expr)\n",
    "plt.subplot(133)\n",
    "yvals = (6*xvals)-(24)\n",
    "plt.plot(xvals, yvals)\n",
    "plt.axhline(y=-24,color='green',linestyle='dashed')\n",
    "plt.axhline(y=24,color='green',linestyle='dashed')\n",
    "plt.axhline(y=0,color='red',linestyle='dashed')\n",
    "plt.axvline(x=0,color='green',linestyle='dashed')\n",
    "plt.plot([0], [-24], 'ro')\n",
    "plt.title('SOC')\n",
    "\n",
    "#used http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.axvline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = x^3 -12x^2-6 $\n",
    "\n",
    "$f'(x) = \\frac{\\mathrm{d}  }{\\mathrm{d} x} (x^3 -12x^2-6) = 3x^2 -24x $\n",
    "\n",
    "Solving f'(x) = 0 \n",
    "\n",
    "$ 3x^2-24x = 0$\n",
    "\n",
    "$3x(x-8) = 0$\n",
    "\n",
    "$3x =0$ or $x-8 = 0$\n",
    "\n",
    "$x=0$ or $x = 8$\n",
    "\n",
    "$f\"(x) = \\frac{\\mathrm{d}^2  }{\\mathrm{d} x^2} (x^3 -12x^2-6) = \\frac{\\mathrm{d} }{\\mathrm{d} x} (3x^2 -24x) = 6x-24$\n",
    "\n",
    "* Looking at the graph of f(x): Optimization function we see that the function is maximum when x =0\n",
    "* Based on the FOC graph we can see the graph hits x at 0 when x =0\n",
    "* Based on the SOC graph we can see that when x = 0, y = -24 which proves that the point x=0 is the local maxima of the function f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unconstrained multi-variate optimization\n",
    "\n",
    "The conditions for relative maxima and minima for multivariate functions are very similar to those for univariate functions, with one additional requirement.\n",
    "\n",
    "##### FOC\n",
    "\n",
    "All first-order partial derivatives must equal zero when evaluated at the same point, called a critical point.  If we are considering a function z with two independent variables x and y, then the three-dimensional shape taken by the function z reaches a high or low point when evaluated at specific values of x and y; these values are determined by setting the first derivatives equal to zero, and then solving the resulting system of equations for the two variables.\n",
    "\n",
    "#### SOC\n",
    "The second-order direct partial derivatives must both be the same sign when evaluated at the critical point(s).  For a maximum, they must both be negative and for a minimum, both positive.  This condition serves the same purpose as the second-order derivative condition in univariate optimization.  It guarantees that the point where the slope is zero is indeed a high point, in the direction of the x variable and also in the direction of the y variable. \n",
    "\n",
    "| Relative Maximum            | Relative Minimum            |\n",
    "|-----------------------------|-----------------------------|\n",
    "| fx,fy=0                     | fx,fy=0                     |\n",
    "| fxx,fyy <0                  | fxx,fyy >0                  |\n",
    "\n",
    "\n",
    "\n",
    "Hessian Matrix:\n",
    "\n",
    "The hessian matrix is a symmetric matrix, that is \n",
    "$\\frac{\\partial^2 f(x)}{\\partial x_{i} \\partial x_{j}} = \\frac{\\partial^2 f(x)}{\\partial x_{j} \\partial x_{i}}$\n",
    "\n",
    "Hessian matrix is used to discover the nature of a stationary point for a function of several variables.\n",
    "\n",
    "Our task is equivalent to working out whether the Hessian matrix is positive definite, negative definite, or indefinite.\n",
    "\n",
    "For an n x n symmetric matrix, a kth order leading principal minor is the determinant of the matrix obtained by deleting the last (n-k) rows and columns\n",
    "\n",
    "(a) If and only if all leading principal minors of the matrix are positive, then the matrix is positive definite. For the Hessian, this implies the stationary point is a minimum.\n",
    "\n",
    "(b) If and only if the kth order leading principal minor of the matrix has sign (-1)k, then the matrix is negative definite. For the Hessian, this implies the stationary point is a maximum.\n",
    "\n",
    "(c) If none of the leading principal minors is zero, and neither (a) nor (b) holds, then the matrix is indefinite. For the Hessian, this implies the stationary point is a saddle point.\n",
    "\n",
    "Otherwise the test is inconclusive. This implies that, at a local minimum (resp. a local maximum), the Hessian is positive-semi-definite (resp. negative semi-definite).\n",
    "\n",
    "Example : \n",
    "\n",
    "Let $f(x_{1},x_{2}) = x_1^2 + 2x_1x_2+3x_2^2+4x_1+5x_2+6  $\n",
    "\n",
    "This function is differentiable and thus taking parital derivative we get below. Thus extrema can occur at points x* such that $\\nabla f(x) = 0 $\n",
    "\n",
    "$ \\nabla f(x) = \\binom{2x_1+2x_2+4}{2x_1+6x_2+5}$\n",
    "\n",
    "Solving the above we get $x_1=\\frac{-7}{4}$ and $x_2=\\frac{-1}{4}$ \n",
    "\n",
    "\n",
    "\n",
    "$H(x) = \\binom{2 \\hspace{5mm} 2}{2 \\hspace{5mm} 6}$\n",
    "\n",
    "$H(-7/4, -1/4) = \\binom{2 \\hspace{5mm} 2}{2 \\hspace{5mm} 6} $\n",
    "Its first principal minor has $det(H_1) = 2 > 0$ and its second principal minor has $det(H_2) = (2*6)-(2*2) = 12-4 = 8 $\n",
    "\n",
    "This implies that (-7/4, -1/4) is a local minimum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW6.2\n",
    "Taking x=1 as the first approximation(xt1) of a root of X^3 + 2x -4 = 0, use the Newton-Raphson method to calculate the second approximation (denoted as xt2) of this root. (Hint the solution is xt2=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\"> Answer </span>\n",
    "\n",
    "The Newton Raphson Method finds the tangent to the function f(x) at $x=x_{0}$ and extrapolates it to intersect the x-axis to get $x_{1}$\n",
    "\n",
    "Newton Raphson iteration formula is given as \n",
    "$x_{i+1}=x_{i}- \\frac{f(x_{i})}{f'(x_{i})}$\n",
    "\n",
    "$f(x) = x^{3}+2x-4=0 $\n",
    "\n",
    "$\\frac{\\mathrm{d} (x^{3}+2x-4)}{\\mathrm{d} x}=3x^{2}+2$\n",
    "\n",
    "$xt2 = xt1 - \\frac{f(x=1)}{f'(x=1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def nr(x):\n",
    "    fx = x**3+2*x-4\n",
    "    der_fx = 3*x**2+2\n",
    "    return (x- (fx/der_fx))\n",
    "nr(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW6.3 Convex optimization \n",
    "What makes an optimization problem convex? What are the first order  Necessary Conditions for Optimality in convex optimization.  What are the second order optimality conditions for convex optimization? Are both necessary to determine the maximum or minimum of candidate optimal solutions?\n",
    "\n",
    "Fill in the BLANKS here:\n",
    "Convex minimization, a subfield of optimization, studies the problem of minimizing BLANK functions over BLANK sets. The BLANK property can make optimization in some sense \"easier\" than the general case - for example, any local minimum must be a global minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\"> Answer </span>\n",
    "\n",
    "An optimization problem is convex if \n",
    "\n",
    "1. Its objective is a convex function \n",
    "2. inequality constraints $f_{j}$ are convex\n",
    "3. the equality constraints $h_{j}$ are affine\n",
    "\n",
    "A convex function is one where the line segment connecting two points 􏱕$(x􏱫,f(􏱕x))$􏱖􏱖 and (􏱕y,􏱫f(􏱕y)) 􏱖lies above the function􏱚 Mathematically􏱘 a function f is convex if􏱘 for all (x􏱫, y) and all 􏱜$0<\\alpha<1$􏱬 􏱝􏱘\n",
    "\n",
    "$f(\\alpha x + (1-\\alpha)y) \\leq \\alpha f(x) + (1-\\alpha) f(y)$\n",
    "\n",
    "##### First Order Optimality condition\n",
    "A function f(x), which is differentiable, is convex if and only if its domain is a convex set and if the following inequality condition is satisfied:\n",
    "$f\\left(y\\right) \\; \\geq \\; f\\left(x\\right) + \\left( \\nabla_x f\\left(x\\right) \\right)^T \\left(y - x\\right) ; \\;\\;\\;\\; \\forall x,y \\in Domain(f) $\n",
    "\n",
    "The first order condition for convexity says that f is convex if and only if the tangent line is a global underestimator of the function f. In other words, if we take our function and draw a tangent line at any point, then every point on this line will lie below the corresponding point on f.\n",
    "\n",
    "##### Second Order Optimality condition\n",
    "There is an easy way to check for convexity when f is twice di􏱍fferentiable􏱋,the function f is convex on domain [a,b] if (and only if) f\"(x) >= 0 for all x in the domain.\n",
    "\n",
    "If f(x) is convex, then any local minimum is also global minimum.\n",
    "\n",
    "#### Fill in the blanks\n",
    "\n",
    "Convex minimization, a subfield of optimization, studies the problem of minimizing <b>convex</b> functions over <b>convex</b> sets. The <b>convexity</b> property can make optimization in some sense \"easier\" than the general case - for example, any local minimum must be a global minimum.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 6.4\n",
    "The learning objective function for weighted ordinary least squares (WOLS) (aka weight linear regression) is defined as follows:\n",
    "\n",
    "0.5* sumOverTrainingExample i (weight_i * (W * X_i - y_i)^2)\n",
    "\n",
    "Where training set consists of input variables X ( in vector form) and a target variable y, and W is the vector of coefficients for the linear regression model.\n",
    "\n",
    "Derive the gradient for this weighted OLS by hand; showing each step and also explaining each step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\"> Answer </span>\n",
    "\n",
    "1. X = Training set of Input vairables in vector form\n",
    "2. y = Target variable\n",
    "3. W = coefficients of the linear regression model\n",
    "4. w = weight vector\n",
    "5. n = number of features\n",
    "6. m = total number of records\n",
    "\n",
    "Now we will define the cost or learning objective function for Weighted ordinary least squares as \n",
    "\n",
    "$ J(W) = \\frac{1}{2}\\sum_{i} w_{i}(WX_{i}-y_{i})^{2} $\n",
    "\n",
    "Derivation of Gradient descent: \n",
    "\n",
    "1. Start with an initial value of W and repeatedly perform the update as below until we have a value of W such that J(W) is minimized\n",
    "\n",
    "$W_{j} = W_{j} - \\alpha \\frac{\\partial J(W)}{\\partial W} $ \n",
    "\n",
    "This update is simultaneously performed for all values of j = 0,...,n\n",
    "\n",
    "2. To implement the above algorithm we need to derive the partial derivative .\n",
    "\n",
    "$\\frac{\\partial J(W)}{\\partial W} =  \\frac{\\partial  }{\\partial W}\\frac{1}{2}\\sum_{i} w_{i}(WX_{i}-y_{i})^{2}$\n",
    "\n",
    "$ = 2 \\frac{1}{2} w_{i}(WX_{i}-y_{i}) \\frac{\\partial  }{\\partial W}\\sum_{i} (WX_{i}-y_{i})$\n",
    "\n",
    "$ = w_{i}(WX_{i}-y_{i})* X_{i}$\n",
    "\n",
    "$ = (WX_{i}-y_{i})w_{i}X_{i}$\n",
    "\n",
    "3. Repeat until convergence {\n",
    "\n",
    "$W_{j} = W_{j} - \\alpha \\sum_{i}(WX_{i}-y_{i})w_{i}X_{i} $\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 6.5\n",
    "Write a MapReduce job in MRJob to do the training at scale of a weighted OLS model using gradient descent.\n",
    "\n",
    "Generate one million datapoints just like in the following notebook:  http://nbviewer.ipython.org/urls/dl.dropbox.com/s/kritdm3mo1daolj/MrJobLinearRegressionGD.ipynb\n",
    "\n",
    "Weight each example as follows: \n",
    "\n",
    "weight(x)= abs(1/x)\n",
    "\n",
    "Sample 1% of the data in MapReduce and use the sampled dataset to train a (weighted if available in SciKit-Learn) linear regression model locally using  SciKit-Learn (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "Plot the resulting weighted linear regression model versus the original model that you used to generate the data. Comment on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\">Answer </span>\n",
    "\n",
    "Data Information:\n",
    "\n",
    "Sizes: 1000000 points\n",
    "True model: y = 1.0 * x - 4\n",
    "Noise：Normal Distributed mean = 0, var = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab \n",
    "import random \n",
    "\n",
    "size = 1000000\n",
    "sample_per = 1\n",
    "x = np.random.uniform(-4, 4, size)\n",
    "y = x * 1.0 - 4 + np.random.normal(0,0.5,size)\n",
    "data = zip(y,x)\n",
    "np.savetxt('LinearRegression.csv',data,delimiter = \",\")\n",
    "\n",
    "\n",
    "# pylab.plot(x, y,'*')\n",
    "# pylab.title(\"Original Model\")\n",
    "# pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile MrJobBatchGD_LinearReg.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "# This MrJob calculates the gradient of the entire training set \n",
    "#     Mapper: calculate partial gradient for each example  \n",
    "\n",
    "class MRJob_GD_LR(MRJob):\n",
    "    def steps(self):\n",
    "        return [MRStep(mapper_init=self.read_weights,\n",
    "                       mapper=self.partial_gradient,\n",
    "                       mapper_final=self.partial_gradient_emit,\n",
    "                       reducer=self.gradient_accumulater,\n",
    "                       jobconf={\n",
    "                        \"mapred.map.tasks\":4,\n",
    "                        \"mapred.reduce.tasks\":1\n",
    "                        }\n",
    "                      )] \n",
    "    \n",
    "    def read_weights(self):\n",
    "        # Read initial_seed file\n",
    "        with open('initial_seed.txt', 'r') as f:\n",
    "            self.seed = [float(v) for v in f.readline().split(',')]\n",
    "        # Initialze gradient for this iteration\n",
    "        self.partial_Gradient = [0]*len(self.seed)\n",
    "        self.partial_count = 0\n",
    "    \n",
    "    def partial_gradient(self,_,line):\n",
    "        y_l,x_l = map(float,line.split(','))\n",
    "        #assume equation is of the form y=b0+b1x. Get values of b0 and b1 from the intial seed\n",
    "        b0,b1 = self.seed\n",
    "        \n",
    "        #calculate weight as abs(1/x)\n",
    "        w_l = abs(1/x_l)\n",
    "        \n",
    "        # y_hat is the predicted value given current weights\n",
    "        y_hat = b0+b1*x_l\n",
    "        \n",
    "        # Update parial gradient vector with gradient form current example\n",
    "        self.partial_Gradient =  [self.partial_Gradient[0]+ (y_l-y_hat)*w_l, self.partial_Gradient[1]+(y_l-y_hat)*x_l*w_l]\n",
    "        self.partial_count = self.partial_count + 1\n",
    "        #yield None, (D[0]-y_hat,(D[0]-y_hat)*D[1],1)\n",
    "    \n",
    "    # Finally emit in-memory partial gradient and partial count\n",
    "    def partial_gradient_emit(self):\n",
    "        yield None, (self.partial_Gradient,self.partial_count)\n",
    "        \n",
    "    # Accumulate partial gradient from mapper and emit total gradient \n",
    "    # Output: key = None, Value = gradient vector\n",
    "    def gradient_accumulater(self, _, partial_Gradient_Record): \n",
    "        total_gradient = [0]*2\n",
    "        total_count = 0\n",
    "        for partial_Gradient,partial_count in partial_Gradient_Record:\n",
    "            total_count = total_count + partial_count\n",
    "            total_gradient[0] += partial_Gradient[0]\n",
    "            total_gradient[1] +=partial_Gradient[1]\n",
    "        yield None, [v/total_count for v in total_gradient]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRJob_GD_LR.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import random,array\n",
    "from MrJobBatchGD_LinearReg import MRJob_GD_LR\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "learning_rate = 0.05\n",
    "stop_criteria = 0.000005\n",
    "\n",
    "# Generate random values as inital seed for b0 and b1\n",
    "weights = array([random.uniform(-3,3),random.uniform(-3,3)])\n",
    "# Write the weights to the files\n",
    "with open('initial_seed.txt', 'w+') as f:\n",
    "    f.writelines(','.join(str(j) for j in weights))\n",
    "\n",
    "# create a mrjob instance for batch gradient descent update over all data\n",
    "mr_job = MRJob_GD_LR(args=['LinearRegression.csv','--file','initial_seed.txt'])\n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    print \"iteration =\"+str(i)+\"  weights =\",weights\n",
    "    # Save weights from previous iteration\n",
    "    weights_old = weights\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            # value is the gradient value\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            # Update weights\n",
    "            weights = weights + learning_rate*array(value)\n",
    "    i = i + 1\n",
    "    # Write the updated weights to file \n",
    "    with open('initial_seed.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in weights))\n",
    "    # Stop if weights get converged\n",
    "    if(sum((weights_old-weights)**2)<stop_criteria):\n",
    "        break\n",
    "        \n",
    "print \"Final weights\\n\"\n",
    "print weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span style = \"color: blue\"> SK Learn Regression Model</span> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "\n",
    "np.random.seed(0)  #  To ensure repeatability of results\n",
    "shuffle = np.random.permutation(np.arange(np.array(x).shape[0]))\n",
    "X,Y=x[shuffle],y[shuffle] #shuffle X and Y and get random sample \n",
    "sample_per = 1\n",
    "sample_sz = size*sample_per/100\n",
    "X,Y=X[:sample_sz].reshape((sample_sz,1)),Y[:sample_sz].reshape((sample_sz,1))\n",
    "\n",
    "#generate weight \n",
    "w=abs(1/X).ravel()\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X, Y,sample_weight=w)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients:', regr.intercept_[0],regr.coef_[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = x * 1.0 - 4\n",
    "mr_gd_lr = x * 1.00690185 - 3.94910008\n",
    "sk_lr = X * 1.0006482340101668 - 3.9707791992944776\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,6))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(x, y,'o',mfc='none',alpha=0.1)\n",
    "plt.plot(x,y_hat,linestyle='dashed',color='green')\n",
    "plt.title(\"Original Model\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(x, y,'o',mfc='none',alpha=0.1)\n",
    "plt.plot(x,mr_gd_lr,linestyle='dashed',color='green')\n",
    "plt.title(\"MR Job\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(X, Y,'o',mfc='none',alpha=0.4)\n",
    "plt.plot(X,sk_lr,linestyle='dashed',color='green')\n",
    "plt.title(\"SK Learn\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(x,y_hat,linestyle='dashed',color='black',label='Original Model',alpha=0.5)\n",
    "plt.plot(x,mr_gd_lr,linestyle='dotted',color='red',label='MR Job')\n",
    "plt.plot(X,sk_lr,color='blue',label='SK Learn',alpha=0.5)\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW6.5.1 (Optional)\n",
    "Using MRJob and in Python, plot the error surface for the weighted linear regression model using a heatmap and contour plot. \n",
    "Also plot the current model in the original domain space.  (Plot them side by side if possible)\n",
    "Plot the path to convergence (during training) for the weighted linear regression model in plot error space and in the original domain space. Make sure to label your plots with iteration numbers, function, model space versus original domain space, etc.\n",
    "Comment on convergence and on the mean squared error using your weighted OLS algorithm on the weighted dataset versus using the weighted OLS algorithm on the uniformly weighted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW6.6 Clean up notebook for GMM via EM\n",
    "\n",
    "Using the following notebook as a starting point:\n",
    "\n",
    "http://nbviewer.jupyter.org/urls/dl.dropbox.com/s/0t7985e40fovlkw/EM-GMM-MapReduce%20Design%201.ipynb \n",
    "\n",
    "Improve this notebook as follows:\n",
    "\n",
    "-- Add in equations into the notebook (not images of equations) \n",
    "\n",
    "-- Number the equations\n",
    "\n",
    "-- Make sure the equation notation matches the code and the code and comments refer to the equations numbers\n",
    "\n",
    "-- Comment the code\n",
    "\n",
    "-- Rename/Reorganize the code to make it more readable\n",
    "\n",
    "-- Rerun the examples similar graphics (or possibly better graphics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\"> Answer </span>\n",
    "\n",
    "Please use the <a href=\"http://nbviewer.jupyter.org/github/hchandaria/UCB_MIDS_W261/blob/master/hw6/EM-GMM-MapReduce%20Design%201_Modified.ipynb\" >link</a> to see the modified ipython notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW6.7  Implement Bernoulli Mixture Model via EM\n",
    "\n",
    "Implement the EM clustering algorithm to determine Bernoulli Mixture Model for discrete data in MRJob.\n",
    "\n",
    "As a unit test use the dataset in the following slides:\n",
    "\n",
    "https://www.dropbox.com/s/maoj9jidxj1xf5l/MIDS-Live-Lecture-06-EM-Bernouilli-MM-Systems-Test.pdf?dl=0\n",
    "\n",
    "Cross-check that you get the same cluster assignments and cluster Bernouilli models as presented in the slides after 25 iterations. Dont forget the smoothing.\n",
    "\n",
    "As a full test: use the same dataset from HW 4.5, the Tweet Dataset. \n",
    "Using this data, you will implement a 1000-dimensional EM-based Bernoulli Mixture Model  algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using K = 4.  Use the same smoothing as in the unit test.\n",
    "\n",
    "Repeat this experiment using your KMeans MRJob implementation fron HW4.\n",
    "Report the rand index score using the class code as ground truth label for both algorithms and comment on your findings.\n",
    "\n",
    "Here is some more information on the Tweet Dataset.\n",
    "\n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of  recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Mixture Model\n",
    "\n",
    "In a Bernoulli Mixture Model a document is a vector of Booleans indicating the presence of a term. In this model, we generate a document by first picking a cluster k with prob- ability αk and then generating the terms of the document according to the parameters qmk.\n",
    "\n",
    "\n",
    "The mixture model is  :\n",
    "\n",
    "\n",
    "\n",
    "$P(d|w_k ; \\theta) = \\sum_{k=1}^{K} \\alpha_k (\\prod_{t_m  \\in {d}  } q_{mk}) ( \\prod_{t_m  \\notin {d}  } (1- q_{mk}   )) \\Rightarrow $ equation 1\n",
    "\n",
    "Where,\n",
    "\n",
    "K = number of clusters\n",
    "\n",
    "\n",
    "The maximization step recomputes the conditional parameters $q_{mk}$ and the priors $\\alpha_k $ as follows:\n",
    "\n",
    "$q_{mk} = \\frac {\\sum_{n=1}^{N}(r_{nk}  )I(t_m \\in d_n) }{\\sum_{n=1}^{N}r_{nk}}  \\Rightarrow$ equation 2\n",
    "\n",
    "$\\alpha_k = \\frac {\\sum_{n=1}^{N}r_{nk}} {N} \\Rightarrow $ equation 3\n",
    "\n",
    "Please note that IR example has smoothing  $\\epsilon$ added in both equation 2 and 3 for but we have done that in the E step.\n",
    "\n",
    "The expectation step computes the soft assignment of documents to clusters given the current parameters $q_{mk}$ and $\\alpha_k$\n",
    "\n",
    "Then, the probability of a document given its class  is simply the product of the probability of the attribute values over all word attributes:\n",
    "\n",
    "$P(d_i|w_k;\\theta) = \\prod_{t=1}^{|V|}$\n",
    "\n",
    "Bit, is either 0 or 1, indicating whether word t_m occurs at least once in the document.\n",
    "\n",
    "$r_{nk} = \\frac{\\alpha_k (\\prod_{t_m  \\in {d}  } q_{mk}) ( \\prod_{t_m  \\notin {d}  } (1- q_{mk}))} {\\sum_{k=1}^{K}\\alpha_k (\\prod_{t_m  \\in {d}  } q_{mk}) ( \\prod_{t_m  \\notin {d}  } (1- q_{mk}))} \\Rightarrow $ equation 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile BMMtest.txt\n",
    "hot chocolate cocoa beans\n",
    "cocoa ghana africa\n",
    "beans harvest ghana \n",
    "cocoa butter\n",
    "butter truffles\n",
    "sweet chocolate\n",
    "sweet sugar\n",
    "sugar cane brazil\n",
    "sweet sugar beet\n",
    "sweet cake icing\n",
    "cake black forest\n",
    "￼￼￼￼￼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BMM Initialization for IR example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mr_BMMEmInitialize.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "from numpy import mat, zeros, shape, random, array, zeros_like, dot, linalg\n",
    "from random import sample\n",
    "import json, re\n",
    "from math import pi, sqrt, exp, pow\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MrBMM_EmInit(MRJob):\n",
    "#     DEFAULT_PROTOCOL = 'json'\n",
    "    vocab={}\n",
    "    qmk ={}\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MrBMM_EmInit, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.numMappers = 1     #number of mappers\n",
    "        self.count = 0\n",
    "        \n",
    "        \n",
    "                                                 \n",
    "    def configure_options(self):\n",
    "        super(MrBMM_EmInit, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--k', dest='k', default=2, type='int',\n",
    "            help='k: number of densities in mixture')\n",
    "        self.add_passthrough_option(\n",
    "            '--pathName', dest='pathName', default=\"\", type='str',\n",
    "            help='pathName: pathname where intermediateResults_BMM.txt is stored')\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        for i in range (0,self.options.k):\n",
    "            self.vocab[i]={}\n",
    "        #build vocab first\n",
    "        with open('BMMtest.txt') as f:\n",
    "            for line in f:\n",
    "                words = re.findall(WORD_RE,line)\n",
    "                for word in words:\n",
    "                    for i in range(self.options.k):\n",
    "                        if word in self.vocab[i]:\n",
    "                            continue\n",
    "                        self.vocab[i][word]=0\n",
    "    \n",
    "    def mapper(self, key, xjIn):\n",
    "        #something simple to grab random starting point\n",
    "        #collect the first 2k\n",
    "        self.count += 1\n",
    "        if self.count == 6:         \n",
    "            yield (0,xjIn) \n",
    "        if self.count ==7:\n",
    "            yield(1,xjIn)\n",
    "        \n",
    "    def reducer(self, key, xjIn):\n",
    "        for xj in xjIn:\n",
    "            words = re.findall(WORD_RE,xj)\n",
    "            for word in words:\n",
    "                if word not in self.vocab[key]:\n",
    "                    continue\n",
    "                self.vocab[key][word]=1\n",
    "                yield key, word\n",
    "                \n",
    "        jDebug = json.dumps([self.vocab])    \n",
    "        debugPath = self.options.pathName + 'debug_BMM.txt'\n",
    "        fileOut = open(debugPath,'w')\n",
    "        fileOut.write(jDebug)\n",
    "        fileOut.close()\n",
    "        \n",
    "        #also need a starting guess at the phi's - prior probabilities\n",
    "        #initialize them all with the same number - 1/k - equally probably for each cluster\n",
    "        \n",
    "        alpha = zeros(self.options.k,dtype=float)\n",
    "        \n",
    "        for i in range(self.options.k):\n",
    "            alpha[i] = 1.0/float(self.options.k)\n",
    "        \n",
    "        #form output object\n",
    "        outputList = [alpha.tolist(),self.vocab]\n",
    "            \n",
    "        jsonOut  = json.dumps(outputList)\n",
    "        \n",
    "        #write new parameters to file\n",
    "        fullPath = self.options.pathName + 'intermediateResults_BMM.txt'\n",
    "        fileOut = open(fullPath,'w')\n",
    "        fileOut.write(jsonOut)\n",
    "        fileOut.close()\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MrBMM_EmInit.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python mr_BMMEmInitialize.py --pathName '/Users/hetal/programming/W261/UCB_MIDS_W261/hw6/' --file 'BMMtest.txt' -q BMMtest.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mr_BMixEmIterate.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "from math import sqrt, exp, pow,pi\n",
    "from numpy import zeros, shape, random, array, zeros_like, dot, linalg, log,exp\n",
    "import numpy as np\n",
    "import json, re\n",
    "import collections\n",
    "from decimal import *\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# @terms = terms present in the incoming document\n",
    "# @alpha = prior probability for cluster k\n",
    "# @qmk_local = vocab probability for cluster k\n",
    "# return partially calculated  rnk (numerator)\n",
    "def bernoulli(terms, alpha,qmk_local,n,k,epsilon):\n",
    "    lprob = 0.0 #log probability\n",
    "    for key,value in qmk_local.iteritems():\n",
    "        if key not in terms: #if we have never seen the term in the document \n",
    "            continue\n",
    "        lprob += log(terms[key]*(value + epsilon) + ((1-terms[key] )*(1-value + epsilon)))\n",
    "    lprob =  Decimal(log(alpha)+lprob).exp()\n",
    "    return lprob\n",
    "        \n",
    "        \n",
    "\n",
    "class MrBMixEm(MRJob):\n",
    "    DEFAULT_PROTOCOL = 'json'\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MrBMixEm, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        fullPath = self.options.pathName + 'intermediateResults_BMM.txt'\n",
    "        fileIn = open(fullPath)\n",
    "        inputJson = fileIn.read()\n",
    "        fileIn.close()\n",
    "        inputList = json.loads(inputJson)\n",
    "        temp = inputList[0]        \n",
    "        self.alpha = array(temp)           #prior class probabilities\n",
    "        temp = inputList[1] #entire vocab         \n",
    "        \n",
    "        self.qmk = {int(k):v for k,v in temp.items()}\n",
    "        \n",
    "        self.vocab = []\n",
    "        self.new_qmk = {}\n",
    "        for i in range(self.options.k):\n",
    "            self.new_qmk[i]={}\n",
    "        for key,value in self.qmk.iteritems():\n",
    "            for term,val in value.iteritems():\n",
    "                self.new_qmk[key][term]=0\n",
    "                if(val != 0):\n",
    "                    self.vocab.append(term)\n",
    "                \n",
    "        self.vocab =  set(self.vocab)\n",
    "            \n",
    "        self.new_alphas = zeros_like(self.alpha)        #partial weighted sum of weights\n",
    "        \n",
    "        self.numMappers = 1             #number of mappers\n",
    "        self.count = 0                  #passes through mapper\n",
    "\n",
    "        \n",
    "    def configure_options(self):\n",
    "        super(MrBMixEm, self).configure_options()\n",
    "\n",
    "        self.add_passthrough_option(\n",
    "            '--k', dest='k', default=2, type='int',\n",
    "            help='k: number of densities in mixture')\n",
    "        self.add_passthrough_option(\n",
    "            '--pathName', dest='pathName', default=\"\", type='str',\n",
    "            help='pathName: pathname where intermediateResults.txt is stored')\n",
    "        \n",
    "    def mapper(self, key, val):\n",
    "        smoothing = 0.0001\n",
    "        #accumulate partial sums for each mapper\n",
    "        val = re.findall(WORD_RE,val)\n",
    "        stripe ={k: 0 for k in self.vocab} \n",
    "        \n",
    "    \n",
    "        for word in val:\n",
    "            if word not in stripe:\n",
    "                continue\n",
    "            stripe[word]=1\n",
    "        rnk=zeros_like(self.alpha)\n",
    "        \n",
    "        for i in range(0,self.options.k):\n",
    "            rnk[i] = bernoulli(stripe,self.alpha[i],self.qmk[i],self.count,i,smoothing)\n",
    "\n",
    "        rnk = rnk/sum(rnk)\n",
    "   \n",
    "        #increment counts \n",
    "        self.count += 1\n",
    "        \n",
    "        #accumulate new alpha \n",
    "        self.new_alphas = self.new_alphas + (rnk)# + smoothing)\n",
    "        \n",
    "        \n",
    "        print \"Doc Probability: \",self.count,[round(i,4) for i in rnk]\n",
    "        \n",
    "        for i in range(self.options.k):\n",
    "            for key in val:\n",
    "                self.new_qmk[i][key] +=  (rnk[i])# + smoothing)\n",
    "#         dummy yield - real output passes to mapper_final in self\n",
    "\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        \n",
    "        out = [self.count, (self.new_alphas).tolist(),self.new_qmk ]\n",
    "        jOut = json.dumps(out)\n",
    "        yield 1,jOut\n",
    "    \n",
    "    \n",
    "    def reducer(self, key, xs):\n",
    "        #accumulate partial sums\n",
    "        first = True        \n",
    "        #accumulate partial sums\n",
    "        #xs us a list of paritial stats, including count, phi, mean, and covariance. \n",
    "        #Each stats is k-length array, storing info for k components\n",
    "        for val in xs:\n",
    "            if first:\n",
    "                temp = json.loads(val)\n",
    "                #totCount, totPhi, totMeans, and totCov are all arrays\n",
    "                totCount = temp[0]\n",
    "                totalpha = array(temp[1])\n",
    "                totqmk = {int(k):v for k,v in temp[2].items()}            \n",
    "                first = False\n",
    "            else:\n",
    "                temp = json.loads(val)\n",
    "                #cumulative sum of four arrays\n",
    "                totCount = totCount + temp[0]\n",
    "                totalpha = totalpha + array(temp[1])\n",
    "                local_qmk = {int(k):v for k,v in temp[2].items()}\n",
    "                for key,value in totqmk.iteritems():\n",
    "                    for term,count in value.iteritems():\n",
    "                        totqmk[key][term] = count + local_qmk[key][term]\n",
    "                \n",
    "        #finish calculation of new probability parameters. array divided by array\n",
    "        newAlpha = totalpha/totCount\n",
    "        print array(newAlpha).tolist()\n",
    "        #initialize these to something handy to get the right size arrays\n",
    "        newqmk = {}\n",
    "        for i in range(self.options.k):\n",
    "            newqmk[i]={}\n",
    "            for key,value in totqmk[i].iteritems():\n",
    "                newqmk[i][key] = value/totalpha[i]\n",
    "            \n",
    "        outputList = [newAlpha.tolist(),newqmk]\n",
    "        jsonOut = json.dumps(outputList)\n",
    "        \n",
    "        #write new parameters to file\n",
    "        fullPath = self.options.pathName + 'intermediateResults_BMM.txt'\n",
    "        fileOut = open(fullPath,'w')\n",
    "        fileOut.write(jsonOut)\n",
    "        fileOut.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MrBMixEm.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python mr_BMixEmIterate.py --file intermediateResults_BMM.txt --pathName '/Users/hetal/programming/W261/UCB_MIDS_W261/hw6/' -q BMMtest.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mr_BMMEmInitialize import MrBMM_EmInit\n",
    "from mr_BMixEmIterate import MrBMixEm\n",
    "import json\n",
    "from math import sqrt\n",
    "\n",
    "#function to calculate distance\n",
    "def dist(x,y):\n",
    "    #euclidean distance between two lists    \n",
    "    sum = 0.0\n",
    "    for i in range(len(x)):\n",
    "        temp = x[i] - y[i]\n",
    "        sum += temp * temp\n",
    "    return sqrt(sum)\n",
    "\n",
    "#first run the initializer to get starting centroids\n",
    "filePath = 'BMMtest.txt'\n",
    "mrJob = MrBMM_EmInit(args=[filePath,'--file',filePath,'--pathName','/Users/hetal/programming/W261/UCB_MIDS_W261/hw6/'])\n",
    "with mrJob.make_runner() as runner:\n",
    "    runner.run()\n",
    "\n",
    "#pull out the centroid values to compare with values after one iteration\n",
    "emPath = \"intermediateResults_BMM.txt\"\n",
    "fileIn = open(emPath)\n",
    "paramJson = fileIn.read()\n",
    "fileIn.close()\n",
    "\n",
    "k = 2\n",
    "\n",
    "delta = 10\n",
    "iter_num = 0\n",
    "#Begin iteration on change in centroids\n",
    "while iter_num <= 24:\n",
    "    print \"Iteration\" + str(iter_num)\n",
    "    iter_num = iter_num + 1\n",
    "    #parse old centroid values\n",
    "    oldParam = json.loads(paramJson)\n",
    "    #run one iteration\n",
    "    oldMeans = oldParam[1]\n",
    "    old_qmk_dict = {int(k):v for k,v in oldMeans.items()}\n",
    "    \n",
    "    mrJob2 = MrBMixEm(args=[filePath,'--file',emPath,'--pathName','/Users/hetal/programming/W261/UCB_MIDS_W261/hw6/'])\n",
    "    with mrJob2.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "    #compare new centroids to old ones\n",
    "    fileIn = open(emPath)\n",
    "    paramJson = fileIn.read()\n",
    "    fileIn.close()\n",
    "    newParam = json.loads(paramJson)\n",
    "\n",
    "    k_means = len(newParam[1])\n",
    "    newMeans = newParam[1]\n",
    "    new_qmk_dict = {int(k):v for k,v in newMeans.items()}\n",
    "\n",
    "    #convert dictionary to array so we can compute distance\n",
    "    new_qmk = old_kmk =  [[]*k for x in xrange(k)]\n",
    "    for i in range(k):\n",
    "        for key , value in new_qmk_dict[i].iteritems() :\n",
    "            new_qmk[i].append(float(value))\n",
    "        for key , value in old_qmk_dict[i].iteritems() :\n",
    "            old_qmk[i].append(float(value))\n",
    "    \n",
    "    delta = 0.0\n",
    "    for i in range(k_means):\n",
    "        delta += dist(new_qmk[i],old_qmk[i])\n",
    "\n",
    "    print old_qmk_dict\n",
    "\n",
    "print \"Iteration\" + str(iter_num)\n",
    "print new_qmk_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initalization Code for Tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mr_BMMEmInitialize.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import json, re\n",
    "from math import pi, sqrt, exp, pow\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MrBMM_EmInit(MRJob):\n",
    "#     DEFAULT_PROTOCOL = 'json\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MrBMM_EmInit, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.numMappers = 1     #number of mappers\n",
    "        self.count = 0\n",
    "        \n",
    "                                                 \n",
    "    def configure_options(self):\n",
    "        super(MrBMM_EmInit, self).configure_options()\n",
    "        self.add_passthrough_option(\n",
    "            '--k', dest='k', default=4, type='int',\n",
    "            help='k: number of densities in mixture')\n",
    "        self.add_passthrough_option(\n",
    "            '--pathName', dest='pathName', default=\"\", type='str',\n",
    "            help='pathName: pathname where intermediateResults_BMM.txt is stored')  \n",
    "    \n",
    "    def mapper(self, key, xjIn):\n",
    "        if self.count <= 2*self.options.k:\n",
    "            self.count += 1\n",
    "            yield (1,xjIn) \n",
    "\n",
    "    def reducer(self, key, xjIn):\n",
    "        cent=[]\n",
    "        for xj in xjIn:\n",
    "            terms = xj.strip().split(',')[3:]\n",
    "            cent.append(terms) #append the data points \n",
    "        index = sample(range(len(cent)), self.options.k) #based on the number of clusters, select those many points randomly\n",
    "        \n",
    "        qmk =np.zeros([self.options.k,len(cent[0])])\n",
    "        qmk_i = 0\n",
    "        for i in index:\n",
    "            for j in range(len(cent[i])):\n",
    "                if (cent[i][j] == '0' ):\n",
    "                    continue\n",
    "                qmk[qmk_i][j]=1\n",
    "            qmk_i +=1\n",
    "               \n",
    "        #also need a starting guess at the phi's - prior probabilities\n",
    "        #initialize them all with the same number - 1/k - equally probably for each cluster\n",
    "        \n",
    "        alpha = np.zeros(self.options.k,dtype=float)\n",
    "        \n",
    "        for i in range(self.options.k):\n",
    "            alpha[i] = 1.0/float(self.options.k)\n",
    "        \n",
    "        #form output object\n",
    "        outputList = [alpha.tolist(),qmk.tolist()]\n",
    "            \n",
    "        jsonOut  = json.dumps(outputList)\n",
    "        \n",
    "        #write new parameters to file\n",
    "        fullPath = self.options.pathName + 'intermediateResults_BMM_tweet.txt'\n",
    "        fileOut = open(fullPath,'w')\n",
    "        fileOut.write(jsonOut)\n",
    "        fileOut.close()\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MrBMM_EmInit.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python mr_BMMEmInitialize.py --pathName '/Users/hetal/programming/W261/UCB_MIDS_W261/hw6/' -q topUsers_Apr-Jul_2014_1000-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr_BMixEmIterate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_BMixEmIterate.py\n",
    "from __future__ import division\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "from math import sqrt, exp, pow,pi\n",
    "from numpy import zeros, shape, random, array, zeros_like, dot, linalg, log,exp\n",
    "import numpy as np\n",
    "import json, re\n",
    "import collections\n",
    "from decimal import *\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# @terms = terms present in the incoming document\n",
    "# @alpha = prior probability for cluster k\n",
    "# @qmk_local = vocab probability for cluster k\n",
    "#@epsilon = smoothing\n",
    "# return partially calculated  rnk (numerator)\n",
    "def bernoulli(terms, alpha,qmk_local,epsilon):\n",
    "    lprob = 0.0 #log probability\n",
    "    for i in range(len(qmk_local)):\n",
    "        if np.isnan(terms[i]) : #if we have never seen the term in the document \n",
    "            continue\n",
    "        lprob += log(terms[i]*(qmk_local[i] + epsilon) + ((1-terms[i] )*(1-qmk_local[i] + epsilon)))\n",
    "    lprob =  Decimal(log(alpha)+lprob).exp()\n",
    "    return lprob\n",
    "        \n",
    "        \n",
    "\n",
    "class MrBMixEm(MRJob):\n",
    "    DEFAULT_PROTOCOL = 'json'\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MrBMixEm, self).__init__(*args, **kwargs)\n",
    "        \n",
    "        fullPath = self.options.pathName + 'intermediateResults_BMM_tweet.txt'\n",
    "        fileIn = open(fullPath)\n",
    "        inputJson = fileIn.read()\n",
    "        fileIn.close()\n",
    "        inputList = json.loads(inputJson)\n",
    "        temp = inputList[0]        \n",
    "        self.alpha = array(temp)           #prior class probabilities\n",
    "        self.qmk = inputList[1] #entire vocab   \n",
    "        self.vocab = [] # for the first round determing which indexes have data\n",
    "        self.new_qmk = zeros_like(self.qmk)\n",
    "        for i in range(len(self.qmk)) :\n",
    "            for j in range(len(self.qmk[i])):\n",
    "                if(self.qmk[i][j] != 0):\n",
    "                    self.vocab.append(j)\n",
    "                \n",
    "        self.vocab =  set(self.vocab)\n",
    "            \n",
    "        self.new_alphas = zeros_like(self.alpha)        #partial weighted sum of weights\n",
    "        \n",
    "#         self.numMappers = 1             #number of mappers\n",
    "        self.count = 0                  #passes through mapper\n",
    "\n",
    "        \n",
    "    def configure_options(self):\n",
    "        super(MrBMixEm, self).configure_options()\n",
    "\n",
    "        self.add_passthrough_option(\n",
    "            '--k', dest='k', default=4, type='int',\n",
    "            help='k: number of densities in mixture')\n",
    "        self.add_passthrough_option(\n",
    "            '--pathName', dest='pathName', default=\"\", type='str',\n",
    "            help='pathName: pathname where intermediateResults.txt is stored')\n",
    "        \n",
    "    def mapper(self, key, line):\n",
    "        smoothing = 0.0001\n",
    "        #accumulate partial sums for each mapper\n",
    "        val = line.strip().split(',')[3:]\n",
    "        stripe=zeros([len(val)])\n",
    "        \n",
    "        for i in range(len(val)):\n",
    "            if i not in self.vocab:\n",
    "                continue\n",
    "            if val[i] != '0' :\n",
    "                stripe[i] = 1.0\n",
    "            else : stripe[i]= None\n",
    "    \n",
    "        rnk=zeros_like(self.alpha)\n",
    "        \n",
    "        for i in range(0,self.options.k):\n",
    "            rnk[i] = bernoulli(stripe,self.alpha[i],self.qmk[i],smoothing)\n",
    "    \n",
    "        rnk = rnk/sum(rnk)\n",
    "   \n",
    "        \n",
    "    \n",
    "        #increment counts \n",
    "        self.count += 1\n",
    "        \n",
    "        #accumulate new alpha \n",
    "        self.new_alphas = self.new_alphas + (rnk)# + smoothing)\n",
    "        \n",
    "        for i in range(self.options.k):\n",
    "            for j in range(len(val)):\n",
    "                if val[j] == '0':\n",
    "                    continue\n",
    "                self.new_qmk[i][j] +=  (rnk[i])# + smoothing)\n",
    "#         dummy yield - real output passes to mapper_final in self\n",
    "\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        \n",
    "        out = [self.count, (self.new_alphas).tolist(),self.new_qmk.tolist() ]\n",
    "        jOut = json.dumps(out)\n",
    "        yield 1,jOut\n",
    "        \n",
    "    \n",
    "    def reducer(self, key, xs):\n",
    "        #accumulate partial sums\n",
    "        first = True        \n",
    "        #accumulate partial sums\n",
    "        #xs us a list of paritial stats, including count, phi, mean, and covariance. \n",
    "        #Each stats is k-length array, storing info for k components\n",
    "        for val in xs:\n",
    "            if first:\n",
    "                temp = json.loads(val)\n",
    "                #totCount, totPhi, totMeans, and totCov are all arrays\n",
    "                totCount = temp[0]\n",
    "                totalpha = array(temp[1])\n",
    "                totqmk = array(temp[2])         \n",
    "                first = False\n",
    "            else:\n",
    "                temp = json.loads(val)\n",
    "                #cumulative sum of four arrays\n",
    "                totCount = totCount + temp[0]\n",
    "                totalpha = totalpha + array(temp[1])\n",
    "                totqmk  = totqmk + array(temp[2])\n",
    "                \n",
    "        #finish calculation of new probability parameters. array divided by array\n",
    "        newAlpha = totalpha/totCount\n",
    "#         print array(newAlpha).tolist()\n",
    "        #initialize these to something handy to get the right size arrays\n",
    "        newqmk = zeros_like(totqmk)\n",
    "        for i in range(self.options.k):\n",
    "            for j in range(len(totqmk[i])):\n",
    "                newqmk[i][j] = totqmk[i][j]/totalpha[i]\n",
    "            \n",
    "        outputList = [newAlpha.tolist(),newqmk.tolist()]\n",
    "        jsonOut = json.dumps(outputList)\n",
    "        \n",
    "        #write new parameters to file\n",
    "        fullPath = self.options.pathName + 'intermediateResults_BMM_tweet.txt'\n",
    "        fileOut = open(fullPath,'w')\n",
    "        fileOut.write(jsonOut)\n",
    "        fileOut.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MrBMixEm.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! python mr_BMixEmIterate.py \\\n",
    "--pathName '/Users/hetal/programming/W261/UCB_MIDS_W261/hw6/' \\\n",
    "--file 'intermediateResults_BMM_tweet.txt' \\\n",
    "--file 'cluster_assignments'\n",
    "-q \\\n",
    "topUsers_Apr-Jul_2014_1000-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Iteration0\n",
      "[0.25, 0.25, 0.25, 0.25]\n",
      "Iteration1\n",
      "[0.9618331222137054, 0.0010000333288894814, 0.016333322263695507, 0.02083352219370948]\n",
      "Iteration2\n",
      "[0.9615336485318104, 0.0010000154383694182, 0.014444728269157705, 0.02302160776066261]\n",
      "Iteration3\n",
      "[0.9611248910967705, 0.0010000081403497355, 0.013815197965741958, 0.02405990279713767]\n",
      "Iteration4\n",
      "[0.9604182116896928, 0.0010000070328964738, 0.013581702152178535, 0.025000079125232454]\n",
      "Iteration5\n",
      "[0.9608553614659487, 0.0010000069630589875, 0.013127448861346081, 0.02501718270964633]\n",
      "Iteration6\n",
      "[0.9600259575269915, 0.0010000069577990945, 0.012972623165717814, 0.026001412349491552]\n",
      "Iteration7\n",
      "[0.9590337669102873, 0.001000006913985314, 0.012969124155125557, 0.026997102020601665]\n",
      "Iteration8\n",
      "[0.9589596198326018, 0.001000006966143332, 0.012966938515359978, 0.027073434685895046]\n",
      "Iteration9\n",
      "[0.958031090845865, 0.0010000069767846659, 0.012966417687920177, 0.02800248448943001]\n",
      "Iteration10\n",
      "[0.9577436116205024, 0.0010000070253156719, 0.01296485173870675, 0.028291529615475042]\n",
      "Iteration11\n",
      "[0.957035251197139, 0.0010000070362882636, 0.012964112077887124, 0.02900062968868552]\n",
      "Iteration12\n",
      "[0.9570015229315424, 0.0010000070484900104, 0.012962857566814714, 0.02903561245315311]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mr_BMMEmInitialize import MrBMM_EmInit\n",
    "from mr_BMixEmIterate import MrBMixEm\n",
    "import json\n",
    "from math import sqrt\n",
    "\n",
    "#function to calculate distance\n",
    "def dist(x,y):\n",
    "    #euclidean distance between two lists    \n",
    "    sum = 0.0\n",
    "    for i in range(len(x)):\n",
    "        temp = x[i] - y[i]\n",
    "        sum += temp * temp\n",
    "    return sqrt(sum)\n",
    "\n",
    "#first run the initializer to get starting centroids\n",
    "filePath = 'topUsers_Apr-Jul_2014_1000-words.txt'\n",
    "mrJob = MrBMM_EmInit(args=[filePath,'--pathName','/Users/hetal/programming/W261/UCB_MIDS_W261/hw6/'])\n",
    "with mrJob.make_runner() as runner:\n",
    "    runner.run()\n",
    "\n",
    "#pull out the centroid values to compare with values after one iteration\n",
    "emPath = \"intermediateResults_BMM_tweet.txt\"\n",
    "fileIn = open(emPath)\n",
    "paramJson = fileIn.read()\n",
    "fileIn.close()\n",
    "\n",
    "k = 4\n",
    "\n",
    "delta = 10\n",
    "iter_num = 0\n",
    "#Begin iteration on change in centroids\n",
    "while delta > 0.0001:\n",
    "    print \"Iteration\" + str(iter_num)\n",
    "    iter_num = iter_num + 1\n",
    "    #parse old centroid values\n",
    "    oldParam = json.loads(paramJson)\n",
    "    old_alpha = oldParam[0]\n",
    "    #run one iteration\n",
    "    old_qmk = oldParam[1]\n",
    "    \n",
    "    mrJob2 = MrBMixEm(args=[filePath,'--file',emPath,'--pathName','/Users/hetal/programming/W261/UCB_MIDS_W261/hw6/'])\n",
    "    with mrJob2.make_runner() as runner:\n",
    "        runner.run()\n",
    "\n",
    "    #compare new centroids to old ones\n",
    "    fileIn = open(emPath)\n",
    "    paramJson = fileIn.read()\n",
    "    fileIn.close()\n",
    "    newParam = json.loads(paramJson)\n",
    "\n",
    "    clusters = len(newParam[1])\n",
    "    new_alpha = newParam[0]\n",
    "    new_qmk = newParam[1]\n",
    "\n",
    "    \n",
    "    delta = 0.0\n",
    "#     for i in range(clusters):\n",
    "    delta += dist(new_alpha,old_alpha)\n",
    "\n",
    "    print oldParam[0]\n",
    "\n",
    "print \"Iteration\" + str(iter_num)\n",
    "print new_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW6.8 (Optional) 1 Million songs\n",
    "Predict the year of the song. Ask Jimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
