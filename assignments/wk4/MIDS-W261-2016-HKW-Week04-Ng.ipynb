{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Email: patng@ischool.berkeley.edu  \n",
    "Class: W261-2  \n",
    "Week: 04  \n",
    "Date of submission: Feb 12, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.0. \n",
    "What is MrJob? How is it different to Hadoop MapReduce? \n",
    "What are the mapper_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.1\n",
    "What is serialization in the context of MrJob or Hadoop? \n",
    "When it used in these frameworks? \n",
    "What is the default serialization mode for input and outputs for MrJob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.2: \n",
    "\n",
    "Recall the Microsoft logfiles data from the async lecture. The logfiles are described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html  \n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    " Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "```\n",
    "C,\"10001\",10001   #Visitor id 10001\n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "C,\"10002\",10002   #Visitor id 10001\n",
    "V\n",
    "Note: #denotes comments\n",
    "to the format:\n",
    "\n",
    "V,1000,1,C, 10001\n",
    "V,1001,1,C, 10001\n",
    "V,1002,1,C, 10001\n",
    "```\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile transform.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import csv\n",
    "\n",
    "curUser = None\n",
    "with open(\"anonymous-msweb.data\", \"r\") as csvfile:\n",
    "    for fields in csv.reader(csvfile):\n",
    "        # We only care about \"V\" and \"C\" records\n",
    "        recType = fields[0]\n",
    "        if recType != \"C\" and recType != \"V\":\n",
    "            continue\n",
    "\n",
    "        if recType == \"C\":\n",
    "            curUser = fields[2]\n",
    "        else:\n",
    "            print \",\".join(fields + [\"C\", curUser])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python transform.py > \"transformed-msweb.data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.3: \n",
    "Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw4.3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw4.3.py\n",
    "\n",
    "'''\n",
    "Input format:\n",
    "V,1000,1,C,10001\n",
    "V,1001,1,C,10001\n",
    "V,1002,1,C,10001\n",
    "V,1001,1,C,10002\n",
    "V,1003,1,C,10002\n",
    "V,1001,1,C,10003\n",
    "V,1003,1,C,10003\n",
    "V,1004,1,C,10003\n",
    "'''\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import csv\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    return csv.reader([line]).next()\n",
    "\n",
    "class PageVisit(MRJob):\n",
    "    \n",
    "    def mapper_get_visit_count(self, line_no, line):\n",
    "        \"\"\"Extracts the page id and visit count\"\"\"\n",
    "        cell = csv_readline(line)\n",
    "        yield cell[1], int(cell[2])\n",
    "\n",
    "    def reducer_get_visit_count(self, pageId, visit_counts):\n",
    "        \"\"\"Sumarizes the visit counts by adding them together.\"\"\"\n",
    "        total = sum(i for i in visit_counts)\n",
    "        yield pageId, total\n",
    "\n",
    "    def reducer_find_top5_pages_init(self):\n",
    "        print \"5 most frequently visited pages:\"\n",
    "        self.printed = 0\n",
    "\n",
    "    def reducer_find_top5_pages(self, pageId, visit_counts):\n",
    "        \"\"\"Print the top 5 pageId's and the counts\"\"\"\n",
    "        if self.printed < 5:\n",
    "            print pageId, visit_counts.next()\n",
    "            self.printed += 1\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_visit_count,\n",
    "                   combiner=self.reducer_get_visit_count,\n",
    "                   reducer=self.reducer_get_visit_count),\n",
    "            MRStep(reducer_init=self.reducer_find_top5_pages_init,\n",
    "                   reducer=self.reducer_find_top5_pages,\n",
    "                   jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"2\",\n",
    "                    \"mapreduce.job.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapreduce.partition.keycomparator.options\":\"-k2,2nr\"\n",
    "                          })\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    PageVisit.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hw4.patrickng.20160208.153355.764045\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hw4.patrickng.20160208.153355.764045/setup-wrapper.sh\n",
      "reading from STDIN\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/hw4.patrickng.20160208.153355.764045/files/\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar4209968991696781533/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob7584792665020681349.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar3250098516624409443/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob9020121546072492556.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/hw4.patrickng.20160208.153355.764045/output\n",
      "5 most frequently visited pages:\t\n",
      "1008 10836\t\n",
      "1034 9383\t\n",
      "1004 8463\t\n",
      "1018 5330\t\n",
      "1017 5108\t\n",
      "STDERR: 16/02/08 23:34:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hw4.patrickng.20160208.153355.764045\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/hw4.patrickng.20160208.153355.764045 from HDFS\n"
     ]
    }
   ],
   "source": [
    "# Run it in Hadoop\n",
    "!python hw4.3.py -r hadoop < transformed-msweb.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.4: \n",
    "Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 4.5 \n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of our recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342  \n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "```\n",
    "topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "```\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "```\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes.\n",
    "\n",
    "For (A),  we select 4 users randomly from a uniform distribution [1,...,1,000]\n",
    "For (B), (C), and (D)  you will have to use data from the auxiliary file: \n",
    "```\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "This file contains 5 special word-frequency distributions:\n",
    "\n",
    "(1) The 1000-user-wide aggregate, which you will perturb for initializations\n",
    "in parts (B) and (C), and\n",
    "\n",
    "(2-5) The 4 class-level aggregates for each of the user-type classes (0/1/2/3)\n",
    "\n",
    "In part (D), just use the (row-normalized) class-level aggregates as 'trained'\n",
    "starting centroids (the training is already done for you!).\n",
    "\n",
    "In parts (B) and (C), you will have to perturb the 1000-user aggregate \n",
    "(after initially normalizing by its sum, which is also provided).\n",
    "So if in (B) you want to create 2 perturbations of the aggregate, start\n",
    "with (1), normalize, and generate 1000 random numbers uniformly \n",
    "from the unit interval (0,1) twice (for two centroids), using:\n",
    "\n",
    "from numpy import random\n",
    "numbers = random.sample(1000)\n",
    "\n",
    "Take these 1000 numbers and add them (component-wise) to the 1000-user aggregate,\n",
    "and then renormalize to obtain one of your aggregate-perturbed initial centroids.\n",
    "——\n",
    "For experiments A, B, C and D and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
