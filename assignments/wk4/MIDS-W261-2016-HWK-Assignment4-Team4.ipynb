{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #4=====\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "DATSCIW261 ASSIGNMENT #4\n",
    "\n",
    "Version 2016-01-27 (FINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hetal Chandaria, Patrick Ng, Marjorie Sayer\n",
    "\n",
    "W261 - 2 , ASSIGNMENT #4\n",
    "\n",
    "Submission Date : Feb 12, 2016\n",
    "\n",
    "Group : 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 4.0. \n",
    "\n",
    "What is MrJob? How is it different to Hadoop MapReduce? \n",
    "\n",
    "What are the mappint_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: blue\"> Answer </span>\n",
    "\n",
    "1. MrJob\n",
    "MrJob is a python package that helps us write and run Hadoop streaming jobs. It assists us in submitting job to Hadoop job tracker and in running each individual step under Hadoop streaming. \n",
    "\n",
    "\n",
    "MapReduce is a framework processing parallelizable problems across huge data sets, using a large number of computers (nodes); cluster or grid. User specify a map fnction that processes a key/value pair to generate intermediate key/value pairs and a reduce function that merges all values associated with the same intermediate key. \n",
    "\n",
    "Hadoop MapReduce is an implementation of Mapreduce programming framework. This API is implemented in Java. The Hadoop streaming library internally uses this. MrJob supports both local , hadoop and AWS EMR modes. For hadoop mode it internally uses hadoop streaming.\n",
    "\n",
    "2.\n",
    "\n",
    "mapper_int() : is used to define an action to be run before the mapper processes any input. mapper_init() is called first if there is function defined.\n",
    "\n",
    "mapper_final(): is used to define an action to run after the mapper reaches the end of input. mapper_final is called after the mapper function has finished processing but before combiner or reducer is called. \n",
    "\n",
    "combiner_final(): is used to define an action to run after the combiner reaches the end of input. combiner_final is called after the combiner function has finished processing but before the reducer is called. \n",
    "\n",
    "reducer_final(): is used to define an action to run after the reducer reaches the end of input.combiner_final is called after the reducer function is finished. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 4.1\n",
    "\n",
    "What is serialization in the context of MrJob or Hadoop? \n",
    "\n",
    "When it used in these frameworks? \n",
    "\n",
    "What is the default serialization mode for input and outputs for MrJob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: blue\"> Answer </span>\n",
    "<span style=\"color: blue\">What is serialization in the context of MrJob or Hadoop?</span>\n",
    "\n",
    "Serialization (and deserialization) implies being able to take an object , convert it into bytes and then be able to reconstruct the object back from those bytes. In the context of Hadoop, the mapreduce APIs act upon keys and values. For primitive types serialization would not be a major issue to overcome. However, for complex types, the MR framework needs to know how to take the raw input and convert it into the keys&values for the mappers, then take the keys and values generated by the mapper, serialize them as needed for the shuffle layer to make them available to the reducer and finally write the key-value output generated by the reducer to HDFS/disk in the format needed. This requires the mapreduce framework to understand the serialization format for these keys and values for the framework to function. In the context of MRJob, it uses json for the most part. This can be overridden via use of mrjob custom protocols.\n",
    "\n",
    "<span style=\"color: blue\">When it used in these frameworks?</span>\n",
    "\n",
    "a. De-serialization: Converting raw input to keys and values for the mapper. \n",
    "\n",
    "b. Serialize and de-serialize: Write mapper output to disk. Convert bytes received from shuffle layer to keys and values for reducer input.\n",
    "\n",
    "c. Serialization: Write reducer output to disk/HDFS.\n",
    "\n",
    "<span style=\"color: blue\">What is the default serialization mode for input and outputs for MrJob?</span>\n",
    "\n",
    "The default input protocol for serialization is RawValueProtocol.\n",
    "The default output and internal protocol are both JSONProtocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 4.2: Recall the Microsoft logfiles data from the async lecture. \n",
    "\n",
    "The logfiles are described are located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html\n",
    "\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    "\n",
    "Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "C,\"10001\",10001   #Visitor id 10001\n",
    "\n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "\n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "\n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "\n",
    "C,\"10002\",10002   #Visitor id 10001\n",
    "\n",
    "V\n",
    "\n",
    "Note: #denotes comments\n",
    "to the format:\n",
    "\n",
    "V,1000,1,C, 10001\n",
    "\n",
    "V,1001,1,C, 10001\n",
    "\n",
    "V,1002,1,C, 10001\n",
    "\n",
    "\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocess_log.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess_log.py\n",
    "#!/usr/bin/python\n",
    "\"\"\" This program process the log file and outputs 2 separate files. \n",
    "    One for the log information and other is page URLS's.\n",
    "\"\"\"\n",
    "import sys\n",
    "import re\n",
    "\n",
    "inputfile = sys.argv[1]\n",
    "outputfile = open('processed_anonymous-msweb.data', 'a')\n",
    "outputfile2 = open('processed_urls.data', 'a')\n",
    "\n",
    "for line in open(inputfile):\n",
    "    v = line.split(',')\n",
    "    # If the line is customer information line, save it to customer_info\n",
    "    if(v[0]=='C'):\n",
    "        customer_info = v\n",
    "    # If the line visit information line, concatenate the line with customer_info\n",
    "    elif(v[0]=='V'):\n",
    "        outputfile.write( line.strip()+','+customer_info[0]+','+customer_info[1].strip('\"')+'\\n')\n",
    "    # Directly output other lines\n",
    "    elif(v[0]=='A'):\n",
    "        outputfile2.write(v[1]+','+ re.sub(r'\\\"+','',v[4]))\n",
    "    else:\n",
    "         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: processed_anonymous-msweb.data: No such file or directory\n",
      "rm: processed_urls.data: No such file or directory\n",
      "   98654 processed_anonymous-msweb.data\n",
      "V,1000,1,C,10001\n",
      "V,1001,1,C,10001\n",
      "V,1002,1,C,10001\n",
      "V,1001,1,C,10002\n",
      "V,1003,1,C,10002\n",
      "V,1001,1,C,10003\n",
      "V,1003,1,C,10003\n",
      "V,1004,1,C,10003\n",
      "V,1005,1,C,10004\n",
      "V,1006,1,C,10005\n",
      "     294 processed_urls.data\n",
      "1287,/autoroute\n",
      "1288,/library\n",
      "1289,/masterchef\n",
      "1297,/centroam\n",
      "1215,/developer\n",
      "1279,/msgolf\n",
      "1239,/msconsult\n",
      "1282,/home\n",
      "1251,/referencesupport\n",
      "1121,/magazine\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x preprocess_log.py\n",
    "! rm processed_anonymous-msweb.data\n",
    "! rm processed_urls.data\n",
    "!./preprocess_log.py anonymous-msweb.data\n",
    "!wc -l processed_anonymous-msweb.data\n",
    "! head processed_anonymous-msweb.data\n",
    "!wc -l processed_urls.data\n",
    "! head processed_urls.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 4.3: \n",
    "Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw4_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw4_3.py\n",
    "\n",
    "'''\n",
    "Input format:\n",
    "....\n",
    "V,1000,1,C,10001\n",
    "V,1001,1,C,10001\n",
    "V,1002,1,C,10001\n",
    "V,1001,1,C,10002\n",
    "V,1003,1,C,10002\n",
    "V,1001,1,C,10003\n",
    "V,1003,1,C,10003\n",
    "V,1004,1,C,10003\n",
    "....\n",
    "'''\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    return csv.reader([line]).next()\n",
    "\n",
    "class PageVisitHW4_3(MRJob):\n",
    "    \n",
    "    def mapper_get_visit_count(self, line_no, line):\n",
    "        \"\"\"Extracts the page id and visit count\"\"\"\n",
    "        cell = csv_readline(line)\n",
    "        yield cell[1], 1\n",
    "\n",
    "    def reducer_get_visit_count(self, pageId, visit_counts):\n",
    "        \"\"\"Sumarizes the visit counts by adding them together.\"\"\"\n",
    "        total = sum(i for i in visit_counts)\n",
    "        yield pageId, total\n",
    "\n",
    "    def reducer_find_top5_pages_init(self):\n",
    "        self.printed = 0\n",
    "\n",
    "    def reducer_find_top5_pages(self, pageId, visit_counts):\n",
    "        \"\"\"Print the top 5 pageId's and the counts\"\"\"\n",
    "        if self.printed < 5:\n",
    "            yield pageId, visit_counts.next()\n",
    "            self.printed += 1\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_visit_count,\n",
    "                   combiner=self.reducer_get_visit_count,\n",
    "                   reducer=self.reducer_get_visit_count),\n",
    "            MRStep(reducer_init=self.reducer_find_top5_pages_init,\n",
    "                   reducer=self.reducer_find_top5_pages,\n",
    "                   jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"2\",\n",
    "                    \"mapreduce.job.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapreduce.partition.keycomparator.options\":\"-k2,2nr\"\n",
    "                          })]\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    PageVisitHW4_3.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most frequently visited pages:\n",
      "('1008', 10836)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:mrjob.fs.hadoop:STDERR: 16/02/11 11:57:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "('1034', 9383)\n",
      "('1004', 8463)\n",
      "('1018', 5330)\n",
      "('1017', 5108)\n"
     ]
    }
   ],
   "source": [
    "from hw4_3 import PageVisitHW4_3\n",
    "mr_job = PageVisitHW4_3(args=['processed_anonymous-msweb.data', '-r', 'hadoop', '--strict-protocols'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    \n",
    "    print \"5 most frequently visited pages:\"\n",
    "\n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 4.4: \n",
    "Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hw4_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hw4_4.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "import mrjob\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "def csv_readline(line):\n",
    "    \"\"\"Given a sting CSV line, return a list of strings.\"\"\"\n",
    "    return csv.reader([line]).next()\n",
    "\n",
    "class PageVisitHW4_4(MRJob):\n",
    "    \n",
    "    BASE_URL = \"http://www.microsoft.com\"\n",
    "    \n",
    "    # Have to use Raw Protocol in order to get sorting (using 3rd key) to work.\n",
    "    INTERNAL_PROTOCOL = mrjob.protocol.RawProtocol\n",
    "    OUTPUT_PROTOCOL = mrjob.protocol.RawProtocol\n",
    "    \n",
    "    def mapper1(self, line_no, line):\n",
    "        cell = csv_readline(line)\n",
    "        # page,visitor \\t count\n",
    "        yield \",\".join([cell[1], cell[4]]), \"1\"\n",
    "        \n",
    "    def reducer1(self, key, values):\n",
    "        \"\"\"Sum up the visit count per (page, visitor) pair.\"\"\"\n",
    "        total = sum([int(v) for v in values])\n",
    "        fields = key.split(\",\")\n",
    "        # page \\t visitor \\t total\n",
    "        yield fields[0], \"\\t\".join([fields[1], str(total)])\n",
    "\n",
    "    def reducer2_init(self):\n",
    "        # Build the dictionary of pageId:url\n",
    "        self.urls = {}\n",
    "        with open(\"processed_urls.data\", \"r\") as f:\n",
    "            for fields in csv.reader(f):\n",
    "                self.urls[fields[0]] = fields[1]\n",
    "                \n",
    "    def reducer2(self, key, values):\n",
    "        # url \\t pageId \\t visitor \\t total\n",
    "        yield self.BASE_URL + self.urls[key] + \"\\t\" + key, values.next()\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1,\n",
    "                   reducer=self.reducer1,\n",
    "                  ),\n",
    "            MRStep(reducer_init=self.reducer2_init,\n",
    "                   reducer=self.reducer2,\n",
    "                   jobconf={\n",
    "                    \"stream.num.map.output.key.fields\":\"3\",\n",
    "                    \"mapreduce.job.output.key.comparator.class\":\n",
    "                        \"org.apache.hadoop.mapred.lib.KeyFieldBasedComparator\",\n",
    "                    \"mapreduce.partition.keycomparator.options\":\"-k3,3nr\"\n",
    "                          })]\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    PageVisitHW4_4.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hw4_4.patrickng.20160211.035741.553883\n",
      "writing wrapper script to /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hw4_4.patrickng.20160211.035741.553883/setup-wrapper.sh\n",
      "Using Hadoop version 2.7.1\n",
      "Copying local files into hdfs:///user/patrickng/tmp/mrjob/hw4_4.patrickng.20160211.035741.553883/files/\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar9199386722825397230/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob4861490282259515716.jar tmpDir=null\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "HADOOP: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "HADOOP: packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar4574631163935162190/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob2516918319451535318.jar tmpDir=null\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Streaming final output from hdfs:///user/patrickng/tmp/mrjob/hw4_4.patrickng.20160211.035741.553883/output\n",
      "STDERR: 16/02/11 11:59:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "removing tmp directory /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hw4_4.patrickng.20160211.035741.553883\n",
      "deleting hdfs:///user/patrickng/tmp/mrjob/hw4_4.patrickng.20160211.035741.553883 from HDFS\n"
     ]
    }
   ],
   "source": [
    "!python ./hw4_4.py \\\n",
    "-r hadoop \\\n",
    "--file processed_urls.data \\\n",
    "--strict-protocols \\\n",
    "processed_anonymous-msweb.data > most_visitors.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage URL, Page Id, Customer Id, Visit Count\n",
      "http://www.microsoft.com/train_cert\t1295\t42616\t1\t\r\n",
      "http://www.microsoft.com/partner\t1284\t41108\t1\t\r\n",
      "http://www.microsoft.com/cinemania\t1283\t41033\t1\t\r\n",
      "http://www.microsoft.com/home\t1282\t41244\t1\t\r\n",
      "http://www.microsoft.com/intellimouse\t1281\t37099\t1\t\r\n",
      "http://www.microsoft.com/music\t1280\t41643\t1\t\r\n",
      "http://www.microsoft.com/msgolf\t1279\t31062\t1\t\r\n",
      "http://www.microsoft.com/hed\t1278\t41317\t1\t\r\n",
      "http://www.microsoft.com/stream\t1277\t30111\t1\t\r\n",
      "http://www.microsoft.com/vtestsupport\t1276\t40810\t1\t\r\n"
     ]
    }
   ],
   "source": [
    "print \"Webpage URL, Page Id, Customer Id, Visit Count\"\n",
    "!head most_visitors.out | sed s/\\\"//g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 4.5 \n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of our recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words\n",
    "\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "\n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution \n",
    "\n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes.\n",
    "\n",
    "and iterate until a threshold (try 0.001) is reached.\n",
    "After convergence, print out a summary of the classes present in each cluster.\n",
    "In particular, report the composition as measured by the total\n",
    "portion of each class type (0-3) contained in each cluster,\n",
    "and discuss your findings and any differences in outcomes across parts A-D.\n",
    "\n",
    "Note that you do not have to compute the aggregated distribution or the \n",
    "class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting custom_func.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_func.py\n",
    "# function to calculate purity of cluster and print it. Its a generic function that can be called\n",
    "\n",
    "def calc_purity(cluster_dist):\n",
    "    #calculate purity and print class distribution\n",
    "    print 'Cluster distribution'\n",
    "    print '-'*100\n",
    "\n",
    "    user_class =  { 0:'Human', 1:'Cyborg', 2:'Robot', 3:'Spammer' }\n",
    "    human = sum([cluster_dist[k].get('0',0) for k in cluster_dist.keys()])\n",
    "    cyborg = sum([cluster_dist[k].get('1',0) for k in cluster_dist.keys()])\n",
    "    robot = sum([cluster_dist[k].get('2',0) for k in cluster_dist.keys()])\n",
    "    spammer = sum([cluster_dist[k].get('3',0) for k in cluster_dist.keys()])\n",
    "    print \"{0:>5} |{1:>15} |{2:>15} |{3:>15} |{4:>15}\".format(\n",
    "        \"k\", \"Human\"+':'+str(human), \"Cyborg\"+':'+str(cyborg), \"Robot\"+':'+str(robot), \"Spammer\"+':'+str(spammer))\n",
    "    print '-'*100\n",
    "    max_cl={}\n",
    "    total = 0\n",
    "    for cid, cvalue in cluster_dist.iteritems():\n",
    "        total += sum(cvalue.values())\n",
    "        print \"{0:>5} |{1:>15} |{2:>15} |{3:>15} |{4:>15}\".format(\n",
    "        cid, cvalue.get('0',0) ,cvalue.get('1',0) ,cvalue.get('2',0) , cvalue.get('3',0))\n",
    "        max_cl[cid]=max(cvalue.values())\n",
    "    print '-'*100\n",
    "    print 'purity : %3.3f' %(100*sum(max_cl.values())*1.0/total)\n",
    "    print '-'*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "from numpy import argmin, array, random\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "#Calculate find the nearest centroid for data point \n",
    "def MinDist(datapoint, centroid_points):\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    # Get the nearest centroid for each instance\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    return minidx\n",
    "\n",
    "#Check whether centroids converge\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "#     return np.alltrue(abs(np.array(centroid_points_new) - np.array(centroid_points_old)) <= T)\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\n",
    "    Flag = True\n",
    "    for i in Diff:\n",
    "        if(i>T):\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    centroid_points=[]\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRJobStep(mapper_init=self.mapper_init,mapper=self.mapper,combiner=self.combiner,reducer=self.reducer)\n",
    "               ]\n",
    "    \n",
    "    #load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"Centroids.txt\").readlines()]\n",
    "    \n",
    "    #load data and output the nearest centroid index and data point \n",
    "    def mapper(self, _, line):\n",
    "        D = line.split(',')\n",
    "        total = int(D[2])\n",
    "        code=int(D[1])\n",
    "        #normalise the input data\n",
    "        data=map(float,D[3:])\n",
    "        normalise_data=map(lambda x:((1.0 * x) / total),data)\n",
    "        #sending the class composition along with the mapper output \n",
    "        yield int(MinDist(normalise_data,self.centroid_points)),(list(normalise_data),1,{code:1})\n",
    "    \n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, inputdata):\n",
    "        code = {} #for class composition\n",
    "        sum_features= None\n",
    "        num = 0 #for count\n",
    "        for features,n,codes in inputdata:\n",
    "            features = array(features) #convert to array first\n",
    "            if sum_features is None: #if looping through first, initialise array of zeros\n",
    "                sum_features = np.zeros(features.size)\n",
    "            sum_features += features #add features\n",
    "            num += n #increment count \n",
    "            #count codes \n",
    "            for k,v in codes.iteritems(): #increment class composition\n",
    "                code[k] = code.get(k,0)+ v\n",
    "            \n",
    "        yield idx,(list(sum_features),num,code)\n",
    "    \n",
    "    #Aggregate sum for each cluster and then calculate the new centroids\n",
    "    def reducer(self, idx, inputdata):\n",
    "        centroids = None\n",
    "        code = {}\n",
    "        num = 0\n",
    "        for features,n,codes in inputdata:\n",
    "            features = array(features)\n",
    "           \n",
    "            if centroids is None:\n",
    "                centroids = np.zeros(features.size)\n",
    "            centroids += features \n",
    "            \n",
    "            num += n #increment predicted cluster count\n",
    "            \n",
    "            #count codes \n",
    "            for k,v in codes.iteritems(): #increment class composition\n",
    "                code[k] = code.get(k,0)+v\n",
    "                \n",
    "        centroids_new = centroids / num #compute new centroid\n",
    "        yield idx, (list(centroids_new),code)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_kmeans.py\n",
    "\n",
    "from numpy import random\n",
    "import numpy as np\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "import sys\n",
    "from custom_func import calc_purity\n",
    "mr_job = MRKmeans(args=['topUsers_Apr-Jul_2014_1000-words.txt'])\n",
    "\n",
    "random.seed(0)\n",
    "#number of features\n",
    "n= 1000\n",
    "\n",
    "#get centroid type and number of clusters from user\n",
    "if len(sys.argv) >2: k = int(sys.argv[2])\n",
    "cen_type = sys.argv[1]\n",
    "\n",
    "#Geneate initial centroids\n",
    "centroid_points = []\n",
    "\n",
    "#based on the centroid type generate centroids\n",
    "if(cen_type=='Uniform'):\n",
    "    rand_int = random.uniform(size=[k,n])\n",
    "    total = np.sum(rand_int,axis=1)\n",
    "    centroid_points = (rand_int.T/total).T\n",
    "    with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "    f.close()\n",
    "    \n",
    "elif(cen_type=='Perturbation'):\n",
    "    data = [s.split('\\n')[0].split(',') for s in \n",
    "                   open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines()][1]\n",
    "    #get the total count of words\n",
    "    total = int(data[2])\n",
    "    feature = map(lambda x:((1.0 * float(x)) / total),data[3:]) #normalise\n",
    "    pertubation = feature + random.sample(size=(k,n)) #generate random sample and add it to feature\n",
    "    sum_per = np.sum(pertubation,axis=1) # calculate the sum to be used for normalization\n",
    "    centroid_points = (pertubation.T/sum_per).T\n",
    "    with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "    f.close()\n",
    "\n",
    "else:\n",
    "    data = [s.split('\\n')[0].split(',') for s in \n",
    "                   open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines()][2:]\n",
    "    for cluster in data:\n",
    "        total = 0\n",
    "        total = int(cluster[2])#get the total count of words\n",
    "        feature = map(lambda x:((1.0 * float(x)) / total),cluster[3:]) #normalise\n",
    "        centroid_points.append(feature)\n",
    "    with open('Centroids.txt', 'w+') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "    f.close()\n",
    "\n",
    "print 'Centroid Type: %s' %cen_type\n",
    "    \n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        centroid_points = []\n",
    "        cluster_dist ={}\n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            centroid, codes = value\n",
    "            centroid_points.append(centroid)\n",
    "            cluster_dist[key]=codes\n",
    "    i = i + 1\n",
    "    \n",
    "    #check if we have convergence\n",
    "    if(stop_criterion(centroid_points_old,centroid_points,0.001)):\n",
    "        break\n",
    " \n",
    "    #write new centroids back to file \n",
    "    with open('Centroids.txt', 'w') as f:\n",
    "        for centroid in centroid_points:\n",
    "            f.writelines(','.join(map(str, centroid)) + '\\n')\n",
    "        f.close()\n",
    "\n",
    "calc_purity(cluster_dist) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid Type: Uniform\n",
      "iteration0:\n",
      "iteration1:\n",
      "iteration2:\n",
      "iteration3:\n",
      "iteration4:\n",
      "iteration5:\n",
      "Cluster distribution\n",
      "----------------------------------------------------------------------------------------------------\n",
      "    k |      Human:752 |      Cyborg:91 |       Robot:54 |    Spammer:103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "    0 |              0 |              0 |             11 |              0\n",
      "    1 |              0 |             51 |              0 |              0\n",
      "    2 |              1 |             37 |             38 |              4\n",
      "    3 |            751 |              3 |              5 |             99\n",
      "----------------------------------------------------------------------------------------------------\n",
      "purity : 85.100\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python run_kmeans.py Uniform 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid Type: Perturbation\n",
      "iteration0:\n",
      "iteration1:\n",
      "iteration2:\n",
      "iteration3:\n",
      "Cluster distribution\n",
      "----------------------------------------------------------------------------------------------------\n",
      "    k |      Human:752 |      Cyborg:91 |       Robot:54 |    Spammer:103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "    0 |            751 |              3 |             16 |             99\n",
      "    1 |              1 |             88 |             38 |              4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "purity : 83.900\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python run_kmeans.py Perturbation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid Type: Perturbation\n",
      "iteration0:\n",
      "iteration1:\n",
      "iteration2:\n",
      "iteration3:\n",
      "iteration4:\n",
      "iteration5:\n",
      "iteration6:\n",
      "Cluster distribution\n",
      "----------------------------------------------------------------------------------------------------\n",
      "    k |      Human:752 |      Cyborg:91 |       Robot:54 |    Spammer:103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "    0 |              0 |              0 |              5 |              0\n",
      "    1 |              0 |             51 |              0 |              0\n",
      "    2 |              1 |             37 |             38 |              4\n",
      "    3 |            751 |              3 |             11 |             99\n",
      "----------------------------------------------------------------------------------------------------\n",
      "purity : 84.500\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python run_kmeans.py Perturbation 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroid Type: Trained\n",
      "iteration0:\n",
      "iteration1:\n",
      "iteration2:\n",
      "iteration3:\n",
      "iteration4:\n",
      "Cluster distribution\n",
      "----------------------------------------------------------------------------------------------------\n",
      "    k |      Human:752 |      Cyborg:91 |       Robot:54 |    Spammer:103\n",
      "----------------------------------------------------------------------------------------------------\n",
      "    0 |            749 |              3 |             14 |             38\n",
      "    1 |              0 |             51 |              0 |              0\n",
      "    2 |              1 |             37 |             40 |              4\n",
      "    3 |              2 |              0 |              0 |             61\n",
      "----------------------------------------------------------------------------------------------------\n",
      "purity : 90.100\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python run_kmeans.py Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW4.6  (OPTIONAL) Scaleable K-MEANS++ \n",
    "\n",
    "Read the following paper entitled \"Scaleable K-MEANS++\" located at:\n",
    "\n",
    "http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf \n",
    "\n",
    "In MrJob, implement K-MEANS|| and compare with a random initializtion for the dataset above. \n",
    "Report on the number passes over the training data, and time required to run all  clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
