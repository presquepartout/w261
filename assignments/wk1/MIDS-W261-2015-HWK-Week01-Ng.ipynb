{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Email: patng@ischool.berkeley.edu  \n",
    "Class: W261-2  \n",
    "Week: 01  \n",
    "Date of submission: Jan 18, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0###\n",
    "Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big data is broad term for data sets so large or complex that traditional data-processing applications are inadequate.  For instance, today a high end laptop may have 1 TB of harddisk and 8GB of memory.  However, for many big data problem, the amount of disk and memory needed is way over what can be fit in such a high end laptop.\n",
    "\n",
    "IBM has also characterized big data by its 4 V's: Volume (scale of data), Velocity (analysis of streaming data), Variety (different forms of data) and Veractiy (uncertainty of data). \n",
    "\n",
    "For example, for a popular website which attracts ten millions visits each day, the amount of web log data generated each day is about 50GB.  The website also uses a recommendation engine to generate recommendations to the visitors.  In order to train the engine each night, we need to cleanse (e.g. remove logs generated by suspected robots) and transform the web log data into a format usable by the training process, and the training itself has to process all the new web log data, together with all data generated from the past.  The whole process has to complete within 6 hours due to business needs.  However, using traditional data-processing techniques the whole proceess could take more than 8 hours.  As a result, we need to make use of big data techniques such as HDFS and parallel computating in order to complete the processing within the required period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1###\n",
    "In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4 and 5, the strategy is:\n",
    "- First, please note that each regression model will produce an estimator <i>g(x)</i> of the true function <i>f(x)</i>.\n",
    "- For each model:\n",
    "    - Using bootstrapping, generate datasets S<sub>1</sub>, S<sub>2</sub>, ..., S<sub>B</sub> from the dataset T.\n",
    "    - For each S<sub>b</sub>:\n",
    "      - Use S<sub>b</sub> to train the model to produce an estimator g<sub>b</sub>(x).\n",
    "      - Let the dataset T<sub>b</sub> = T \\ S<sub>b</sub> be the data points that do not appear in S<sub>b</sub>\n",
    "      - Calculate the predicted value g<sub>b</sub>(x) for each x in T<sub>b</sub>.\n",
    "    - For each data point x in T  :\n",
    "        - We have calculated several predictions $y_{1},\\; y_{2},\\; ...\\; ,\\; y_{K}$. From them we can calculate the average prediction $\\underline{h} = \\sum_{K}{ y_{k} } /\\; K$\n",
    "            - K is the number of predictions made for data point x\n",
    "            - $y_{k}$ is the k<sup>th</sup> prediction made for data point x\n",
    "        - And from that we can calculate the bias, the variance and the irreduciable error of the model for the data point x:\n",
    "            - $Bias = \\underline{h}\\; -\\; y$\n",
    "            - $Variance = \\sum_{K}^{}{\\left( y_{k}\\; -\\underline{h}\\right)^{2}\\; } /  \\left({K\\; -\\; 1} \\right)$\n",
    "            - $Irreduciable Error / Noise:$\n",
    "                - Since we do not have the true function f(x), we have to estimate the noise at each data point x:\n",
    "                    - In T, if several data points exists at x, then:\n",
    "                        - noise = variance of the multiple y's from those data points\n",
    "                    - Otherwise, assume noise = 0 for x\n",
    "                  \n",
    "To select a model, for each model we calculate the <i>expected prediction error</i>:\n",
    "- Expected prediction error = $\\mbox{E}_{X}\\left[ Variance\\; +\\; \\mbox{Bi}as^{2}\\; +\\; Noise^{2\\; } \\right]$\n",
    "    - E<sub>X</sub> means we average it over all data points.\n",
    "\n",
    "\n",
    "We then plot the numbers, similar to the graph below.  Please note that we do not have the figures for the <i>Test Error</i> line.\n",
    "<img src=\"bias-variance-tradeoff.png\" width=\"350\">\n",
    "\n",
    "\n",
    "From the graph, we identify the point where $\\mbox{E}_{X}\\left[ Variance\\; +\\; \\mbox{Bi}as^{2} \\right]$ is the smallest.  The model corresponding to that point (e.g. model of degree 3 in the graph) is the model we choose.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the control script ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1 ###\n",
    "Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2 ###\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "\n",
    "   To do so, make sure that\n",
    "   \n",
    "   - mapper.py counts all occurrences of a single word, and\n",
    "   - reducer.py collates the counts of the single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: mapper code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "findword = findwords[0] # for HW1.2 we handle only a single word\n",
    "regex = re.compile(r'\\b' + re.escape(findword) + r'\\b')\n",
    "count = 0\n",
    "\n",
    "# Loop thru all the lines in the file and count the total occurrence count of findword\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        \n",
    "        subject = \"\" if parts[2].strip() == \"NA\" else parts[2]\n",
    "        body = \"\" if parts[3].strip() == \"NA\" else parts[3]\n",
    "        text = subject + \" \" + body\n",
    "        \n",
    "        count += len(regex.findall(text))\n",
    "\n",
    "print '%s\\t%d' % (findword, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Patrick Ng\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "totalCount = 0\n",
    "findword = None\n",
    "\n",
    "# Go thru all the files, and sum up the number of occurrence counters.\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        line = f.readline()\n",
    "        parts = line.split('\\t')\n",
    "        findword = parts[0]\n",
    "        count = int(parts[1])\n",
    "        totalCount += count\n",
    "\n",
    "print '%s\\t%d' % (findword, totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; chmod +x pNaiveBayes.sh;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 assistance\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. ###  \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. To do so, make sure that\n",
    "   \n",
    "   - mapper.py and\n",
    "   - reducer.py \n",
    "\n",
    "that performs a single word Naive Bayes classification. For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "    the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "NOTE if  “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeld as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimated of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "findword = findwords[0] # for HW1.3 we handle only a single word\n",
    "\n",
    "# Regex for counting the number of findword in msg\n",
    "regex = re.compile(r'\\b' + re.escape(findword) + r'\\b')\n",
    "\n",
    "# Regex for counting the total number of words in a msg\n",
    "regexAll = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "# For each of the count list below, index=0 means non-spam, index=1 means spam.\n",
    "findwordCounts = [0, 0] # The findword counts among the two classes\n",
    "totalwordCounts = [0, 0] # The total counts among the two classes\n",
    "msgCounts = [0, 0]  # Total count of msg among the two classes\n",
    "\n",
    "# For each msg: MsgId, its True class, and the occurrence count of findword\n",
    "findwordInMessages = {}\n",
    "\n",
    "# Go through each line of the file, and update the various counters\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        \n",
    "        msgId = parts[0]\n",
    "        spamIndicator = int(parts[1])\n",
    "\n",
    "        subject = \"\" if parts[2].strip() == \"NA\" else parts[2]\n",
    "        body = \"\" if parts[3].strip() == \"NA\" else parts[3]\n",
    "        text = subject + \" \" + body\n",
    "        \n",
    "        # Update the various counters\n",
    "        \n",
    "        msgCounts[spamIndicator] += 1\n",
    "        \n",
    "        findwordCount = len(regex.findall(text))\n",
    "        findwordCounts[spamIndicator] += findwordCount\n",
    "        findwordInMessages[msgId] = [spamIndicator, findwordCount]\n",
    "        \n",
    "        totalwordCounts[spamIndicator] += len(regexAll.findall(text))\n",
    "\n",
    "\n",
    "# Msg count in each class\n",
    "print \"%d\\t%d\" % (msgCounts[0], msgCounts[1]) \n",
    "\n",
    "# Occurrence count of the findword in each class\n",
    "print \"%d\\t%d\" % (findwordCounts[0], findwordCounts[1]) \n",
    "\n",
    "# Total word count in each class\n",
    "print \"%d\\t%d\" % (totalwordCounts[0], totalwordCounts[1])\n",
    "\n",
    "# For each msg: MsgId, occurrence count of findword, its True class\n",
    "for k, v in findwordInMessages.iteritems():\n",
    "    print \"%s\\t%d\\t%d\" % (k, v[0], v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Patrick Ng\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "\n",
    "# Helper function to read the findwordInMessages data from the file\n",
    "def readFindwordInMessages(f, countDict):\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        msgId = parts[0]\n",
    "        trueClass = int(parts[1])\n",
    "        count = int(parts[2])\n",
    "        countDict[msgId] = [trueClass, count]\n",
    "\n",
    "# Helper function to read two tab separated counters from a line in the file\n",
    "def readCounts(f, counts):\n",
    "    line = f.readline()\n",
    "    parts = re.split(\"\\t\", line)\n",
    "    counts[0] += int(parts[0])\n",
    "    counts[1] += int(parts[1])\n",
    "\n",
    "# Train the Multinomial Naive Bayes model and return the Priors and \n",
    "# the conditional Probabilities\n",
    "def trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts):\n",
    "    \"\"\"\n",
    "    Train the Multinomial Naive Bayes model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        msgCounts : list of int\n",
    "            Count of messages in each class (0=non-spam, 1=spam)\n",
    "            \n",
    "        findwordCounts : list of int\n",
    "            Count of findword occurrence in each class (0=non-spam, 1=spam)\n",
    "            \n",
    "        totalwordCounts : list of int\n",
    "            Total number of words in each class (0=non-spam, 1=spam)\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "        priors : dict of float\n",
    "            The priors (Pr(c)) of each class.\n",
    "            \n",
    "        condProbs : dict of float\n",
    "            The conditional probability (Pr(X=t|c)) of each class.\n",
    "    \"\"\"\n",
    "    priors = {} # Pr(c)\n",
    "    condProbs = {} # Pr(X=t|c)\n",
    "    msgTotal = sum(msgCounts)\n",
    "    \n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        priors[c] = float(msgCounts[c]) / msgTotal\n",
    "        condProbs[c] = float(findwordCounts[c]) / totalwordCounts[c]\n",
    "        \n",
    "    return priors, condProbs\n",
    "\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "# For each of the count list below, index=0 means non-spam, index=1 means spam.\n",
    "findwordCounts = [0.0, 0.0] # The findword counts among the two classes\n",
    "totalwordCounts = [0.0, 0.0] # The total counts among the two classes\n",
    "msgCounts = [0.0, 0.0]  # Total count of msg among the two classes\n",
    "\n",
    "# For each msg: MsgId, occurrence count of findword, its True class\n",
    "findwordInMessages = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        # Msg count in each class\n",
    "        readCounts(f, msgCounts)\n",
    "        \n",
    "        # Occurrence count of the findword in each class\n",
    "        readCounts(f, findwordCounts)\n",
    "        \n",
    "        # Total word count in each class\n",
    "        readCounts(f, totalwordCounts)\n",
    "\n",
    "        # The number of occurrence of findword in each message, key'ed by the message id\n",
    "        readFindwordInMessages(f, findwordInMessages)\n",
    "\n",
    "# Train the model\n",
    "(priors, condProbs) = trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts)\n",
    "\n",
    "# Predict the class of each message\n",
    "# Please note that we sort the messages by msgId, so that we can display the result\n",
    "# in a consistent order\n",
    "for msgId, data in collections.OrderedDict(sorted(findwordInMessages.items())).iteritems():\n",
    "    trueClass = data[0]\n",
    "    findwordCount = data[1]\n",
    "    scores = {}\n",
    "    \n",
    "    # For each message, calculate the score of each class.\n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        scores[c] = math.log(priors[c]) + findwordCount * math.log(condProbs[c])\n",
    "    \n",
    "    # Predicted class = the one with the higher score\n",
    "    predictedClass = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    print \"%s\\t%d\\t%d\" % (msgId, trueClass, predictedClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 assistance\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4###\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results (accuracy)\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py \n",
    "\n",
    "performs the multiple-word multinomial Naive Bayes classification via the chosen list.\n",
    "No smoothing is needed in this HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: mapper code for HW1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import cPickle\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "regexes = {}\n",
    "\n",
    "# Create the list of compiled regex, one for each findword\n",
    "for word in findwords:\n",
    "    regexes[word] = re.compile(r'\\b' + re.escape(word) + r'\\b')\n",
    "\n",
    "# Regex for counting the total number of words in a msg\n",
    "regexAll = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "# For each of the count lists below, index=0 means non-spam, index=1 means spam.\n",
    "\n",
    "# For each class, the occurrence count of each word.\n",
    "findwordCounts = [defaultdict(int), defaultdict(int)] \n",
    "totalwordCounts = [0, 0] # The total counts among the two classes.\n",
    "msgCounts = [0, 0]  # Total count of msg among the two classes.\n",
    "\n",
    "# A dictionary holding the findword info for each message.\n",
    "# Key = MsgId\n",
    "# Value = a list: [message's true class, the occurrence count of each word]\n",
    "# The \"occurrence count of each word\" is stored as a dictionary where the key is the word\n",
    "# and the value is the count.\n",
    "findwordInMessages = {}\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        \n",
    "        msgId = parts[0]\n",
    "        spamIndicator = int(parts[1])\n",
    "\n",
    "        subject = \"\" if parts[2].strip() == \"NA\" else parts[2]\n",
    "        body = \"\" if parts[3].strip() == \"NA\" else parts[3]\n",
    "        text = subject + \" \" + body\n",
    "        \n",
    "        # Update the various counters\n",
    "        \n",
    "        msgCounts[spamIndicator] += 1\n",
    "        \n",
    "        wordCountsOfOneMsg = {} # Store the occurrence count of each word in this message\n",
    "        for word in findwords:\n",
    "            count = len(regexes[word].findall(text))\n",
    "            if count > 0:\n",
    "                findwordCounts[spamIndicator][word] += count\n",
    "                wordCountsOfOneMsg[word] = count\n",
    "            \n",
    "        findwordInMessages[msgId] = [spamIndicator, wordCountsOfOneMsg]\n",
    "        \n",
    "        totalwordCounts[spamIndicator] += len(regexAll.findall(text))\n",
    "\n",
    "# Use cPickle to dump all the data\n",
    "data = [msgCounts, findwordCounts, totalwordCounts, findwordInMessages]\n",
    "print cPickle.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Patrick Ng\n",
    "## Description: reducer code for HW1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import collections\n",
    "import cPickle\n",
    "\n",
    "def trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts, vocab):\n",
    "    \"\"\"\n",
    "    Train the Multinomial Naive Bayes model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        msgCounts : list of int\n",
    "            Count of messages in each class (0=non-spam, 1=spam)\n",
    "            \n",
    "        findwordCounts : list of dict of int\n",
    "            The list contains the occurrence count info in each class (0=non-spam, 1=spam)\n",
    "            In each dict, each item's key is the word, and each item's value is the occurrence\n",
    "            count of that word.\n",
    "            \n",
    "        totalwordCounts : list of int\n",
    "            Total number of words in each class (0=non-spam, 1=spam)\n",
    "            \n",
    "        vocab : set of str\n",
    "            The vocabulary set\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "        priors : dict of float\n",
    "            The priors (Pr(c)) of each class.\n",
    "            \n",
    "        condProbs : dict of dict of float\n",
    "            The outer dictionary contains the conditional probability of each class.\n",
    "            For each of the inner dict, each item's key is the word, and each item's value\n",
    "            is the conditional probability (Pr(X=t|c)) of that word.\n",
    "    \"\"\"\n",
    "    priors = {} # P(c)\n",
    "    condProbs = {0:{}, 1:{}} # P(X=t|c) of each word in each class\n",
    "    msgTotal = sum(msgCounts)\n",
    "    \n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        priors[c] = float(msgCounts[c]) / msgTotal\n",
    "        for term in vocab:\n",
    "            # By default we'll set word count to be zero if the term isn't found in the class.\n",
    "            wordCount = findwordCounts[c].get(term, 0)\n",
    "            \n",
    "            # Note: we will use Laplace smoothing because the word \"enlargementWithATypo\"\n",
    "            #       isn't found in the whole corpus.\n",
    "            condProbs[c][term] = float(wordCount + 1) / (totalwordCounts[c] + len(vocab))\n",
    "    \n",
    "    return priors, condProbs\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "# For each of the count list below, index=0 means non-spam, index=1 means spam.\n",
    "\n",
    "# For each class (each item in the list), the occurrence count of each word (in each dict, \n",
    "# each item's key is the word, and the item's value is the count)\n",
    "findwordCounts = [defaultdict(int), defaultdict(int)] \n",
    "totalwordCounts = [0, 0] # The total counts among the two classes.\n",
    "msgCounts = [0, 0]  # Total count of msg among the two classes.\n",
    "\n",
    "# A dictionary holding the findword info for each message.\n",
    "# Key = MsgId\n",
    "# Value = a list: [message's true class, the occurrence count of each word]\n",
    "# The \"occurrence count of each word\" is stored as a dictionary where the key is the word\n",
    "# and the value is the count.\n",
    "findwordInMessages = {}\n",
    "\n",
    "# The vocab seen\n",
    "vocab = set()\n",
    "    \n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        # Use cPickle to read the output from mapper\n",
    "        (msgCountsPart, findwordCountsPart, totalwordCountsPart, findwordInMessagesPart) = \\\n",
    "            cPickle.loads(f.read())\n",
    "        \n",
    "        # Update all our counters\n",
    "        \n",
    "        # Add up the message count of each class\n",
    "        msgCounts = [x + y for x, y in zip(msgCounts, msgCountsPart)]\n",
    "        \n",
    "        # Add up the total word count of each class\n",
    "        totalwordCounts = [x + y for x, y in zip(totalwordCounts, totalwordCountsPart)]\n",
    "        \n",
    "        # For each class, update the occurrence count of each word\n",
    "        for c in [0,1]:\n",
    "            # Both findwordCounts and findwordCountsPart are dictionaries where the key\n",
    "            # is the word, and the value is the count.\n",
    "            findwordCounts[c] = dict(Counter(findwordCounts[c]) + Counter(findwordCountsPart[c]))\n",
    "        \n",
    "        # Update the dictionary which holds the findword info about all the messages\n",
    "        findwordInMessages.update(findwordInMessagesPart)\n",
    "\n",
    "# Calculate the vocab seen in all the docs\n",
    "for c in [0,1]:\n",
    "    vocab = vocab.union(set(findwordCounts[c].keys()))\n",
    "\n",
    "# Train the model\n",
    "(priors, condProbs) = trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts, vocab)\n",
    "\n",
    "\n",
    "correctPrediction = 0\n",
    "\n",
    "# Predict the class of each message\n",
    "# Please note that we sort the messages by msgId, so that we can display the result\n",
    "# in a consistent order\n",
    "for msgId, data in collections.OrderedDict(sorted(findwordInMessages.items())).iteritems():\n",
    "    trueClass = data[0]\n",
    "    wordCountsOfOneMsg = data[1]\n",
    "    wordsOfOneMsg = data[1].keys()\n",
    "    scores = {}\n",
    "\n",
    "    # For each message, calculate the score of each class.\n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        scores[c] = math.log(priors[c])\n",
    "        for term in wordsOfOneMsg:\n",
    "            scores[c] += math.log(condProbs[c][term])\n",
    "\n",
    "    # Predicted class = the one with the higher score\n",
    "    predictedClass = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    if predictedClass == trueClass:\n",
    "        correctPrediction += 1\n",
    "    \n",
    "    print \"%s\\t%d\\t%d\" % (msgId, trueClass, predictedClass)\n",
    "\n",
    "print\n",
    "print \"Accuracy = %.2f\" % (float(correctPrediction) / len(findwordInMessages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "\r\n",
      "Accuracy = 0.63\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.5###\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present.\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of all words, and\n",
    "   - reducer.py performs a word-distribution-wide Naive Bayes classification.\n",
    "\n",
    "In all cases, mapper.py will read in a portion of the email data,\n",
    "count some words and print out counts to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: mapper code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import cPickle\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "regexes = {}\n",
    "\n",
    "useAllWords = False\n",
    "\n",
    "if findwords[0] == '*':\n",
    "    useAllWords = True\n",
    "else:\n",
    "    # Create the list of compiled regex, one for each findword\n",
    "    for word in findwords:\n",
    "        regexes[word] = re.compile(r'\\b' + re.escape(word) + r'\\b')\n",
    "\n",
    "# Regex for counting the total number of words in a msg\n",
    "regexAll = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "# For each of the count lists below, index=0 means non-spam, index=1 means spam.\n",
    "\n",
    "# For each class (each item in the list), the occurrence count of each word (in each dict, \n",
    "# each item's key is the word, and the item's value is the count)\n",
    "findwordCounts = [defaultdict(int), defaultdict(int)] \n",
    "totalwordCounts = [0, 0] # The total counts among the two classes.\n",
    "msgCounts = [0, 0]  # Total count of msg among the two classes.\n",
    "\n",
    "# A dictionary holding the findword info for each message.\n",
    "# Key = MsgId\n",
    "# Value = a list: [message's true class, the occurrence count of each word]\n",
    "# The \"occurrence count of each word\" is stored as a dictionary where the key is the word\n",
    "# and the value is the count.\n",
    "findwordInMessages = {}\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        \n",
    "        msgId = parts[0]\n",
    "        spamIndicator = int(parts[1])\n",
    "\n",
    "        subject = \"\" if parts[2].strip() == \"NA\" else parts[2]\n",
    "        body = \"\" if parts[3].strip() == \"NA\" else parts[3]\n",
    "        text = subject + \" \" + body\n",
    "        \n",
    "        # Update the various counters\n",
    "        \n",
    "        msgCounts[spamIndicator] += 1\n",
    "        \n",
    "        wordCountsOfOneMsg = {} # Occurrence count of each word in this message\n",
    "        wordsInMsg = regexAll.findall(text) # The vocab in this message\n",
    "        \n",
    "        # Update the counters based on the occurrence count of each word this message\n",
    "        for word in wordsInMsg:\n",
    "            regex = regexes.get(word, re.compile(r'\\b' + re.escape(word) + r'\\b'))\n",
    "            count = len(regex.findall(text))\n",
    "            if count > 0:\n",
    "                findwordCounts[spamIndicator][word] += count\n",
    "                wordCountsOfOneMsg[word] = count\n",
    "            \n",
    "        findwordInMessages[msgId] = [spamIndicator, wordCountsOfOneMsg]\n",
    "        \n",
    "        totalwordCounts[spamIndicator] += len(regexAll.findall(text))\n",
    "\n",
    "# Use cPickle to output the data\n",
    "data = [msgCounts, findwordCounts, totalwordCounts, findwordInMessages]\n",
    "print cPickle.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Patrick Ng\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import collections\n",
    "import cPickle\n",
    "    \n",
    "def trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts, vocab):\n",
    "    priors = {} # P(c)\n",
    "    condProbs = {0:{}, 1:{}} # P(X=t|c)\n",
    "    msgTotal = sum(msgCounts)\n",
    "    \n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        priors[c] = float(msgCounts[c]) / msgTotal\n",
    "        for term in vocab:\n",
    "            # By default we'll set word count to be zero if the term isn't found in the class.\n",
    "            wordCount = findwordCounts[c].get(term, 0)\n",
    "            \n",
    "            # Note: we will use Laplace smoothing because the word \"enlargementWithATypo\"\n",
    "            #       isn't found in the whole corpus.\n",
    "            condProbs[c][term] = float(wordCount + 1) / (totalwordCounts[c] + len(vocab))\n",
    "    \n",
    "    return priors, condProbs\n",
    "\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "# For each of the count list below, index=0 means non-spam, index=1 means spam.\n",
    "\n",
    "# For each class (each item in the list), the occurrence count of each word (in each dict, \n",
    "# each item's key is the word, and the item's value is the count)\n",
    "findwordCounts = [defaultdict(int), defaultdict(int)] \n",
    "totalwordCounts = [0, 0] # The total counts among the two classes.\n",
    "msgCounts = [0, 0]  # Total count of msg among the two classes.\n",
    "\n",
    "# A dictionary holding the findword info for each message.\n",
    "# Key = MsgId\n",
    "# Value = a list: [message's true class, the occurrence count of each word]\n",
    "# The \"occurrence count of each word\" is stored as a dictionary where the key is the word\n",
    "# and the value is the count.\n",
    "findwordInMessages = {}\n",
    "\n",
    "# The vocab seen\n",
    "vocab = set()\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        (msgCountsPart, findwordCountsPart, totalwordCountsPart, findwordInMessagesPart) = \\\n",
    "            cPickle.loads(f.read())\n",
    "        \n",
    "        # Update all our counters\n",
    "        \n",
    "        # Add up the message count of each class\n",
    "        msgCounts = [x + y for x, y in zip(msgCounts, msgCountsPart)]\n",
    "        \n",
    "        # Add up the total count of each class\n",
    "        totalwordCounts = [x + y for x, y in zip(totalwordCounts, totalwordCountsPart)]\n",
    "        \n",
    "        # For each class, update the occurrence count of each word\n",
    "        for c in [0,1]:\n",
    "            # Both findwordCounts and findwordCountsPart are dictionaries where the key\n",
    "            # is the word, and the value is the count.\n",
    "            findwordCounts[c] = dict(Counter(findwordCounts[c]) + Counter(findwordCountsPart[c]))\n",
    "\n",
    "        # Update the dictionary which holds the findword info about all the messages\n",
    "        findwordInMessages.update(findwordInMessagesPart)\n",
    "\n",
    "# Calculate the vocab seen in all the docs\n",
    "for c in [0,1]:\n",
    "    vocab = vocab.union(set(findwordCounts[c].keys()))\n",
    "\n",
    "# Train the model\n",
    "(priors, condProbs) = trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts, vocab)\n",
    "\n",
    "\n",
    "correctPrediction = 0\n",
    "\n",
    "# Predict the class of each message\n",
    "# Please note that we sort the messages by msgId, so that we can display the result\n",
    "# in a consistent order\n",
    "for msgId, data in collections.OrderedDict(sorted(findwordInMessages.items())).iteritems():\n",
    "    trueClass = data[0]\n",
    "    wordCountsOfOneMsg = data[1]\n",
    "    wordsOfOneMsg = data[1].keys()\n",
    "    scores = {}\n",
    "    \n",
    "    # For each message, calculate the score of each class.\n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        scores[c] = math.log(priors[c])\n",
    "        for term in wordsOfOneMsg:\n",
    "            scores[c] += math.log(condProbs[c][term])\n",
    "\n",
    "    # Predicted class is the one with the higher score\n",
    "    predictedClass = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    if predictedClass == trueClass:\n",
    "        correctPrediction += 1\n",
    "    \n",
    "    print \"%s\\t%d\\t%d\" % (msgId, trueClass, predictedClass)\n",
    "\n",
    "print\n",
    "print \"Accuracy = %.2f\" % (float(correctPrediction) / len(findwordInMessages) )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t1\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t1\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t1\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t1\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t1\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0006.1999-12-13.kaminski\t0\t1\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t1\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t1\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t1\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t1\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t1\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0016.1999-12-15.farmer\t0\t1\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t1\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t1\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0018.1999-12-14.kaminski\t0\t1\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "\r\n",
      "Accuracy = 0.86\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 *\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
