{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Email: patng@ischool.berkeley.edu  \n",
    "Class: W261-2  \n",
    "Week: 01  \n",
    "Date of submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.0## \n",
    "**Big data** is broad term for data sets so large or complex that traditional data-processing applications are inadequate.  IBM has characterized big data by its 4 V's: Volume (scale of data), Velocity (analysis of streaming data), Variety (different forms of data) and Veractiy (uncertainty of data). \n",
    "\n",
    "For example, for a popular website which attracts ten millions visits each day, the amount of web log data generated each day is about 50GB.  The website also uses a recommendation engine to generate recommendations to the visitors.  In order to train the engine each night, we need to cleanse (e.g. remove logs generated by suspected robots) and transform the web log data into a format usable by the training process, and the training itself has to process all the new web log data, together with all data generated from the past.  The whole process has to complete within 6 hours due to business needs.  However, using traditional data-processing techniques the whole proceess could take more than 8 hours.  As a result, we need to make use of big data techniques such as HDFS and parallel computating in order to complete the processing within the required period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.1##\n",
    "To estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4 and 5, the high level strategy is:\n",
    "- Please note that each regression model will produce an estimator <i>g(x)</i> of the true function <i>f(x)</i>.\n",
    "- For each model:\n",
    "    - Using bootstrapping, generate datasets S<sub>1</sub>, S<sub>2</sub>, ..., S<sub>B</sub> from the dataset T.\n",
    "    - For each S<sub>b</sub>:\n",
    "      - Use S<sub>b</sub> to train the model to produce an estimator g<sub>b</sub>(x).\n",
    "      - Let the dataset T<sub>b</sub> = T \\ S<sub>b</sub> be the data points that do not appear in S<sub>b</sub>\n",
    "      - Calculate the predicted value g<sub>b</sub>(x) for each x in T<sub>b</sub>.\n",
    "    - This way we produce B estimators.  Average them out will produce the average estimator E[g(x)].\n",
    "  - We then calculate the bias, the variance and the irreduciable error of each model:\n",
    "    - For each estimaor\n",
    "    - Bias:\n",
    "      - The formula is: Bias = E[g(x)] - f(x)\n",
    "      - But "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the control script ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.1 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW1.1. Read through the provided control script (pNaiveBayes.sh)\n",
    "and all of its comments. When you are comfortable with their\n",
    "purpose and function, respond to the remaining homework questions below. \n",
    "'''\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.2 ##\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: mapper code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "findword = findwords[0] # for HW1.2 we handle only a single word\n",
    "regex = re.compile(r'\\b' + re.escape(findword) + r'\\b')\n",
    "count = 0\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        text = parts[2] + \" \" + parts[3] # Subjects and contents\n",
    "        count += len(regex.findall(text))\n",
    "\n",
    "print '%s\\t%d' % (findword, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "totalCount = 0\n",
    "findword = None\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        line = f.readline()\n",
    "        parts = line.split('\\t')\n",
    "        findword = parts[0]\n",
    "        count = int(parts[1])\n",
    "        totalCount += count\n",
    "\n",
    "print '%s\\t%d' % (findword, totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; chmod +x pNaiveBayes.sh;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 assistance\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.3. ##  \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "findword = findwords[0] # for HW1.3 we handle only a single word\n",
    "regex = re.compile(r'\\b' + re.escape(findword) + r'\\b')\n",
    "regexAll = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "# For each of the count list below, index=0 means non-spam, index=1 means spam.\n",
    "findwordCounts = [0, 0] # The findword counts among the two classes\n",
    "totalwordCounts = [0, 0] # The total counts among the two classes\n",
    "msgCounts = [0, 0]  # Total count of msg among the two classes\n",
    "\n",
    "# For each msg: MsgId, its True class, and the occurrence count of findword\n",
    "findwordInMessages = {}\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        \n",
    "        msgId = parts[0]\n",
    "        spamIndicator = int(parts[1])\n",
    "        text = parts[2] + \" \" + parts[3] # Subjects and contents\n",
    "        \n",
    "        # Update the various counters\n",
    "        \n",
    "        msgCounts[spamIndicator] += 1\n",
    "        \n",
    "        findwordCount = len(regex.findall(text))\n",
    "        findwordCounts[spamIndicator] += findwordCount\n",
    "        findwordInMessages[msgId] = [spamIndicator, findwordCount]\n",
    "        \n",
    "        totalwordCounts[spamIndicator] += len(regexAll.findall(text))\n",
    "\n",
    "\n",
    "# Msg count in each class\n",
    "print \"%d\\t%d\" % (msgCounts[0], msgCounts[1]) \n",
    "\n",
    "# Occurrence count of the findword in each class\n",
    "print \"%d\\t%d\" % (findwordCounts[0], findwordCounts[1]) \n",
    "\n",
    "# Total word count in each class\n",
    "print \"%d\\t%d\" % (totalwordCounts[0], totalwordCounts[1])\n",
    "\n",
    "# For each msg: MsgId, occurrence count of findword, its True class\n",
    "for k, v in findwordInMessages.iteritems():\n",
    "    print \"%s\\t%d\\t%d\" % (k, v[0], v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "# For each of the count list below, index=0 means non-spam, index=1 means spam.\n",
    "findwordCounts = [0.0, 0.0] # The findword counts among the two classes\n",
    "totalwordCounts = [0.0, 0.0] # The total counts among the two classes\n",
    "msgCounts = [0.0, 0.0]  # Total count of msg among the two classes\n",
    "\n",
    "# For each msg: MsgId, occurrence count of findword, its True class\n",
    "findwordInMessages = {}\n",
    "\n",
    "def readFindwordInMessages(f, countDict):\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        msgId = parts[0]\n",
    "        trueClass = int(parts[1])\n",
    "        count = float(parts[2])\n",
    "        countDict[msgId] = [trueClass, count]\n",
    "\n",
    "def readCounts(f, counts):\n",
    "    line = f.readline()\n",
    "    parts = re.split(\"\\t\", line)\n",
    "    counts[0] += float(parts[0])\n",
    "    counts[1] += float(parts[1])\n",
    "    \n",
    "def trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts):\n",
    "    priors = {} # P(c)\n",
    "    condProbs = {} # P(X=t|c)\n",
    "    msgTotal = sum(msgCounts)\n",
    "    \n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        priors[c] = msgCounts[c] / msgTotal\n",
    "        condProbs[c] = findwordCounts[c] / totalwordCounts[c]\n",
    "        \n",
    "    return priors, condProbs\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        # Msg count in each class\n",
    "        readCounts(f, msgCounts)\n",
    "        \n",
    "        # Occurrence count of the findword in each class\n",
    "        readCounts(f, findwordCounts)\n",
    "        \n",
    "        # Total word count in each class\n",
    "        readCounts(f, totalwordCounts)\n",
    "\n",
    "        # The number of occurrence of findword in each message, key'ed by the message id\n",
    "        readFindwordInMessages(f, findwordInMessages)\n",
    "\n",
    "(priors, condProbs) = trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts)\n",
    "\n",
    "\n",
    "# Predict the class of each message\n",
    "# Please note that we sort the messages by msgId, so that we can display the result\n",
    "# in a consistent order\n",
    "for msgId, data in collections.OrderedDict(sorted(findwordInMessages.items())).iteritems():\n",
    "    trueClass = data[0]\n",
    "    findwordCount = data[1]\n",
    "    scores = {}\n",
    "    \n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        scores[c] = math.log(priors[c]) + findwordCount * math.log(condProbs[c])\n",
    "    \n",
    "    predictedClass = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    print \"%s\\t%d\\t%d\" % (msgId, trueClass, predictedClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 assistance\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.4##\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
