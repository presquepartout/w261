{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Email: patng@ischool.berkeley.edu  \n",
    "Class: W261-2  \n",
    "Week: 01  \n",
    "Date of submission: Jan 18, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.0## \n",
    "**Big data** is broad term for data sets so large or complex that traditional data-processing applications are inadequate.  IBM has characterized big data by its 4 V's: Volume (scale of data), Velocity (analysis of streaming data), Variety (different forms of data) and Veractiy (uncertainty of data). \n",
    "\n",
    "For example, for a popular website which attracts ten millions visits each day, the amount of web log data generated each day is about 50GB.  The website also uses a recommendation engine to generate recommendations to the visitors.  In order to train the engine each night, we need to cleanse (e.g. remove logs generated by suspected robots) and transform the web log data into a format usable by the training process, and the training itself has to process all the new web log data, together with all data generated from the past.  The whole process has to complete within 6 hours due to business needs.  However, using traditional data-processing techniques the whole proceess could take more than 8 hours.  As a result, we need to make use of big data techniques such as HDFS and parallel computating in order to complete the processing within the required period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.1##\n",
    "To estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4 and 5, the strategy is:\n",
    "- First, please note that each regression model will produce an estimator <i>g(x)</i> of the true function <i>f(x)</i>.\n",
    "- For each model:\n",
    "    - Using bootstrapping, generate datasets S<sub>1</sub>, S<sub>2</sub>, ..., S<sub>B</sub> from the dataset T.\n",
    "    - For each S<sub>b</sub>:\n",
    "      - Use S<sub>b</sub> to train the model to produce an estimator g<sub>b</sub>(x).\n",
    "      - Let the dataset T<sub>b</sub> = T \\ S<sub>b</sub> be the data points that do not appear in S<sub>b</sub>\n",
    "      - Calculate the predicted value g<sub>b</sub>(x) for each x in T<sub>b</sub>.\n",
    "    - Now we have several predictions for each data point x in T. From them we can calculate E<sub>B</sub>[g(x)] for each x.\n",
    "    - For each data point x, we can now calculate the bias, the variance and the irreduciable error of the model:\n",
    "        - Bias = E<sub>B</sub>[g(x)] - y\n",
    "        - Variance = $\\sum_{k}$(y<sub>k</sub> - E<sub>B</sub>[g(x)])<sup>2</sup> / (K-1)\n",
    "            - where K is the number of predictions made for each data point x in T\n",
    "        - Irreduciable Error / Noise:\n",
    "            - Since we don't have the true function f(x), we have to estimate the noise at each data point x:\n",
    "                - In T, if several data points exists at x, then:\n",
    "                    - noise = variance of the multiple y's from those data points\n",
    "                - Otherwise, assume noise = 0 for x\n",
    "                  \n",
    "To select a model, for each model we calculate the <i>expected prediction error</i>:\n",
    "- Expected prediction error = E<sub>X</sub>[Variance + Bias<sup>2</sup> + Noise<sup>2</sup>]\n",
    "    - Note: E<sub>X</sub> means we average it over all data points.\n",
    "\n",
    "The model with the lowest expected prediction error will be chosen.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the control script ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.1 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "HW1.1. Read through the provided control script (pNaiveBayes.sh)\n",
    "and all of its comments. When you are comfortable with their\n",
    "purpose and function, respond to the remaining homework questions below. \n",
    "'''\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.2 ##\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: mapper code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "findword = findwords[0] # for HW1.2 we handle only a single word\n",
    "regex = re.compile(r'\\b' + re.escape(findword) + r'\\b')\n",
    "count = 0\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        \n",
    "        subject = \"\" if parts[2].strip() == \"NA\" else parts[2]\n",
    "        body = \"\" if parts[3].strip() == \"NA\" else parts[3]\n",
    "        text = subject + \" \" + body\n",
    "        \n",
    "        count += len(regex.findall(text))\n",
    "\n",
    "print '%s\\t%d' % (findword, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "totalCount = 0\n",
    "findword = None\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        line = f.readline()\n",
    "        parts = line.split('\\t')\n",
    "        findword = parts[0]\n",
    "        count = int(parts[1])\n",
    "        totalCount += count\n",
    "\n",
    "print '%s\\t%d' % (findword, totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; chmod +x pNaiveBayes.sh;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 assistance\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.3. ##  \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "findword = findwords[0] # for HW1.3 we handle only a single word\n",
    "\n",
    "# Regex for counting the number of findword in msg\n",
    "regex = re.compile(r'\\b' + re.escape(findword) + r'\\b')\n",
    "\n",
    "# Regex for counting the total number of words in a msg\n",
    "regexAll = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "# For each of the count list below, index=0 means non-spam, index=1 means spam.\n",
    "findwordCounts = [0, 0] # The findword counts among the two classes\n",
    "totalwordCounts = [0, 0] # The total counts among the two classes\n",
    "msgCounts = [0, 0]  # Total count of msg among the two classes\n",
    "\n",
    "# For each msg: MsgId, its True class, and the occurrence count of findword\n",
    "findwordInMessages = {}\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        \n",
    "        msgId = parts[0]\n",
    "        spamIndicator = int(parts[1])\n",
    "\n",
    "        subject = \"\" if parts[2].strip() == \"NA\" else parts[2]\n",
    "        body = \"\" if parts[3].strip() == \"NA\" else parts[3]\n",
    "        text = subject + \" \" + body\n",
    "        \n",
    "        # Update the various counters\n",
    "        \n",
    "        msgCounts[spamIndicator] += 1\n",
    "        \n",
    "        findwordCount = len(regex.findall(text))\n",
    "        findwordCounts[spamIndicator] += findwordCount\n",
    "        findwordInMessages[msgId] = [spamIndicator, findwordCount]\n",
    "        \n",
    "        totalwordCounts[spamIndicator] += len(regexAll.findall(text))\n",
    "\n",
    "\n",
    "# Msg count in each class\n",
    "print \"%d\\t%d\" % (msgCounts[0], msgCounts[1]) \n",
    "\n",
    "# Occurrence count of the findword in each class\n",
    "print \"%d\\t%d\" % (findwordCounts[0], findwordCounts[1]) \n",
    "\n",
    "# Total word count in each class\n",
    "print \"%d\\t%d\" % (totalwordCounts[0], totalwordCounts[1])\n",
    "\n",
    "# For each msg: MsgId, occurrence count of findword, its True class\n",
    "for k, v in findwordInMessages.iteritems():\n",
    "    print \"%s\\t%d\\t%d\" % (k, v[0], v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import collections\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "# For each of the count list below, index=0 means non-spam, index=1 means spam.\n",
    "findwordCounts = [0.0, 0.0] # The findword counts among the two classes\n",
    "totalwordCounts = [0.0, 0.0] # The total counts among the two classes\n",
    "msgCounts = [0.0, 0.0]  # Total count of msg among the two classes\n",
    "\n",
    "# For each msg: MsgId, occurrence count of findword, its True class\n",
    "findwordInMessages = {}\n",
    "\n",
    "def readFindwordInMessages(f, countDict):\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        msgId = parts[0]\n",
    "        trueClass = int(parts[1])\n",
    "        count = float(parts[2])\n",
    "        countDict[msgId] = [trueClass, count]\n",
    "\n",
    "def readCounts(f, counts):\n",
    "    line = f.readline()\n",
    "    parts = re.split(\"\\t\", line)\n",
    "    counts[0] += float(parts[0])\n",
    "    counts[1] += float(parts[1])\n",
    "    \n",
    "def trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts):\n",
    "    priors = {} # P(c)\n",
    "    condProbs = {} # P(X=t|c)\n",
    "    msgTotal = sum(msgCounts)\n",
    "    \n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        priors[c] = msgCounts[c] / msgTotal\n",
    "        condProbs[c] = findwordCounts[c] / totalwordCounts[c]\n",
    "        \n",
    "    return priors, condProbs\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        # Msg count in each class\n",
    "        readCounts(f, msgCounts)\n",
    "        \n",
    "        # Occurrence count of the findword in each class\n",
    "        readCounts(f, findwordCounts)\n",
    "        \n",
    "        # Total word count in each class\n",
    "        readCounts(f, totalwordCounts)\n",
    "\n",
    "        # The number of occurrence of findword in each message, key'ed by the message id\n",
    "        readFindwordInMessages(f, findwordInMessages)\n",
    "\n",
    "(priors, condProbs) = trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts)\n",
    "\n",
    "\n",
    "# Predict the class of each message\n",
    "# Please note that we sort the messages by msgId, so that we can display the result\n",
    "# in a consistent order\n",
    "for msgId, data in collections.OrderedDict(sorted(findwordInMessages.items())).iteritems():\n",
    "    trueClass = data[0]\n",
    "    findwordCount = data[1]\n",
    "    scores = {}\n",
    "    \n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        scores[c] = math.log(priors[c]) + findwordCount * math.log(condProbs[c])\n",
    "    \n",
    "    predictedClass = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    print \"%s\\t%d\\t%d\" % (msgId, trueClass, predictedClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 assistance\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.4##\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: mapper code for HW1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import cPickle\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "regexes = {}\n",
    "\n",
    "# Create the list of compiled regex, one for each findword\n",
    "for word in findwords:\n",
    "    regexes[word] = re.compile(r'\\b' + re.escape(word) + r'\\b')\n",
    "\n",
    "# Regex for counting the total number of words in a msg\n",
    "regexAll = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "# For each of the count lists below, index=0 means non-spam, index=1 means spam.\n",
    "\n",
    "# For each class, the occurrence count of each word.\n",
    "findwordCounts = [defaultdict(int), defaultdict(int)] \n",
    "totalwordCounts = [0, 0] # The total counts among the two classes.\n",
    "msgCounts = [0, 0]  # Total count of msg among the two classes.\n",
    "\n",
    "# For each msg: MsgId, its True class, and the occurrence count of each word.\n",
    "findwordInMessages = {}\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        \n",
    "        msgId = parts[0]\n",
    "        spamIndicator = int(parts[1])\n",
    "\n",
    "        subject = \"\" if parts[2].strip() == \"NA\" else parts[2]\n",
    "        body = \"\" if parts[3].strip() == \"NA\" else parts[3]\n",
    "        text = subject + \" \" + body\n",
    "        \n",
    "        # Update the various counters\n",
    "        \n",
    "        msgCounts[spamIndicator] += 1\n",
    "        \n",
    "        wordCountsOfOneMsg = {} # Store the occurrence count of each word in this message\n",
    "        for word in findwords:\n",
    "            count = len(regexes[word].findall(text))\n",
    "            if count > 0:\n",
    "                findwordCounts[spamIndicator][word] += count\n",
    "                wordCountsOfOneMsg[word] = count\n",
    "            \n",
    "        findwordInMessages[msgId] = [spamIndicator, wordCountsOfOneMsg]\n",
    "        \n",
    "        totalwordCounts[spamIndicator] += len(regexAll.findall(text))\n",
    "\n",
    "# Use cPickle to dump all the data\n",
    "data = [msgCounts, findwordCounts, totalwordCounts, findwordInMessages]\n",
    "print cPickle.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: reducer code for HW1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import collections\n",
    "import cPickle\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "# For each of the count list below, index=0 means non-spam, index=1 means spam.\n",
    "findwordCounts = [defaultdict(int), defaultdict(int)] # For each class, the occurrence count of each word.\n",
    "totalwordCounts = [0, 0] # The total counts among the two classes.\n",
    "msgCounts = [0, 0]  # Total count of msg among the two classes.\n",
    "\n",
    "# For each msg: MsgId, its True class, and the occurrence count of each word.\n",
    "findwordInMessages = {}\n",
    "\n",
    "# The vocab seen\n",
    "vocab = set()\n",
    "    \n",
    "def trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts, vocab):\n",
    "    priors = {} # P(c)\n",
    "    condProbs = {0:{}, 1:{}} # P(X=t|c)\n",
    "    msgTotal = sum(msgCounts)\n",
    "    \n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        priors[c] = float(msgCounts[c]) / msgTotal\n",
    "        for term in vocab:\n",
    "            # By default we'll set word count to be zero if the term isn't found in the class.\n",
    "            wordCount = findwordCounts[c].get(term, 0)\n",
    "            \n",
    "            # Note: we will use Laplace smoothing because the word \"enlargementWithATypo\"\n",
    "            #       isn't found in the whole corpus.\n",
    "            condProbs[c][term] = float(wordCount + 1) / (totalwordCounts[c] + len(vocab))\n",
    "    \n",
    "    return priors, condProbs\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        # Use cPickle to read the output from mapper\n",
    "        (msgCountsPart, findwordCountsPart, totalwordCountsPart, findwordInMessagesPart) = \\\n",
    "            cPickle.loads(f.read())\n",
    "        \n",
    "        # Update all our counters\n",
    "        msgCounts = [x + y for x, y in zip(msgCounts, msgCountsPart)]\n",
    "        totalwordCounts = [x + y for x, y in zip(totalwordCounts, totalwordCountsPart)]\n",
    "        \n",
    "        # For each class, update the occurrence count of each word\n",
    "        for c in [0,1]:\n",
    "            findwordCounts[c] = dict(Counter(findwordCounts[c]) + Counter(findwordCountsPart[c]))\n",
    "        \n",
    "        findwordInMessages.update(findwordInMessagesPart)\n",
    "\n",
    "# Calculate the vocab seen in all the docs\n",
    "for c in [0,1]:\n",
    "    vocab = vocab.union(set(findwordCounts[c].keys()))\n",
    "        \n",
    "(priors, condProbs) = trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts, vocab)\n",
    "\n",
    "\n",
    "# Predict the class of each message\n",
    "# Please note that we sort the messages by msgId, so that we can display the result\n",
    "# in a consistent order\n",
    "for msgId, data in collections.OrderedDict(sorted(findwordInMessages.items())).iteritems():\n",
    "    trueClass = data[0]\n",
    "    wordCountsOfOneMsg = data[1]\n",
    "    wordsOfOneMsg = data[1].keys()\n",
    "    scores = {}\n",
    "    \n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        scores[c] = math.log(priors[c])\n",
    "        for term in wordsOfOneMsg:\n",
    "            scores[c] += math.log(condProbs[c][term])\n",
    "    \n",
    "    predictedClass = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    print \"%s\\t%d\\t%d\" % (msgId, trueClass, predictedClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.5##\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: mapper code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import cPickle\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "regexes = {}\n",
    "\n",
    "useAllWords = False\n",
    "\n",
    "if findwords[0] == '*':\n",
    "    useAllWords = True\n",
    "else:\n",
    "    # Create the list of compiled regex, one for each findword\n",
    "    for word in findwords:\n",
    "        regexes[word] = re.compile(r'\\b' + re.escape(word) + r'\\b')\n",
    "\n",
    "# Regex for counting the total number of words in a msg\n",
    "regexAll = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "# For each of the count lists below, index=0 means non-spam, index=1 means spam.\n",
    "\n",
    "# For each class, the occurrence count of each word.\n",
    "findwordCounts = [defaultdict(int), defaultdict(int)] \n",
    "totalwordCounts = [0, 0] # The total counts among the two classes.\n",
    "msgCounts = [0, 0]  # Total count of msg among the two classes.\n",
    "\n",
    "# For each msg: MsgId, its True class, and the occurrence count of each word.\n",
    "findwordInMessages = {}\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = re.split(\"\\t\", line)\n",
    "        \n",
    "        msgId = parts[0]\n",
    "        spamIndicator = int(parts[1])\n",
    "\n",
    "        subject = \"\" if parts[2].strip() == \"NA\" else parts[2]\n",
    "        body = \"\" if parts[3].strip() == \"NA\" else parts[3]\n",
    "        text = subject + \" \" + body\n",
    "        \n",
    "        # Update the various counters\n",
    "        \n",
    "        msgCounts[spamIndicator] += 1\n",
    "        \n",
    "        wordCountsOfOneMsg = {} # Occurrencd count of each word in this message\n",
    "        wordsInMsg = regexAll.findall(text) # The vocab in this message\n",
    "        \n",
    "        # Update the counters based on the occurrence count of each word this message\n",
    "        for word in wordsInMsg:\n",
    "            regex = regexes.get(word, re.compile(r'\\b' + re.escape(word) + r'\\b'))\n",
    "            count = len(regex.findall(text))\n",
    "            if count > 0:\n",
    "                findwordCounts[spamIndicator][word] += count\n",
    "                wordCountsOfOneMsg[word] = count\n",
    "            \n",
    "        findwordInMessages[msgId] = [spamIndicator, wordCountsOfOneMsg]\n",
    "        \n",
    "        totalwordCounts[spamIndicator] += len(regexAll.findall(text))\n",
    "\n",
    "# Use cPickle to output the data\n",
    "data = [msgCounts, findwordCounts, totalwordCounts, findwordInMessages]\n",
    "print cPickle.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Patrick Ng\n",
    "## Description: reducer code for HW1.5\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import collections\n",
    "import cPickle\n",
    "\n",
    "## collect user input\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "# For each of the count list below, index=0 means non-spam, index=1 means spam.\n",
    "findwordCounts = [defaultdict(int), defaultdict(int)] # For each class, the occurrence count of each word.\n",
    "totalwordCounts = [0, 0] # The total counts among the two classes.\n",
    "msgCounts = [0, 0]  # Total count of msg among the two classes.\n",
    "\n",
    "# For each msg: MsgId, its True class, and the occurrence count of each word.\n",
    "findwordInMessages = {}\n",
    "\n",
    "# The vocab seen\n",
    "vocab = set()\n",
    "    \n",
    "def trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts, vocab):\n",
    "    priors = {} # P(c)\n",
    "    condProbs = {0:{}, 1:{}} # P(X=t|c)\n",
    "    msgTotal = sum(msgCounts)\n",
    "    \n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        priors[c] = float(msgCounts[c]) / msgTotal\n",
    "        for term in vocab:\n",
    "            # By default we'll set word count to be zero if the term isn't found in the class.\n",
    "            wordCount = findwordCounts[c].get(term, 0)\n",
    "            \n",
    "            # Note: we will use Laplace smoothing because the word \"enlargementWithATypo\"\n",
    "            #       isn't found in the whole corpus.\n",
    "            condProbs[c][term] = float(wordCount + 1) / (totalwordCounts[c] + len(vocab))\n",
    "    \n",
    "    return priors, condProbs\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        (msgCountsPart, findwordCountsPart, totalwordCountsPart, findwordInMessagesPart) = \\\n",
    "            cPickle.loads(f.read())\n",
    "        \n",
    "        # Update all our counters\n",
    "        msgCounts = [x + y for x, y in zip(msgCounts, msgCountsPart)]\n",
    "        totalwordCounts = [x + y for x, y in zip(totalwordCounts, totalwordCountsPart)]\n",
    "        \n",
    "        # For each class, update the occurrence count of each word\n",
    "        for c in [0,1]:\n",
    "            findwordCounts[c] = dict(Counter(findwordCounts[c]) + Counter(findwordCountsPart[c]))\n",
    "        \n",
    "        findwordInMessages.update(findwordInMessagesPart)\n",
    "\n",
    "# Calculate the vocab seen in all the docs\n",
    "for c in [0,1]:\n",
    "    vocab = vocab.union(set(findwordCounts[c].keys()))\n",
    "        \n",
    "(priors, condProbs) = trainMultinomialNB(msgCounts, findwordCounts, totalwordCounts, vocab)\n",
    "\n",
    "\n",
    "# Predict the class of each message\n",
    "# Please note that we sort the messages by msgId, so that we can display the result\n",
    "# in a consistent order\n",
    "for msgId, data in collections.OrderedDict(sorted(findwordInMessages.items())).iteritems():\n",
    "    trueClass = data[0]\n",
    "    wordCountsOfOneMsg = data[1]\n",
    "    wordsOfOneMsg = data[1].keys()\n",
    "    scores = {}\n",
    "    \n",
    "    # 0: non-spam, 1: spam\n",
    "    for c in [0, 1]:\n",
    "        scores[c] = math.log(priors[c])\n",
    "        for term in wordsOfOneMsg:\n",
    "            scores[c] += math.log(condProbs[c][term])\n",
    "    \n",
    "    predictedClass = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    print \"%s\\t%d\\t%d\" % (msgId, trueClass, predictedClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t1\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t1\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t1\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t1\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t1\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0006.1999-12-13.kaminski\t0\t1\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t1\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t1\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t1\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t1\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t1\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0016.1999-12-15.farmer\t0\t1\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t1\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t1\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0018.1999-12-14.kaminski\t0\t1\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 *\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
