{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Email: patng@ischool.berkeley.edu  \n",
    "Class: W261-2  \n",
    "Week: 03  \n",
    "Date of submission: Feb 01, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.0.\n",
    "\n",
    "What is a merge sort? Where is it used in Hadoop?  \n",
    "How is  a combiner function in the context of Hadoop?   \n",
    "Give an example where it can be used and justify why it should be used in the context of this problem.  \n",
    "What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.1 \n",
    "Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, letâ€™s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    fields = line.split(\",\")\n",
    "\n",
    "    # Skip header row\n",
    "    if fields[0] == \"Complaint ID\":\n",
    "        continue\n",
    "\n",
    "    reason = fields[1].lower()\n",
    "    \n",
    "    if reason == \"debt collection\":\n",
    "        counter = \"debt\"\n",
    "    elif reason == \"mortgage\":\n",
    "        counter = \"mortgage\"\n",
    "    else:\n",
    "        counter = \"others\"\n",
    "        \n",
    "    sys.stderr.write('reporter:counter:custom,' + counter + ',1\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,mortgage,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,debt,1\r\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "!head -n 20 Consumer_Complaints.csv  | python mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 17:26:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 17:26:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# upload input file to hdfs\n",
    "!hdfs dfs -rm -f Consumer_Complaints.csv\n",
    "!hdfs dfs -put Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 17:40:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.1\n",
      "16/01/30 17:40:12 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/30 17:40:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar6298878087895734314/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob2623231940027838210.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r hw3.1\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=0 \\\n",
    "-file mapper.py -mapper mapper.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output hw3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "![result](https://photos-3.dropbox.com/t/2/AAD96apEVb1NEyOodPWCtkdcs8K_w-nW4PJtqZe6LTUSCQ/12/15674996/png/32x32/1/_/1/2/Screenshot%202016-01-30%2017.42.35.png/EKi01gsYnS8gBygH/Hs_dhV-YD5vL1Ja5tyxj1tFInEhRb2H_uNNtVF96zBs?size=1024x768&size_mode=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2  - Part 1\n",
    "\n",
    "Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 17:53:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `input.txt': No such file or directory\n",
      "16/01/30 17:53:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Prepare input file\n",
    "!echo \"foo foo quux labs foo bar quux\" > input.txt\n",
    "!hdfs dfs -rm -r input.txt\n",
    "!hdfs dfs -put input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "    words = line.strip().split(\" \")\n",
    "    for word in words:\n",
    "        print word + \"\\t1\" \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "wordCount = 0\n",
    "prev = None # the word previously seen\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    (word, count) = line.strip().split('\\t')\n",
    "    count = int(count)\n",
    "    \n",
    "    # If we have encountered a new word, output the answer of the previous word\n",
    "    if prev != word:\n",
    "        if prev is not None:\n",
    "            sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "            print \"%s\\t%d\" % (prev, wordCount)\n",
    "            wordCount = 0\n",
    "            \n",
    "    wordCount += count\n",
    "    prev = word\n",
    "\n",
    "\n",
    "# Output for the last word seen\n",
    "if prev is not None:\n",
    "    sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "    print \"%s\\t%d\" % (prev, wordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,mapper_called,1\n",
      "reporter:counter:custom,reducer_called,1\n",
      "bar\t1\n",
      "reporter:counter:custom,reducer_called,1\n",
      "foo\t3\n",
      "reporter:counter:custom,reducer_called,1\n",
      "labs\t1\n",
      "reporter:counter:custom,reducer_called,1\n",
      "quux\t2\n"
     ]
    }
   ],
   "source": [
    "!cat input.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 18:22:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.2-part1\n",
      "16/01/31 18:22:52 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 18:22:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar1822858052759819100/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob7516553153088005811.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw3.2-part1\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-file mapper.py -mapper mapper.py \\\n",
    "-file reducer.py -reducer reducer.py \\\n",
    "-input input.txt \\\n",
    "-output hw3.2-part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 18:23:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "bar\t1\n",
      "foo\t3\n",
      "labs\t1\n",
      "quux\t2\n"
     ]
    }
   ],
   "source": [
    "# Show result\n",
    "!hdfs dfs -cat hw3.2-part1/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Part 1\n",
    "\n",
    "The mapper was called once, because there is only one line of input.  \n",
    "The reducer was called four times, because there are four words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2  - Part 2\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper (reduce last Reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Word delimiters are space, \\ and \"\n",
    "regex = re.compile(r\"[\\s/\\\"]+\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    fields = line.strip().split(\",\")\n",
    "\n",
    "    # Skip header row\n",
    "    if fields[0] == \"Complaint ID\":\n",
    "        continue\n",
    "\n",
    "    sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "    \n",
    "    words = filter(None, regex.split(fields[3]))\n",
    "    for word in words:\n",
    "        print word + \"\\t1\" \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,mapper_called,1\n",
      "reporter:counter:custom,mapper_called,1\n",
      "reporter:counter:custom,mapper_called,1\n",
      "reporter:counter:custom,reducer_called,1\n",
      "Deposits\t1\n",
      "reporter:counter:custom,reducer_called,1\n",
      "Disclosure\t2\n",
      "reporter:counter:custom,reducer_called,1\n",
      "and\t1\n",
      "reporter:counter:custom,reducer_called,1\n",
      "debt\t2\n",
      "reporter:counter:custom,reducer_called,1\n",
      "of\t2\n",
      "reporter:counter:custom,reducer_called,1\n",
      "verification\t2\n",
      "reporter:counter:custom,reducer_called,1\n",
      "withdrawals\t1\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 Consumer_Complaints.csv | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 18:23:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.2-part2\n",
      "16/01/31 18:23:54 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 18:23:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar6488566181085939433/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob1722145701917745206.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw3.2-part2\n",
    "\n",
    "# Use 4 mappers and 4 reducers\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapred.map.tasks=4 -D mapred.reduce.tasks=4 \\\n",
    "-file mapper.py -mapper mapper.py \\\n",
    "-file reducer.py -reducer reducer.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output hw3.2-part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Part 2\n",
    "\n",
    "The mapper was called 312,912 times.  \n",
    "The reducer was called 180 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.2 - Part 3\n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combiner (Note: Re-use the previous mapper and reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "wordCount = 0\n",
    "prev = None # the word previously seen\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    (word, count) = line.split('\\t')\n",
    "    count = int(count)\n",
    "    \n",
    "    # If we have encountered a new word, output the answer of the previous word\n",
    "    if prev != word:\n",
    "        if prev is not None:\n",
    "            sys.stderr.write('reporter:counter:custom,combiner_called,1\\n')\n",
    "            print \"%s\\t%d\" % (prev, wordCount)\n",
    "            wordCount = 0\n",
    "            \n",
    "    wordCount += count\n",
    "    prev = word\n",
    "\n",
    "\n",
    "# Output for the last word seen\n",
    "if prev is not None:\n",
    "    sys.stderr.write('reporter:counter:custom,combiner_called,1\\n')\n",
    "    print \"%s\\t%d\" % (prev, wordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,mapper_called,1\r\n",
      "reporter:counter:custom,mapper_called,1\r\n",
      "reporter:counter:custom,mapper_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "Deposits\t1\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "Disclosure\t2\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "and\t1\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "debt\t2\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "of\t2\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "verification\t2\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "withdrawals\t1\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 Consumer_Complaints.csv | python mapper.py | python combiner.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 18:25:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.2-part3\n",
      "16/01/31 18:25:27 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 18:25:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, combiner.py, reducer.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar5822869549960484937/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob1339676925685870465.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw3.2-part3\n",
    "\n",
    "# Use 4 mappers and 4 reducers\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapred.map.tasks=4 -D mapred.reduce.tasks=4 \\\n",
    "-file mapper.py -mapper mapper.py \\\n",
    "-file combiner.py -combiner combiner.py \\\n",
    "-file reducer.py -reducer reducer.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output hw3.2-part3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Part 3\n",
    "\n",
    "The mapper was called 312,912 times.  \n",
    "The combiner was called 628 times.  \n",
    "The reducer was called 180 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2 Part 4\n",
    "\n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    (word, count) = line.strip().split(\"\\t\")\n",
    "\n",
    "    sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "    \n",
    "    print count + \"\\t\" + word \n",
    "\n",
    "    # Use order inversion so that reducer can count the total word count in a single pass\n",
    "    print str(sys.maxint) + \"\\t\" + count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys\n",
    "\n",
    "wordCount = 0 # Count of each word\n",
    "totalCount = 0 # Total number of words\n",
    "prev = None # the word previously seen\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    (count, word) = line.strip().split('\\t')\n",
    "    count = int(count)\n",
    "\n",
    "    # Find out the total word count.\n",
    "    # We use count == sys.maxint as the special key for order inversion.\n",
    "    if count == sys.maxint:\n",
    "        totalCount += int(word) # The word is the count\n",
    "        continue\n",
    "    \n",
    "    # If we have encountered a new word, output the answer of the previous word\n",
    "    if prev != word:\n",
    "        if prev is not None:\n",
    "            sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "            print \"%s\\t%d\\t%f\" % (prev, wordCount, wordCount/totalCount)\n",
    "            wordCount = 0\n",
    "            \n",
    "    wordCount += count\n",
    "    prev = word\n",
    "\n",
    "\n",
    "# Output for the last word seen\n",
    "if prev is not None:\n",
    "    sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "    print \"%s\\t%d\\t%f\" % (prev, wordCount, wordCount/totalCount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 18:44:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 18:44:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `hw3.2-part4-input.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "# Prepare test input\n",
    "!hdfs dfs -getmerge hw3.2-part3/part-* hw3.2-part4-input.txt\n",
    "!hdfs dfs -put hw3.2-part4-input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,mapper_called,1\r\n",
      "reporter:counter:custom,mapper_called,1\r\n",
      "reporter:counter:custom,mapper_called,1\r\n",
      "reporter:counter:custom,mapper_called,1\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "Account\t16555\t0.874399\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "Can't\t1999\t0.105583\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "Cash\t240\t0.012676\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "Applied\t139\t0.007342\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 hw3.2-part4-input.txt | python mapper.py | sort -g -t\\t -k1 -r | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 18:54:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.2-part4\n",
      "16/01/31 18:54:53 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 18:54:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar1705867549816850294/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob5040353578864240849.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw3.2-part4\n",
    "\n",
    "# During sorting, use the 1st field (count) as the primary key, in reverse order.\n",
    "# Since we specify 2 key fields, the 2nd field (word) will be used as secondary key.\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k1nr -k2\" \\\n",
    "-file mapper.py -mapper mapper.py \\\n",
    "-file reducer.py -reducer reducer.py \\\n",
    "-input hw3.2-part3/part-* \\\n",
    "-output hw3.2-part4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 18:56:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 18:56:29 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "\n",
      "Top 50:\n",
      "Loan\t107254\t0.109389\n",
      "modification\t70487\t0.071890\n",
      "credit\t40483\t0.041289\n",
      "servicing\t36767\t0.037499\n",
      "report\t34903\t0.035598\n",
      "Incorrect\t29133\t0.029713\n",
      "information\t29069\t0.029648\n",
      "on\t29069\t0.029648\n",
      "or\t22533\t0.022982\n",
      "debt\t17966\t0.018324\n",
      "Account\t16555\t0.016885\n",
      "and\t16448\t0.016775\n",
      "opening\t16205\t0.016528\n",
      "Credit\t14768\t0.015062\n",
      "club\t12545\t0.012795\n",
      "health\t12545\t0.012795\n",
      "loan\t12376\t0.012622\n",
      "not\t12353\t0.012599\n",
      "Cont'd\t11848\t0.012084\n",
      "attempts\t11848\t0.012084\n",
      "collect\t11848\t0.012084\n",
      "owed\t11848\t0.012084\n",
      "of\t10885\t0.011102\n",
      "my\t10731\t0.010945\n",
      "Deposits\t10555\t0.010765\n",
      "withdrawals\t10555\t0.010765\n",
      "Problems\t9484\t0.009673\n",
      "Application\t8868\t0.009045\n",
      "to\t8401\t0.008568\n",
      "Billing\t8158\t0.008320\n",
      "Other\t7886\t0.008043\n",
      "disputes\t6938\t0.007076\n",
      "Communication\t6920\t0.007058\n",
      "tactics\t6920\t0.007058\n",
      "reporting\t6559\t0.006690\n",
      "lease\t6337\t0.006463\n",
      "the\t6248\t0.006372\n",
      "being\t5663\t0.005776\n",
      "by\t5663\t0.005776\n",
      "caused\t5663\t0.005776\n",
      "funds\t5663\t0.005776\n",
      "low\t5663\t0.005776\n",
      "process\t5505\t0.005615\n",
      "Disclosure\t5214\t0.005318\n",
      "verification\t5214\t0.005318\n",
      "Managing\t5006\t0.005106\n",
      "company's\t4858\t0.004955\n",
      "investigation\t4858\t0.004955\n",
      "card\t4405\t0.004493\n",
      "Unable\t4357\t0.004444\n",
      "\n",
      "Bottom 10:\n",
      "received\t98\t0.000100\n",
      "Payment\t92\t0.000094\n",
      "credited\t92\t0.000094\n",
      "Convenience\t75\t0.000076\n",
      "checks\t75\t0.000076\n",
      "amt\t71\t0.000072\n",
      "day\t71\t0.000072\n",
      "wrong\t71\t0.000072\n",
      "disclosures\t64\t0.000065\n",
      "missing\t64\t0.000065\n"
     ]
    }
   ],
   "source": [
    "# Show the reults\n",
    "!rm -f hw3.2-part4.result\n",
    "!hdfs dfs -get hw3.2-part4/part-00000 hw3.2-part4.result\n",
    "\n",
    "!echo\n",
    "!echo \"Top 50:\"\n",
    "!head -n 50 hw3.2-part4.result\n",
    "!echo\n",
    "!echo \"Bottom 10:\"\n",
    "!tail -n 10 hw3.2-part4.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 00:00:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Top 50 words (word, count, relative frequency):\t\n",
      "Loan, 107254, 0.109389\t\n",
      "modification, 70487, 0.071890\t\n",
      "credit, 40483, 0.041289\t\n",
      "servicing, 36767, 0.037499\t\n",
      "report, 34903, 0.035598\t\n",
      "Incorrect, 29133, 0.029713\t\n",
      "information, 29069, 0.029648\t\n",
      "on, 29069, 0.029648\t\n",
      "or, 22533, 0.022982\t\n",
      "debt, 17966, 0.018324\t\n",
      "Account, 16555, 0.016885\t\n",
      "and, 16448, 0.016775\t\n",
      "opening, 16205, 0.016528\t\n",
      "Credit, 14768, 0.015062\t\n",
      "club, 12545, 0.012795\t\n",
      "health, 12545, 0.012795\t\n",
      "loan, 12376, 0.012622\t\n",
      "not, 12353, 0.012599\t\n",
      "Cont'd, 11848, 0.012084\t\n",
      "attempts, 11848, 0.012084\t\n",
      "collect, 11848, 0.012084\t\n",
      "owed, 11848, 0.012084\t\n",
      "of, 10885, 0.011102\t\n",
      "my, 10731, 0.010945\t\n",
      "Deposits, 10555, 0.010765\t\n",
      "withdrawals, 10555, 0.010765\t\n",
      "Problems, 9484, 0.009673\t\n",
      "Application, 8868, 0.009045\t\n",
      "to, 8401, 0.008568\t\n",
      "Billing, 8158, 0.008320\t\n",
      "Other, 7886, 0.008043\t\n",
      "disputes, 6938, 0.007076\t\n",
      "Communication, 6920, 0.007058\t\n",
      "tactics, 6920, 0.007058\t\n",
      "reporting, 6559, 0.006690\t\n",
      "lease, 6337, 0.006463\t\n",
      "the, 6248, 0.006372\t\n",
      "being, 5663, 0.005776\t\n",
      "by, 5663, 0.005776\t\n",
      "caused, 5663, 0.005776\t\n",
      "funds, 5663, 0.005776\t\n",
      "low, 5663, 0.005776\t\n",
      "process, 5505, 0.005615\t\n",
      "Disclosure, 5214, 0.005318\t\n",
      "verification, 5214, 0.005318\t\n",
      "Managing, 5006, 0.005106\t\n",
      "company's, 4858, 0.004955\t\n",
      "investigation, 4858, 0.004955\t\n",
      "card, 4405, 0.004493\t\n",
      "Unable, 4357, 0.004444\t\n",
      "\t\n",
      "Bottom 10 words (word, count, relative frequency):\t\n",
      "disclosures, 64, 0.000065\t\n",
      "missing, 64, 0.000065\t\n",
      "amt, 71, 0.000072\t\n",
      "day, 71, 0.000072\t\n",
      "wrong, 71, 0.000072\t\n",
      "Convenience, 75, 0.000076\t\n",
      "checks, 75, 0.000076\t\n",
      "Payment, 92, 0.000094\t\n",
      "credited, 92, 0.000094\t\n",
      "Wrong, 98, 0.000100\t\n"
     ]
    }
   ],
   "source": [
    "# Get the results\n",
    "!hdfs dfs -get hw3.2-part4/part-00000 hw3.2-part4.result\n",
    "# Show the reults\n",
    "!rm -f w2.1.result\n",
    "!hdfs dfs -get sortRandomNums/part-00000 w2.1.result\n",
    "!echo\n",
    "!echo \"Top 50:\"\n",
    "!tail -n 50 hw3.2-part4.result\n",
    "!echo\n",
    "!echo \"Bottom 10:\"\n",
    "!head -n 10 hw3.2-part4.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 OPTIONAL \n",
    "Using 2 reducers: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.3. \n",
    "\n",
    "Shopping Cart Analysis  \n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.  \n",
    "\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session.   \n",
    "The items are separated by spaces.\n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
