{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Email: patng@ischool.berkeley.edu  \n",
    "Class: W261-2  \n",
    "Week: 03  \n",
    "Date of submission: Feb 01, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.0.\n",
    "\n",
    "What is a merge sort? Where is it used in Hadoop?  \n",
    "How is  a combiner function in the context of Hadoop?   \n",
    "Give an example where it can be used and justify why it should be used in the context of this problem.  \n",
    "What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge sort is a sorting algorithm based on divide-n-conquer.  \n",
    "\n",
    "\n",
    "1. Divide by finding the number qqq of the position midway between ppp and rrr. Do this step the same way we found the midpoint in binary search: add ppp and rrr, divide by 2, and round down.\n",
    "2. Conquer by recursively sorting the subarrays in each of the two subproblems created by the divide step. That is, recursively sort the subarray array[p..q] and recursively sort the subarray array[q+1..r].\n",
    "3. Combine by merging the two sorted subarrays back into the single sorted subarray array[p..r].  \n",
    "\n",
    "In Hadoop, it is used in the *shuffle and sort* phase of a mapreduce job.  \n",
    "\n",
    "In the context of Hadoop, a combiner acts as a mini-reducer which runs on the same node as the mapper.  It helps to reduce the size of the data which has to be transferred and processed at the shuffle and sort phase.  \n",
    "\n",
    "One example of the use of combiner is in the basic Word Count problem.  The mapper will emit a < word, count > pair for every word encountered in the input.  Running a combiner will merge all the records which have the same words into a single record.  This greatly reduce the size of the data which Hadoop needs to shuffle and sort.\n",
    "\n",
    "MapReduce makes the guarantee that the input to every reducer is sorted by key.\n",
    "The process by which the system performs the sort—and transfers the map outputs to the reducers as inputs—is known as the shuffle.  It includes partition, sort and combine, both in memory and on disk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.1 \n",
    "Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for fields in csv.reader(sys.stdin):\n",
    "\n",
    "    # Skip header row\n",
    "    if fields[0] == \"Complaint ID\":\n",
    "        continue\n",
    "\n",
    "    reason = fields[1].lower()\n",
    "    \n",
    "    if reason == \"debt collection\":\n",
    "        counter = \"debt\"\n",
    "    elif reason == \"mortgage\":\n",
    "        counter = \"mortgage\"\n",
    "    else:\n",
    "        counter = \"others\"\n",
    "        \n",
    "    sys.stderr.write('reporter:counter:custom,' + counter + ',1\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,mortgage,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,others,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,debt,1\r\n",
      "reporter:counter:custom,debt,1\r\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "!head -n 20 Consumer_Complaints.csv  | python mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 17:26:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 17:26:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# upload input file to hdfs\n",
    "!hdfs dfs -rm -f Consumer_Complaints.csv\n",
    "!hdfs dfs -put Consumer_Complaints.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 17:40:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.1\n",
      "16/01/30 17:40:12 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/30 17:40:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar6298878087895734314/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob2623231940027838210.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r hw3.1\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=0 \\\n",
    "-file mapper.py -mapper mapper.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output hw3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "![result](https://photos-3.dropbox.com/t/2/AAD96apEVb1NEyOodPWCtkdcs8K_w-nW4PJtqZe6LTUSCQ/12/15674996/png/32x32/1/_/1/2/Screenshot%202016-01-30%2017.42.35.png/EKi01gsYnS8gBygH/Hs_dhV-YD5vL1Ja5tyxj1tFInEhRb2H_uNNtVF96zBs?size=1024x768&size_mode=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2  - Part 1\n",
    "\n",
    "Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 15:06:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted input.txt\n",
      "16/02/01 15:06:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Prepare input file\n",
    "!echo \"foo foo quux labs foo bar quux\" > input.txt\n",
    "!hdfs dfs -rm -r input.txt\n",
    "!hdfs dfs -put input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_3.2_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.2_1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for words in csv.reader(sys.stdin, delimiter=' '):\n",
    "    for word in words:\n",
    "        print word + \"\\t1\" \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_3.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_3.2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "wordCount = 0\n",
    "cur = None # the current word\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "\n",
    "# input comes from STDIN\n",
    "for (word, count) in csv.reader(sys.stdin, delimiter='\\t'):\n",
    "    count = int(count)\n",
    "    \n",
    "    # If we have encountered a new word, output the answer of the current word\n",
    "    if cur != word:\n",
    "        if cur is not None:\n",
    "            print \"%s\\t%d\" % (cur, wordCount)\n",
    "            wordCount = 0\n",
    "            \n",
    "    wordCount += count\n",
    "    cur = word\n",
    "\n",
    "\n",
    "# Output for the last word seen\n",
    "if cur is not None:\n",
    "    print \"%s\\t%d\" % (cur, wordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,mapper_called,1\n",
      "reporter:counter:custom,reducer_called,1\n",
      "bar\t1\n",
      "foo\t3\n",
      "labs\t1\n",
      "quux\t2\n"
     ]
    }
   ],
   "source": [
    "!cat input.txt | python mapper_3.2_1.py | sort | python reducer_3.2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:52:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.2-part1\n",
      "16/02/03 10:52:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar1700378626734870731/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob5817016397314229988.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw3.2-part1\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-files mapper_3.2_1.py,reducer_3.2.py \\\n",
    "-mapper mapper_3.2_1.py \\\n",
    "-reducer reducer_3.2.py \\\n",
    "-numReduceTasks 4 \\\n",
    "-input input.txt \\\n",
    "-output hw3.2-part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:52:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "quux\t2\n",
      "foo\t3\n",
      "bar\t1\n",
      "labs\t1\n"
     ]
    }
   ],
   "source": [
    "# Show result\n",
    "!hdfs dfs -cat hw3.2-part1/part-*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Part 1\n",
    "\n",
    "The mapper was called once, because there is only one line of input.  \n",
    "The reducer was called four times, because we specified 4 reducers in the option.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2  - Part 2\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper (re-use last Reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_3.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Word delimiters are space, \\, comma and \"\n",
    "regex = re.compile(r\"[\\s/\\\",]+\")\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for fields in csv.reader(sys.stdin):\n",
    "\n",
    "    # Skip header row\n",
    "    if fields[0] == \"Complaint ID\":\n",
    "        continue\n",
    "    \n",
    "    words = filter(None, regex.split(fields[3]))\n",
    "    for word in words:\n",
    "        print word + \"\\t1\" \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,reducer_called,1\n",
      "reporter:counter:custom,mapper_called,1\n",
      "ATM\t1\n",
      "Account\t1\n",
      "Advertising\t1\n",
      "Balance\t1\n",
      "Communication\t6\n",
      "Cont'd\t19\n",
      "Credit\t8\n",
      "Deposits\t5\n",
      "Disclosure\t7\n",
      "Embezzlement\t1\n",
      "False\t1\n",
      "Fraud\t1\n",
      "Identity\t1\n",
      "Improper\t3\n",
      "Incorrect\t29\n",
      "Loan\t5\n",
      "Managing\t2\n",
      "Problems\t1\n",
      "Taking\t1\n",
      "Unable\t7\n",
      "Using\t1\n",
      "a\t1\n",
      "account\t2\n",
      "and\t6\n",
      "attempts\t19\n",
      "being\t1\n",
      "by\t1\n",
      "card\t1\n",
      "caused\t1\n",
      "closing\t1\n",
      "collect\t19\n",
      "collection\t3\n",
      "company's\t7\n",
      "contact\t2\n",
      "credit\t44\n",
      "debit\t1\n",
      "debt\t26\n",
      "escrow\t2\n",
      "foreclosure\t3\n",
      "funds\t1\n",
      "get\t7\n",
      "identity\t1\n",
      "info\t2\n",
      "information\t29\n",
      "investigation\t7\n",
      "lease\t3\n",
      "loan\t3\n",
      "low\t1\n",
      "management\t1\n",
      "marketing\t1\n",
      "modification\t3\n",
      "monitoring\t1\n",
      "my\t2\n",
      "not\t19\n",
      "of\t10\n",
      "on\t29\n",
      "opening\t1\n",
      "or\t9\n",
      "out\t1\n",
      "owed\t19\n",
      "payments\t2\n",
      "protection\t1\n",
      "report\t37\n",
      "reporting\t7\n",
      "representation\t1\n",
      "score\t7\n",
      "servicing\t2\n",
      "sharing\t2\n",
      "statements\t1\n",
      "tactics\t6\n",
      "the\t3\n",
      "theft\t1\n",
      "to\t7\n",
      "transfer\t1\n",
      "use\t1\n",
      "verification\t7\n",
      "withdrawals\t5\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 Consumer_Complaints.csv | python mapper_3.2.py | \\\n",
    "sort -k1,1 | \\\n",
    "python reducer_3.2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:54:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.2-part2\n",
      "16/02/03 10:54:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar1434959601069990937/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob6662875735736771107.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw3.2-part2\n",
    "\n",
    "\n",
    "# Use 4 mappers and 4 reducers\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-files mapper_3.2.py,reducer_3.2.py \\\n",
    "-D mapreduce.job.name=\"HW3.2-part2\" \\\n",
    "-D mapred.map.tasks=4 \\\n",
    "-numReduceTasks 4 \\\n",
    "-mapper mapper_3.2.py \\\n",
    "-reducer reducer_3.2.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output hw3.2-part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Part 2\n",
    "\n",
    "The mapper was called 4 times.  \n",
    "The reducer was called 4 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.2 - Part 3\n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combiner (Note: Re-use the previous mapper and reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing combiner_3.2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner_3.2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "wordCount = 0\n",
    "curr = None # the current word\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,combiner_called,1\\n')\n",
    "\n",
    "# input comes from STDIN\n",
    "for (word, count) in csv.reader(sys.stdin, delimiter='\\t'):\n",
    "    count = int(count)\n",
    "    \n",
    "    # If we have encountered a new word, output the answer of the current word\n",
    "    if curr != word:\n",
    "        if curr is not None:\n",
    "            print \"%s\\t%d\" % (curr, wordCount)\n",
    "            wordCount = 0\n",
    "            \n",
    "    wordCount += count\n",
    "    curr = word\n",
    "\n",
    "# Output for the last word seen\n",
    "if curr is not None:\n",
    "    print \"%s\\t%d\" % (curr, wordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,mapper_called,1\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "reporter:counter:custom,combiner_called,1\r\n",
      "ATM\t1\r\n",
      "Account\t1\r\n",
      "Advertising\t1\r\n",
      "Balance\t1\r\n",
      "Communication\t6\r\n",
      "Cont'd\t19\r\n",
      "Credit\t8\r\n",
      "Deposits\t5\r\n",
      "Disclosure\t7\r\n",
      "Embezzlement\t1\r\n",
      "False\t1\r\n",
      "Fraud\t1\r\n",
      "Identity\t1\r\n",
      "Improper\t3\r\n",
      "Incorrect\t29\r\n",
      "Loan\t5\r\n",
      "Managing\t2\r\n",
      "Problems\t1\r\n",
      "Taking\t1\r\n",
      "Unable\t7\r\n",
      "Using\t1\r\n",
      "a\t1\r\n",
      "account\t2\r\n",
      "and\t6\r\n",
      "attempts\t19\r\n",
      "being\t1\r\n",
      "by\t1\r\n",
      "card\t1\r\n",
      "caused\t1\r\n",
      "closing\t1\r\n",
      "collect\t19\r\n",
      "collection\t3\r\n",
      "company's\t7\r\n",
      "contact\t2\r\n",
      "credit\t44\r\n",
      "debit\t1\r\n",
      "debt\t26\r\n",
      "escrow\t2\r\n",
      "foreclosure\t3\r\n",
      "funds\t1\r\n",
      "get\t7\r\n",
      "identity\t1\r\n",
      "info\t2\r\n",
      "information\t29\r\n",
      "investigation\t7\r\n",
      "lease\t3\r\n",
      "loan\t3\r\n",
      "low\t1\r\n",
      "management\t1\r\n",
      "marketing\t1\r\n",
      "modification\t3\r\n",
      "monitoring\t1\r\n",
      "my\t2\r\n",
      "not\t19\r\n",
      "of\t10\r\n",
      "on\t29\r\n",
      "opening\t1\r\n",
      "or\t9\r\n",
      "out\t1\r\n",
      "owed\t19\r\n",
      "payments\t2\r\n",
      "protection\t1\r\n",
      "report\t37\r\n",
      "reporting\t7\r\n",
      "representation\t1\r\n",
      "score\t7\r\n",
      "servicing\t2\r\n",
      "sharing\t2\r\n",
      "statements\t1\r\n",
      "tactics\t6\r\n",
      "the\t3\r\n",
      "theft\t1\r\n",
      "to\t7\r\n",
      "transfer\t1\r\n",
      "use\t1\r\n",
      "verification\t7\r\n",
      "withdrawals\t5\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 Consumer_Complaints.csv | python mapper_3.2.py | \\\n",
    "python combiner_3.2.py | sort -k1,1 | \\\n",
    "python reducer_3.2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:55:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.2-part3\n",
      "16/02/03 10:55:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [/var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar6033747649411275576/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob8819446693044245170.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw3.2-part3\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-files mapper_3.2.py,combiner_3.2.py,reducer_3.2.py \\\n",
    "-D mapreduce.job.name=\"HW3.2-part3\" \\\n",
    "-D mapred.map.tasks=4 \\\n",
    "-numReduceTasks 4 \\\n",
    "-mapper mapper_3.2.py \\\n",
    "-combiner combiner_3.2.py \\\n",
    "-reducer reducer_3.2.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output hw3.2-part3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Part 3\n",
    "\n",
    "The mapper was called 4 times.  \n",
    "The combiner was called 16 times.  \n",
    "The reducer was called 4 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 3.2 Part 4\n",
    "\n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_3.2_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.2_4.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    (word, count) = line.strip().split(\"\\t\")\n",
    "\n",
    "    print line.strip()\n",
    "\n",
    "    # Use order inversion so that reducer can get the total word count in a single pass.\n",
    "    # That is for calculating the relative frequenc for each word.\n",
    "    print count + \"\\t\" + str(sys.maxint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_3.2_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_3.2_4.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys, Queue, csv\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "\n",
    "n_max, n_min = 50, 10\n",
    "q_min = Queue.Queue(n_min)\n",
    "a_max = []\n",
    "\n",
    "def updateResult(word, count, freq):\n",
    "    global n_max, q_min, a_max\n",
    "    \n",
    "    rec = [word, count, freq]\n",
    "    # put the biggest\n",
    "    if len(a_max) < n_max:\n",
    "        a_max.append(rec)\n",
    "    \n",
    "    # whatever left is the smallest\n",
    "    if q_min.full():\n",
    "        q_min.get()\n",
    "    q_min.put(rec)\n",
    "    \n",
    "wordCount = 0 # Count of each word\n",
    "totalCount = 0 # Total number of words\n",
    "curr = None # the current word\n",
    "\n",
    "# input comes from STDIN\n",
    "for fields in csv.reader(sys.stdin, delimiter='\\t'):\n",
    "    word = fields[0]\n",
    "    count = fields[1]\n",
    "    count = int(count)\n",
    "\n",
    "    # Find out the total word count.\n",
    "    # We use count == sys.maxint as the special key for order inversion.\n",
    "    if count == sys.maxint:\n",
    "        totalCount += int(word) # The word is the count\n",
    "        continue\n",
    "    \n",
    "    # If we have encountered a new word, output the answer of the current word\n",
    "    if curr != word:\n",
    "        if curr is not None:\n",
    "            updateResult(curr, wordCount, wordCount/totalCount)\n",
    "            wordCount = 0\n",
    "            \n",
    "    wordCount += count\n",
    "    curr = word\n",
    "\n",
    "# Handle the last word seen\n",
    "if curr is not None:\n",
    "    updateResult(curr, wordCount, wordCount/totalCount)\n",
    "    \n",
    "# Output the result\n",
    "print \"Top %d words\" % n_max\n",
    "for rec in a_max:\n",
    "    print \"%s\\t%d\\t%f\" % tuple(rec)\n",
    "    \n",
    "print\n",
    "print \"Bottom %d words\" % n_min\n",
    "while not q_min.empty():\n",
    "    print \"%s\\t%d\\t%f\" % tuple(q_min.get())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:56:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "# Prepare test input\n",
    "!hdfs dfs -cat hw3.2-part3/part-* > hw3.2-part4-input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,mapper_called,1\n",
      "reporter:counter:custom,reducer_called,1\n",
      "Top 50 words\n",
      "Loan\t107254\t0.079547\n",
      "collection\t70487\t0.052278\n",
      "foreclosure\t70487\t0.052278\n",
      "modification\t70487\t0.052278\n",
      "account\t40893\t0.030329\n",
      "or\t40508\t0.030044\n",
      "credit\t40483\t0.030025\n",
      "payments\t39993\t0.029662\n",
      "escrow\t36767\t0.027269\n",
      "servicing\t36767\t0.027269\n",
      "report\t34903\t0.025887\n",
      "Incorrect\t29133\t0.021607\n",
      "information\t29069\t0.021560\n",
      "on\t29069\t0.021560\n",
      "debt\t26531\t0.019677\n",
      "not\t18477\t0.013704\n",
      "Cont'd\t17972\t0.013329\n",
      "attempts\t17972\t0.013329\n",
      "collect\t17972\t0.013329\n",
      "owed\t17972\t0.013329\n",
      "Account\t16555\t0.012278\n",
      "and\t16448\t0.012199\n",
      "closing\t16205\t0.012019\n",
      "management\t16205\t0.012019\n",
      "opening\t16205\t0.012019\n",
      "Credit\t14768\t0.010953\n",
      "of\t13983\t0.010371\n",
      "loan\t12376\t0.009179\n",
      "my\t10731\t0.007959\n",
      "Deposits\t10555\t0.007828\n",
      "withdrawals\t10555\t0.007828\n",
      "Problems\t9484\t0.007034\n",
      "Application\t8868\t0.006577\n",
      "Communication\t8671\t0.006431\n",
      "tactics\t8671\t0.006431\n",
      "broker\t8625\t0.006397\n",
      "mortgage\t8625\t0.006397\n",
      "originator\t8625\t0.006397\n",
      "to\t8401\t0.006231\n",
      "Billing\t8158\t0.006051\n",
      "Other\t7886\t0.005849\n",
      "Disclosure\t7655\t0.005677\n",
      "verification\t7655\t0.005677\n",
      "disputes\t6938\t0.005146\n",
      "reporting\t6559\t0.004865\n",
      "lease\t6337\t0.004700\n",
      "the\t6248\t0.004634\n",
      "being\t5663\t0.004200\n",
      "by\t5663\t0.004200\n",
      "caused\t5663\t0.004200\n",
      "\n",
      "Bottom 10 words\n",
      "received\t98\t0.000073\n",
      "Payment\t92\t0.000068\n",
      "credited\t92\t0.000068\n",
      "Convenience\t75\t0.000056\n",
      "checks\t75\t0.000056\n",
      "amt\t71\t0.000053\n",
      "day\t71\t0.000053\n",
      "wrong\t71\t0.000053\n",
      "disclosures\t64\t0.000047\n",
      "missing\t64\t0.000047\n"
     ]
    }
   ],
   "source": [
    "!cat hw3.2-part4-input.txt | python mapper_3.2_4.py | \\\n",
    "sort -t$'\\t' -k2,2nr -k1,1 | python reducer_3.2_4.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:57:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.2-part4\n",
      "16/02/03 10:57:09 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 10:57:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper_3.2_4.py, reducer_3.2_4.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar3871592559344650025/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob2016275701778847096.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw3.2-part4\n",
    "\n",
    "# For sorting, use the 2nd field count as the primary key, in numeric, reverse order, and\n",
    "# use 1st field as our secondary sort.\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapreduce.job.name=\"HW3.2-part4\" \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-file mapper_3.2_4.py -mapper mapper_3.2_4.py \\\n",
    "-file reducer_3.2_4.py -reducer reducer_3.2_4.py \\\n",
    "-input hw3.2-part3/part-* \\\n",
    "-output hw3.2-part4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:58:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Top 50 words\t\n",
      "Loan\t107254\t0.079547\n",
      "collection\t70487\t0.052278\n",
      "foreclosure\t70487\t0.052278\n",
      "modification\t70487\t0.052278\n",
      "account\t40893\t0.030329\n",
      "or\t40508\t0.030044\n",
      "credit\t40483\t0.030025\n",
      "payments\t39993\t0.029662\n",
      "escrow\t36767\t0.027269\n",
      "servicing\t36767\t0.027269\n",
      "report\t34903\t0.025887\n",
      "Incorrect\t29133\t0.021607\n",
      "information\t29069\t0.021560\n",
      "on\t29069\t0.021560\n",
      "debt\t26531\t0.019677\n",
      "not\t18477\t0.013704\n",
      "Cont'd\t17972\t0.013329\n",
      "attempts\t17972\t0.013329\n",
      "collect\t17972\t0.013329\n",
      "owed\t17972\t0.013329\n",
      "Account\t16555\t0.012278\n",
      "and\t16448\t0.012199\n",
      "closing\t16205\t0.012019\n",
      "management\t16205\t0.012019\n",
      "opening\t16205\t0.012019\n",
      "Credit\t14768\t0.010953\n",
      "of\t13983\t0.010371\n",
      "loan\t12376\t0.009179\n",
      "my\t10731\t0.007959\n",
      "Deposits\t10555\t0.007828\n",
      "withdrawals\t10555\t0.007828\n",
      "Problems\t9484\t0.007034\n",
      "Application\t8868\t0.006577\n",
      "Communication\t8671\t0.006431\n",
      "tactics\t8671\t0.006431\n",
      "broker\t8625\t0.006397\n",
      "mortgage\t8625\t0.006397\n",
      "originator\t8625\t0.006397\n",
      "to\t8401\t0.006231\n",
      "Billing\t8158\t0.006051\n",
      "Other\t7886\t0.005849\n",
      "Disclosure\t7655\t0.005677\n",
      "verification\t7655\t0.005677\n",
      "disputes\t6938\t0.005146\n",
      "reporting\t6559\t0.004865\n",
      "lease\t6337\t0.004700\n",
      "the\t6248\t0.004634\n",
      "being\t5663\t0.004200\n",
      "by\t5663\t0.004200\n",
      "caused\t5663\t0.004200\n",
      "\t\n",
      "Bottom 10 words\t\n",
      "received\t98\t0.000073\n",
      "Payment\t92\t0.000068\n",
      "credited\t92\t0.000068\n",
      "Convenience\t75\t0.000056\n",
      "checks\t75\t0.000056\n",
      "amt\t71\t0.000053\n",
      "day\t71\t0.000053\n",
      "wrong\t71\t0.000053\n",
      "disclosures\t64\t0.000047\n",
      "missing\t64\t0.000047\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat hw3.2-part4/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.2.1 OPTIONAL \n",
    "Using 2 reducers: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First create a job to generate the partition file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_3.2.1_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.2.1_1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# The input is the result from hw3.2-part3, which has the format:\n",
    "# word<tab>count\n",
    "\n",
    "n = 0\n",
    "rate = 5 # sample one out of every five counts\n",
    "samples = []\n",
    "\n",
    "# We want to sample the count\n",
    "for line in sys.stdin:\n",
    "    (word, count) = line.strip().split(\"\\t\")\n",
    "\n",
    "    if n % rate == 0:\n",
    "        samples.append(int(count))\n",
    "        \n",
    "    n += 1\n",
    "    \n",
    "# Now we have a sample of counts.  Let's find the 50% percentile, as we only have 2 reducers.\n",
    "p50 = np.percentile(samples, 50)\n",
    "\n",
    "print p50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 11:54:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "3095.0\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "!hdfs dfs -cat hw3.2-part3/part* | python mapper_3.2.1_1.py > partitions.txt\n",
    "!cat partitions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_3.2.1_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.2.1_2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--partitionFile\", default=\"partitions.txt\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "\n",
    "\n",
    "# The input is the result from hw3.2-part3, which has the format:\n",
    "# word<tab>count\n",
    "\n",
    "# First read the partition file and get the 50th percentile\n",
    "with open(args.partitionFile, 'r') as f:\n",
    "    p50 = float(f.readline())\n",
    "\n",
    "groups = [\"g\" + str(x) for x in range(2)] # names of the two groups\n",
    "\n",
    "for line in sys.stdin:\n",
    "    (word, count) = line.strip().split(\"\\t\")\n",
    "    count = int(count)\n",
    "\n",
    "    # Assign it to different reducers based on the count value\n",
    "    if count >= p50:\n",
    "        group = groups[0]\n",
    "    else:\n",
    "        group = groups[1]\n",
    "        \n",
    "    print group + \"\\t\" + line.strip()\n",
    "\n",
    "    # Use order inversion so that reducer can count the total word count in a single pass\n",
    "    # Need to send it to each group\n",
    "    for g in groups:\n",
    "        print g + \"\\t\" + str(count) + \"\\t\" + str(sys.maxint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 12:24:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "partitions.txt:\n",
      "3095.0\n",
      "\n",
      "reporter:counter:custom,mapper_called,1\n",
      "g0\t1061\t9223372036854775807\n",
      "g0\t1098\t9223372036854775807\n",
      "g0\t1155\t9223372036854775807\n",
      "g0\t1343\t9223372036854775807\n",
      "g0\t139\t9223372036854775807\n",
      "g0\t139\t9223372036854775807\n",
      "g0\t16205\t9223372036854775807\n",
      "g0\t163\t9223372036854775807\n",
      "g0\t16555\t9223372036854775807\n",
      "g0\t17972\t9223372036854775807\n",
      "g0\t1999\t9223372036854775807\n",
      "g0\t240\t9223372036854775807\n",
      "g0\t26531\t9223372036854775807\n",
      "g0\t274\t9223372036854775807\n",
      "g0\t2795\t9223372036854775807\n",
      "g0\t29133\t9223372036854775807\n",
      "g0\t3226\t9223372036854775807\n",
      "g0\t350\t9223372036854775807\n",
      "g0\t3503\t9223372036854775807\n",
      "g0\t40893\t9223372036854775807\n",
      "g0\t4357\t9223372036854775807\n",
      "g0\t4858\t9223372036854775807\n",
      "g0\t5663\t9223372036854775807\n",
      "g0\t5663\t9223372036854775807\n",
      "g0\t640\t9223372036854775807\n",
      "g0\t6938\t9223372036854775807\n",
      "g0\t75\t9223372036854775807\n",
      "g0\t925\t9223372036854775807\n",
      "g0\t929\t9223372036854775807\n",
      "g0\t98\t9223372036854775807\n",
      "g0\taccount\t40893\n",
      "g0\tIncorrect\t29133\n",
      "g0\tdebt\t26531\n",
      "g0\tCont'd\t17972\n",
      "g0\tAccount\t16555\n",
      "g0\tclosing\t16205\n",
      "g0\tdisputes\t6938\n",
      "g0\tby\t5663\n",
      "g0\tcaused\t5663\n",
      "g0\tcompany's\t4858\n",
      "g0\tUnable\t4357\n",
      "g0\ta\t3503\n",
      "g0\tMaking\t3226\n",
      "g1\t1061\t9223372036854775807\n",
      "g1\t1098\t9223372036854775807\n",
      "g1\t1155\t9223372036854775807\n",
      "g1\t1343\t9223372036854775807\n",
      "g1\t139\t9223372036854775807\n",
      "g1\t139\t9223372036854775807\n",
      "g1\t16205\t9223372036854775807\n",
      "g1\t163\t9223372036854775807\n",
      "g1\t16555\t9223372036854775807\n",
      "g1\t17972\t9223372036854775807\n",
      "g1\t1999\t9223372036854775807\n",
      "g1\t240\t9223372036854775807\n",
      "g1\t26531\t9223372036854775807\n",
      "g1\t274\t9223372036854775807\n",
      "g1\t2795\t9223372036854775807\n",
      "g1\t29133\t9223372036854775807\n",
      "g1\t3226\t9223372036854775807\n",
      "g1\t350\t9223372036854775807\n",
      "g1\t3503\t9223372036854775807\n",
      "g1\t40893\t9223372036854775807\n",
      "g1\t4357\t9223372036854775807\n",
      "g1\t4858\t9223372036854775807\n",
      "g1\t5663\t9223372036854775807\n",
      "g1\t5663\t9223372036854775807\n",
      "g1\t640\t9223372036854775807\n",
      "g1\t6938\t9223372036854775807\n",
      "g1\t75\t9223372036854775807\n",
      "g1\t925\t9223372036854775807\n",
      "g1\t929\t9223372036854775807\n",
      "g1\t98\t9223372036854775807\n",
      "g1\tClosing\t2795\n",
      "g1\tCan't\t1999\n",
      "g1\tDebt\t1343\n",
      "g1\tPayoff\t1155\n",
      "g1\tissue\t1098\n",
      "g1\tDelinquent\t1061\n",
      "g1\tfor\t929\n",
      "g1\tI\t925\n",
      "g1\tissuance\t640\n",
      "g1\tWorkout\t350\n",
      "g1\tavailable\t274\n",
      "g1\tCash\t240\n",
      "g1\tacct\t163\n",
      "g1\tApplied\t139\n",
      "g1\tSale\t139\n",
      "g1\tWrong\t98\n",
      "g1\tchecks\t75\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "!hdfs dfs -cat hw3.2-part3/part* > t\n",
    "!cat t | python mapper_3.2.1_1.py > partitions.txt\n",
    "\n",
    "print \"partitions.txt:\"\n",
    "!cat partitions.txt\n",
    "print\n",
    "\n",
    "!head -n 30 t | \\\n",
    "python mapper_3.2.1_2.py | \\\n",
    "sort -k1,1 -k3,3nr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_3.2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_3.2.1.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys, Queue, csv\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "    \n",
    "wordCount = 0 # Count of each word\n",
    "totalCount = 0 # Total number of words\n",
    "curr = None # the current word\n",
    "\n",
    "# input format: \n",
    "# Total count: group \\t count \\t max.int\n",
    "# Word: group \\t word \\t count\n",
    "for fields in csv.reader(sys.stdin, delimiter='\\t'):\n",
    "    group = fields[0]\n",
    "    word = fields[1]\n",
    "    count = fields[2]\n",
    "    count = int(count)\n",
    "\n",
    "    # Find out the total word count.\n",
    "    # We use count == sys.maxint as the special key for order inversion.\n",
    "    if count == sys.maxint:\n",
    "        totalCount += int(word) # The word is the count\n",
    "        continue\n",
    "    \n",
    "    # If we have encountered a new word, output the answer of the current word\n",
    "    if curr != word:\n",
    "        if curr is not None:\n",
    "            print \"\\t\".join([group, curr, str(wordCount), str(wordCount/totalCount)])\n",
    "            wordCount = 0\n",
    "            \n",
    "    wordCount += count\n",
    "    curr = word\n",
    "\n",
    "# Handle the last word seen\n",
    "if curr is not None:\n",
    "    print \"\\t\".join([group, curr, str(wordCount), str(wordCount/totalCount)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 14:04:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "partitions.txt:\n",
      "3095.0\n",
      "\n",
      "reporter:counter:custom,reducer_called,1\n",
      "reporter:counter:custom,mapper_called,1\n",
      "g0\taccount\t40893\t0.209793761543\n",
      "g0\tIncorrect\t29133\t0.149461317464\n",
      "g0\tdebt\t26531\t0.13611225118\n",
      "g0\tCont'd\t17972\t0.0922019289965\n",
      "g0\tAccount\t16555\t0.0849322799097\n",
      "g0\tclosing\t16205\t0.083136671455\n",
      "g0\tdisputes\t6938\t0.035594089883\n",
      "g0\tby\t5663\t0.0290529447979\n",
      "g0\tcaused\t5663\t0.0290529447979\n",
      "g0\tcompany's\t4858\t0.0249230453519\n",
      "g0\tUnable\t4357\t0.0223527601067\n",
      "g0\ta\t3503\t0.0179714754771\n",
      "g1\tMaking\t3226\t0.00827518982147\n",
      "g1\tClosing\t2795\t0.00716960804433\n",
      "g1\tCan't\t1999\t0.00512774471578\n",
      "g1\tDebt\t1343\t0.00344500307819\n",
      "g1\tPayoff\t1155\t0.00296275395034\n",
      "g1\tissue\t1098\t0.00281654011902\n",
      "g1\tDelinquent\t1061\t0.00272162938641\n",
      "g1\tfor\t929\t0.00238302893495\n",
      "g1\tI\t925\t0.00237276831521\n",
      "g1\tissuance\t640\t0.00164169915863\n",
      "g1\tWorkout\t350\t0.000897804227375\n",
      "g1\tavailable\t274\t0.000702852452288\n",
      "g1\tCash\t240\t0.000615637184486\n",
      "g1\tacct\t163\t0.000418120254463\n",
      "g1\tApplied\t139\t0.000356556536015\n",
      "g1\tSale\t139\t0.000356556536015\n",
      "g1\tWrong\t98\t0.000251385183665\n",
      "g1\tchecks\t75\t0.000192386620152\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "!hdfs dfs -cat hw3.2-part3/part* > t\n",
    "!cat t | python mapper_3.2.1_1.py > partitions.txt\n",
    "\n",
    "print \"partitions.txt:\"\n",
    "!cat partitions.txt\n",
    "print\n",
    "\n",
    "!head -n 30 t | \\\n",
    "python mapper_3.2.1_2.py | \\\n",
    "sort -k1,1 -k3,3nr | \\\n",
    "python reducer_3.2.1.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 12:28:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `hw3.2.1-part1': No such file or directory\n",
      "reporter:counter:custom,reducer_called,1\n",
      "reporter:counter:custom,mapper_called,1\n",
      "account\t40893\t0.209793761543\n",
      "Incorrect\t29133\t0.149461317464\n",
      "debt\t26531\t0.13611225118\n",
      "Cont'd\t17972\t0.0922019289965\n",
      "Account\t16555\t0.0849322799097\n",
      "closing\t16205\t0.083136671455\n",
      "disputes\t6938\t0.035594089883\n",
      "by\t5663\t0.0290529447979\n",
      "caused\t5663\t0.0290529447979\n",
      "company's\t4858\t0.0249230453519\n",
      "Unable\t4357\t0.0223527601067\n",
      "a\t3503\t0.0179714754771\n",
      "Making\t3226\t0.00827518982147\n",
      "Closing\t2795\t0.00716960804433\n",
      "Can't\t1999\t0.00512774471578\n",
      "Debt\t1343\t0.00344500307819\n",
      "Payoff\t1155\t0.00296275395034\n",
      "issue\t1098\t0.00281654011902\n",
      "Delinquent\t1061\t0.00272162938641\n",
      "for\t929\t0.00238302893495\n",
      "I\t925\t0.00237276831521\n",
      "issuance\t640\t0.00164169915863\n",
      "Workout\t350\t0.000897804227375\n",
      "available\t274\t0.000702852452288\n",
      "Cash\t240\t0.000615637184486\n",
      "acct\t163\t0.000418120254463\n",
      "Applied\t139\t0.000356556536015\n",
      "Sale\t139\t0.000356556536015\n",
      "Wrong\t98\t0.000251385183665\n",
      "checks\t75\t0.000192386620152\n",
      "16/02/03 12:29:01 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 12:29:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper_3.2.1_1.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar8037161275481417824/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob439373068655075367.jar tmpDir=null\n",
      "16/02/03 12:29:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# First job - generate partitions file\n",
    "!hdfs dfs -rm -r hw3.2.1-part1\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapreduce.job.name=\"hw3.2.1-part1\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=0 \\\n",
    "-file mapper_3.2.1_1.py -mapper mapper_3.2.1_1.py \\\n",
    "-input hw3.2-part3/part* \\\n",
    "-output hw3.2.1-part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 14:25:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.2.1-part2\n",
      "16/02/03 14:25:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 14:25:18 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper_3.2.1_2.py, reducer_3.2.1.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar2352388023510399363/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob5675893172764108239.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw3.2.1-part2\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-files \"hdfs://localhost:8020/user/patrickng/hw3.2.1-part1/part-00000#partitions.txt\" \\\n",
    "-D mapreduce.job.name=\"hw3.2.1-part2\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k3,3nr -k2,2\" \\\n",
    "-file mapper_3.2.1_2.py -mapper mapper_3.2.1_2.py \\\n",
    "-file reducer_3.2.1.py -reducer reducer_3.2.1.py \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-input hw3.2-part3/part* \\\n",
    "-output hw3.2.1-part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.3. \n",
    "\n",
    "Shopping Cart Analysis  \n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.  \n",
    "\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session.   \n",
    "The items are separated by spaces.\n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    items = line.strip().split()\n",
    "    \n",
    "    # Use order inversion to help calculate the largest basket\n",
    "    print \"#\\t\" + str(len(items)) # Use # as the special key\n",
    "\n",
    "    for item in items:\n",
    "        print item + \"\\t1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys\n",
    "\n",
    "largestBasket = None # Largest basket seen\n",
    "uniqueCount = 0 # Number of unqiue items\n",
    "itemCount = 0 # Item count\n",
    "curr = None # The current item\n",
    "        \n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    (item, count) = line.strip().split('\\t')\n",
    "    count = int(count)\n",
    "\n",
    "    # Find out the largest basket size.  The first set of input is for largest basket.\n",
    "    if largestBasket is None:\n",
    "        largestBasket = count\n",
    "        \n",
    "    # Skip all the largest basket records\n",
    "    if item == \"#\":\n",
    "        continue\n",
    "            \n",
    "    # If we have encountered a new item, output the count of the last item\n",
    "    if curr is not None and curr != item:\n",
    "        print \"%d\\t%s\" % (itemCount, curr)        \n",
    "        # Use order inversion to calculate the total number of items\n",
    "        print \"%d\\t%d\" % (sys.maxint, itemCount)\n",
    "        # Increase the number of unique items\n",
    "        uniqueCount += 1       \n",
    "        itemCount = 0\n",
    "            \n",
    "    itemCount += count\n",
    "    curr = item\n",
    "\n",
    "# Handle the last item seen\n",
    "if curr is not None:\n",
    "    print \"%d\\t%s\" % (itemCount, curr)        \n",
    "    # Use order inversion to calculate the total number of items\n",
    "    print \"%d\\t%d\" % (sys.maxint, itemCount)\n",
    "    # Increase the number of unique items\n",
    "    uniqueCount += 1       \n",
    "    itemCount = 0\n",
    "    \n",
    "# Report the \"Largest Basket\" and \"Number of unique items\"\n",
    "print \"%d\\t%d\" % (sys.maxint-1, largestBasket)\n",
    "print \"%d\\t%d\" % (sys.maxint-2, uniqueCount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys\n",
    "\n",
    "totalCount = 0 # Total number of items\n",
    "itemPrinted = 0\n",
    "itemCount = 0\n",
    "curr = None # The current item\n",
    "        \n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    (count, item) = line.strip().split('\\t')\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except:\n",
    "        print \"Unexpected error:\", sys.exc_info()[0]\n",
    "        continue\n",
    "    \n",
    "    # The first part is for calculating the total number of items\n",
    "    if count == sys.maxint:\n",
    "        totalCount += int(item)\n",
    "        continue\n",
    "        \n",
    "    if count == sys.maxint - 1:\n",
    "        print \"Largest basket size:\", item\n",
    "        print\n",
    "        continue\n",
    "        \n",
    "    if count == sys.maxint - 2:\n",
    "        print \"Number of unique items:\", item\n",
    "        print\n",
    "        continue\n",
    "\n",
    "    # If we have encountered a new item, output the count of the current item\n",
    "    if curr is not None and curr != item:\n",
    "        if itemPrinted < 50:\n",
    "            print \"%s\\t%d\\t%f\" % (curr, itemCount, itemCount/totalCount) \n",
    "            itemPrinted += 1\n",
    "            \n",
    "        itemCount = 0\n",
    "    \n",
    "    itemCount += count\n",
    "    curr = item\n",
    "\n",
    "    \n",
    "# Handle the last item seen\n",
    "if itemPrinted < 50 and curr is not None:\n",
    "    print \"%s\\t%d\\t%f\" % (curr, itemCount, itemCount/totalCount)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest basket size: 12\r\n",
      "\r\n",
      "Number of unique items: 22\r\n",
      "\r\n",
      "ELE17451\t4\t0.142857\r\n",
      "FRO86643\t2\t0.071429\r\n",
      "GRO99222\t2\t0.071429\r\n",
      "SNA11465\t2\t0.071429\r\n",
      "DAI22896\t1\t0.035714\r\n",
      "ELE23393\t1\t0.035714\r\n",
      "ELE26917\t1\t0.035714\r\n",
      "ELE37798\t1\t0.035714\r\n",
      "ELE52966\t1\t0.035714\r\n",
      "ELE89019\t1\t0.035714\r\n",
      "ELE91550\t1\t0.035714\r\n",
      "FRO11987\t1\t0.035714\r\n",
      "FRO12685\t1\t0.035714\r\n",
      "FRO84225\t1\t0.035714\r\n",
      "FRO90334\t1\t0.035714\r\n",
      "GRO12298\t1\t0.035714\r\n",
      "GRO56989\t1\t0.035714\r\n",
      "GRO73461\t1\t0.035714\r\n",
      "SNA30755\t1\t0.035714\r\n",
      "SNA80192\t1\t0.035714\r\n",
      "SNA90258\t1\t0.035714\r\n",
      "SNA99873\t1\t0.035714\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 ProductPurchaseData.txt | \\\n",
    "python mapper1.py | \\\n",
    "sort -t$'\\t' -k1,1 -k2,2nr | \\\n",
    "python reducer1.py | \\\n",
    "/bin/cat | \\\n",
    "sort -t$'\\t' -k1,1nr -k2,2 | \\\n",
    "python reducer2.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 23:09:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 23:09:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# upload input file to hdfs\n",
    "!hdfs dfs -rm -f ProductPurchaseData.txt\n",
    "!hdfs dfs -put ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 16:18:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.3-part1\n",
      "16/02/02 16:18:33 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/02 16:18:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper1.py, reducer1.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar6220574856817021032/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob7573617829780053436.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# First job\n",
    "!hdfs dfs -rm -r hw3.3-part1\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapred.map.tasks=2 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k1,1 -k2,2nr\" \\\n",
    "-file mapper1.py -mapper mapper1.py \\\n",
    "-file reducer1.py -reducer reducer1.py \\\n",
    "-input ProductPurchaseData.txt \\\n",
    "-output hw3.3-part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 16:19:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.3-part2\n",
      "16/02/02 16:19:22 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/02 16:19:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [reducer2.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar7447411094773386909/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob2637008777364281477.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Second job, using the result from first job as input.\n",
    "# We use /bin/cat as the Identity Mapper\n",
    "!hdfs dfs -rm -r hw3.3-part2\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k1,1nr -k2,2\" \\\n",
    "-mapper /bin/cat \\\n",
    "-file reducer2.py -reducer reducer2.py \\\n",
    "-input hw3.3-part1/part-* \\\n",
    "-output hw3.3-part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/02 16:20:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Largest basket size: 37\t\n",
      "\t\n",
      "Number of unique items: 12592\t\n",
      "\t\n",
      "DAI62779\t6667\t0.017507\n",
      "FRO40251\t3881\t0.010191\n",
      "ELE17451\t3875\t0.010175\n",
      "GRO73461\t3602\t0.009458\n",
      "SNA80324\t3044\t0.007993\n",
      "ELE32164\t2851\t0.007486\n",
      "DAI75645\t2736\t0.007184\n",
      "SNA45677\t2455\t0.006447\n",
      "FRO31317\t2330\t0.006118\n",
      "DAI85309\t2293\t0.006021\n",
      "ELE26917\t2292\t0.006019\n",
      "FRO80039\t2233\t0.005864\n",
      "GRO21487\t2115\t0.005554\n",
      "SNA99873\t2083\t0.005470\n",
      "GRO59710\t2004\t0.005262\n",
      "GRO71621\t1920\t0.005042\n",
      "FRO85978\t1918\t0.005036\n",
      "GRO30386\t1840\t0.004832\n",
      "ELE74009\t1816\t0.004769\n",
      "GRO56726\t1784\t0.004685\n",
      "DAI63921\t1773\t0.004656\n",
      "GRO46854\t1756\t0.004611\n",
      "ELE66600\t1713\t0.004498\n",
      "DAI83733\t1712\t0.004496\n",
      "FRO32293\t1702\t0.004469\n",
      "ELE66810\t1697\t0.004456\n",
      "SNA55762\t1646\t0.004322\n",
      "DAI22177\t1627\t0.004272\n",
      "FRO78087\t1531\t0.004020\n",
      "ELE99737\t1516\t0.003981\n",
      "ELE34057\t1489\t0.003910\n",
      "GRO94758\t1489\t0.003910\n",
      "FRO35904\t1436\t0.003771\n",
      "FRO53271\t1420\t0.003729\n",
      "SNA93860\t1407\t0.003695\n",
      "SNA90094\t1390\t0.003650\n",
      "GRO38814\t1352\t0.003550\n",
      "ELE56788\t1345\t0.003532\n",
      "GRO61133\t1321\t0.003469\n",
      "DAI88807\t1316\t0.003456\n",
      "ELE74482\t1316\t0.003456\n",
      "ELE59935\t1311\t0.003443\n",
      "SNA96271\t1295\t0.003401\n",
      "DAI43223\t1290\t0.003387\n",
      "ELE91337\t1289\t0.003385\n",
      "GRO15017\t1275\t0.003348\n",
      "DAI31081\t1261\t0.003311\n",
      "GRO81087\t1220\t0.003204\n",
      "DAI22896\t1219\t0.003201\n",
      "GRO85051\t1214\t0.003188\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat hw3.3-part2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.4. Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (the number of records where they coccur/the number of baskets in the dataset) in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3) to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer (part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_3.4_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.4_1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    items = line.strip().split()\n",
    "    \n",
    "    # Use order inversion to help calculate the total # of baskets\n",
    "    print \"#\\t\\t1\" # Use # as the special key\n",
    "    \n",
    "    # Output all co-occuring pairs.\n",
    "    for subset in itertools.combinations(sorted(set(items)), 2):\n",
    "        print \"%s\\t%s\\t1\" % (subset[0], subset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_3.4_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_3.4_1.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--threshold\", type=int, default=100)\n",
    "args = parser.parse_args()\n",
    "supportThreshold = args.threshold\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "\n",
    "supportCount = 0 # Support count of each pair\n",
    "totalCount = 0 # Total number of baskets\n",
    "lastPair = None # The pair previously seen\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    (item1, item2, count) = line.strip().split('\\t')\n",
    "    count = int(count)\n",
    "\n",
    "    # Find out the total basket count.\n",
    "    # We use # as the special key for order inversion.\n",
    "    if item1 == \"#\":\n",
    "        totalCount += count\n",
    "        continue\n",
    "        \n",
    "    pair = [item1, item2]\n",
    "    \n",
    "    # If we have encountered a new pair, output the count of the last pair\n",
    "    if lastPair and lastPair != pair:\n",
    "        if supportCount >= supportThreshold:\n",
    "            print \"%s\\t%s\\t%d\\t%f\" % (lastPair[0], lastPair[1], supportCount, supportCount / totalCount)\n",
    "        supportCount = 0\n",
    "            \n",
    "    supportCount += count\n",
    "    lastPair = pair\n",
    "\n",
    "# Handle the last pair seen\n",
    "if lastPair is not None:\n",
    "    if supportCount >= supportThreshold:\n",
    "        print \"%s\\t%s\\t%d\\t%f\" % (lastPair[0], lastPair[1], supportCount, supportCount / totalCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper and Reducer (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_3.4_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.4_2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    (item1, item2, count, support) = line.strip().split(\"\\t\")\n",
    "    \n",
    "    # Output the count as the first key field, so that we can use\n",
    "    # Hadoop to sort it in reverse order.\n",
    "    print \"\\t\".join([count, item1, item2, support])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_3.4_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_3.4_2.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys, Queue\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "\n",
    "n_max = 50\n",
    "a_max = []\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    if len(a_max) < n_max:\n",
    "        a_max.append(line)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Output the result\n",
    "print \"Top %d pairs\" % n_max\n",
    "for line in a_max:\n",
    "    rec = line.strip().split('\\t')\n",
    "    print \",\".join(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,mapper_called,1\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "reporter:counter:custom,reducer_called,1\r\n",
      "Top 50 pairs\r\n",
      "ELE17451,GRO99222,2,1.000000\r\n",
      "ELE17451,ELE26917,1,0.500000\r\n",
      "ELE17451,ELE52966,1,0.500000\r\n",
      "ELE17451,ELE89019,1,0.500000\r\n",
      "ELE17451,ELE91550,1,0.500000\r\n",
      "ELE17451,FRO11987,1,0.500000\r\n",
      "ELE17451,FRO12685,1,0.500000\r\n",
      "ELE17451,FRO84225,1,0.500000\r\n",
      "ELE17451,FRO90334,1,0.500000\r\n",
      "ELE17451,GRO12298,1,0.500000\r\n",
      "ELE17451,SNA11465,1,0.500000\r\n",
      "ELE17451,SNA30755,1,0.500000\r\n",
      "ELE17451,SNA80192,1,0.500000\r\n",
      "ELE17451,SNA90258,1,0.500000\r\n",
      "ELE26917,ELE52966,1,0.500000\r\n",
      "ELE26917,ELE91550,1,0.500000\r\n",
      "ELE26917,FRO12685,1,0.500000\r\n",
      "ELE26917,FRO84225,1,0.500000\r\n",
      "ELE26917,FRO90334,1,0.500000\r\n",
      "ELE26917,GRO12298,1,0.500000\r\n",
      "ELE26917,GRO99222,1,0.500000\r\n",
      "ELE26917,SNA11465,1,0.500000\r\n",
      "ELE26917,SNA30755,1,0.500000\r\n",
      "ELE26917,SNA80192,1,0.500000\r\n",
      "ELE52966,ELE91550,1,0.500000\r\n",
      "ELE52966,FRO12685,1,0.500000\r\n",
      "ELE52966,FRO84225,1,0.500000\r\n",
      "ELE52966,FRO90334,1,0.500000\r\n",
      "ELE52966,GRO12298,1,0.500000\r\n",
      "ELE52966,GRO99222,1,0.500000\r\n",
      "ELE52966,SNA11465,1,0.500000\r\n",
      "ELE52966,SNA30755,1,0.500000\r\n",
      "ELE52966,SNA80192,1,0.500000\r\n",
      "ELE89019,FRO11987,1,0.500000\r\n",
      "ELE89019,GRO99222,1,0.500000\r\n",
      "ELE89019,SNA90258,1,0.500000\r\n",
      "ELE91550,FRO12685,1,0.500000\r\n",
      "ELE91550,FRO84225,1,0.500000\r\n",
      "ELE91550,FRO90334,1,0.500000\r\n",
      "ELE91550,GRO12298,1,0.500000\r\n",
      "ELE91550,GRO99222,1,0.500000\r\n",
      "ELE91550,SNA11465,1,0.500000\r\n",
      "ELE91550,SNA30755,1,0.500000\r\n",
      "ELE91550,SNA80192,1,0.500000\r\n",
      "FRO11987,GRO99222,1,0.500000\r\n",
      "FRO11987,SNA90258,1,0.500000\r\n",
      "FRO12685,FRO84225,1,0.500000\r\n",
      "FRO12685,FRO90334,1,0.500000\r\n",
      "FRO12685,GRO12298,1,0.500000\r\n",
      "FRO12685,GRO99222,1,0.500000\r\n"
     ]
    }
   ],
   "source": [
    "# Note: have to change threshold to 1\n",
    "!head -n 2 ProductPurchaseData.txt | \\\n",
    "python mapper_3.4_1.py | \\\n",
    "sort -t$'\\t' -k1,1 | \\\n",
    "python reducer_3.4_1.py --threshold 1 | \\\n",
    "/bin/cat | \\\n",
    "sort -t$'\\t' -k3,3rn -k1,2 | \\\n",
    "python reducer_3.4_2.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:13:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.4-part1\n",
      "16/02/03 10:13:31 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 10:13:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper1.py, reducer1.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar7921158822365522625/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob2250619433173984701.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "!hdfs dfs -rm -r hw3.4-part1\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapreduce.job.name='HW3.4-part1' \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k1,1 -k2,2\" \\\n",
    "-file mapper1.py -mapper mapper1.py \\\n",
    "-file reducer1.py -reducer reducer1.py \\\n",
    "-input ProductPurchaseData.txt \\\n",
    "-output hw3.4-part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:14:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.4-part2\n",
      "16/02/03 10:14:52 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 10:14:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [reducer2.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar8293456977848402676/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob2176540031542185349.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "\n",
    "!hdfs dfs -rm -r hw3.4-part2\n",
    "\n",
    "# For sorting, use the 1st field (count) as the primary key, in reverse order.\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapreduce.job.name='HW3.4-part2' \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k3,3rn -k1,2\" \\\n",
    "-mapper /bin/cat \\\n",
    "-file reducer2.py -reducer reducer2.py \\\n",
    "-input hw3.4-part1/part-* \\\n",
    "-output hw3.4-part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:15:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Top 50 pairs\t\n",
      "DAI62779,ELE17451,1592,0.051188\t\n",
      "FRO40251,SNA80324,1412,0.045400\t\n",
      "DAI75645,FRO40251,1254,0.040320\t\n",
      "FRO40251,GRO85051,1213,0.039002\t\n",
      "DAI62779,GRO73461,1139,0.036623\t\n",
      "DAI75645,SNA80324,1130,0.036333\t\n",
      "DAI62779,FRO40251,1070,0.034404\t\n",
      "DAI62779,SNA80324,923,0.029678\t\n",
      "DAI62779,DAI85309,918,0.029517\t\n",
      "ELE32164,GRO59710,911,0.029292\t\n",
      "DAI62779,DAI75645,882,0.028359\t\n",
      "FRO40251,GRO73461,882,0.028359\t\n",
      "DAI62779,ELE92920,877,0.028198\t\n",
      "FRO40251,FRO92469,835,0.026848\t\n",
      "DAI62779,ELE32164,832,0.026752\t\n",
      "DAI75645,GRO73461,712,0.022893\t\n",
      "DAI43223,ELE32164,711,0.022861\t\n",
      "DAI62779,GRO30386,709,0.022797\t\n",
      "ELE17451,FRO40251,697,0.022411\t\n",
      "DAI85309,ELE99737,659,0.021189\t\n",
      "DAI62779,ELE26917,650,0.020900\t\n",
      "GRO21487,GRO73461,631,0.020289\t\n",
      "DAI62779,SNA45677,604,0.019421\t\n",
      "ELE17451,SNA80324,597,0.019196\t\n",
      "DAI62779,GRO71621,595,0.019131\t\n",
      "DAI62779,SNA55762,593,0.019067\t\n",
      "DAI62779,DAI83733,586,0.018842\t\n",
      "ELE17451,GRO73461,580,0.018649\t\n",
      "GRO73461,SNA80324,562,0.018070\t\n",
      "DAI62779,GRO59710,561,0.018038\t\n",
      "DAI62779,FRO80039,550,0.017684\t\n",
      "DAI75645,ELE17451,547,0.017588\t\n",
      "DAI62779,SNA93860,537,0.017266\t\n",
      "DAI55148,DAI62779,526,0.016913\t\n",
      "DAI43223,GRO59710,512,0.016462\t\n",
      "ELE17451,ELE32164,511,0.016430\t\n",
      "DAI62779,SNA18336,506,0.016270\t\n",
      "ELE32164,GRO73461,486,0.015627\t\n",
      "DAI62779,FRO78087,482,0.015498\t\n",
      "DAI85309,ELE17451,482,0.015498\t\n",
      "DAI62779,GRO94758,479,0.015401\t\n",
      "DAI62779,GRO21487,471,0.015144\t\n",
      "GRO85051,SNA80324,471,0.015144\t\n",
      "ELE17451,GRO30386,468,0.015048\t\n",
      "FRO85978,SNA95666,463,0.014887\t\n",
      "DAI62779,FRO19221,462,0.014855\t\n",
      "DAI62779,GRO46854,461,0.014823\t\n",
      "DAI43223,DAI62779,459,0.014758\t\n",
      "ELE92920,SNA18336,455,0.014630\t\n",
      "DAI88079,FRO40251,446,0.014340\t\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "!hdfs dfs -cat hw3.4-part2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Report \n",
    "\n",
    "Compute time: 46 + 16 = 62 sec   \n",
    "Setup: Quad Core, OS X, 3 mappers, 1 reducer  \n",
    "Mapper called: 3 + 2 = 5 times  \n",
    "Reducer called: 1 + 1 = 2 times  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_3.5_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.5_1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    items = line.strip().split()\n",
    "    \n",
    "    # Use order inversion to help calculate the total # of baskets\n",
    "    print \"#\\t1\" # Use # as the special key\n",
    "    \n",
    "    # First ignore the corner case\n",
    "    if len(items) <= 1:\n",
    "        continue\n",
    "        \n",
    "    items = sorted(items) # Sort it because we will list pairs info in sorted order\n",
    "        \n",
    "    for i in range(len(items) - 1):\n",
    "        cooccur = OrderedDict()  \n",
    "        x = items[i]\n",
    "        for j in range(i+1, len(items)):\n",
    "            y = items[j]\n",
    "            if x == y:\n",
    "                continue # skip duplicate items\n",
    "            \n",
    "            if y in cooccur:\n",
    "                cooccur[y] += 1\n",
    "            else:\n",
    "                cooccur[y] = 1\n",
    "        \n",
    "        # Output the coocurr info about x\n",
    "        stripe = \",\".join([k+\",\"+str(v) for k,v in cooccur.items()])\n",
    "        print \"%s\\t%s\" % (x, stripe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_3.5_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_3.5_1.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys\n",
    "from collections import Counter\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--threshold\", type=int, default=100)\n",
    "args = parser.parse_args()\n",
    "supportThreshold = args.threshold\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "\n",
    "stripeCombined = Counter() # Stripe info about the current item\n",
    "totalCount = 0 # Total number of baskets\n",
    "curr = None # The current item\n",
    "\n",
    "# Read the stripe input and convert it into a dict\n",
    "def readStripe(data):\n",
    "    stripe = Counter()\n",
    "    \n",
    "    # E.g. SNA90258,1,ELE17451,1,ELE89019,1,GRO99222,1\n",
    "    items = data.split(',')\n",
    "    for i in range(0, len(items), 2):\n",
    "        stripe[items[i]] = int(items[i+1])\n",
    "    return stripe\n",
    "\n",
    "def outputStripe(item, stripe):\n",
    "    # only keep those which pass the threshold\n",
    "    filtered = {k:v for k,v in stripe.items() if v >= supportThreshold}\n",
    "    \n",
    "    # Output if we have at least one pair which has met the threshold\n",
    "    if len(filtered) > 0:\n",
    "        print \"%s\\t%s\" % (item, \",\".join([\":\".join([k, str(v), str(v/totalCount)]) \n",
    "                             for k,v in filtered.items()]))\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    (item, data) = line.strip().split('\\t')\n",
    "\n",
    "    # Find out the total basket count.\n",
    "    # We use # as the special key for order inversion.\n",
    "    if item == \"#\":\n",
    "        totalCount += int(data)\n",
    "        continue\n",
    "    \n",
    "    # If we have encountered a new item, output the stripe info of the last item\n",
    "    if curr and curr != item:\n",
    "        outputStripe(curr, stripeCombined)\n",
    "        stripeCombined.clear()\n",
    "            \n",
    "    # Merge the stripe info of this input into stripeCombined\n",
    "    stripeCombined = stripeCombined + readStripe(data)\n",
    "    \n",
    "    curr = item\n",
    "\n",
    "# Handle the last pair seen\n",
    "if curr is not None:\n",
    "    outputStripe(curr, stripeCombined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test for mapper/reducer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,mapper_called,1\n",
      "reporter:counter:custom,reducer_called,1\n",
      "DAI14125\tFRO18919:1:0.1,SNA90258:1:0.1,ELE11375:1:0.1,ELE17451:1:0.1,FRO78087:1:0.1,ELE28573:1:0.1,SNA69641:1:0.1\n",
      "DAI22177\tDAI91535:1:0.1,DAI49199:1:0.1,FRO18919:1:0.1,SNA91554:1:0.1,SNA80192:2:0.2,SNA85662:2:0.2,ELE17451:2:0.2,FRO81176:1:0.1,GRO94758:1:0.1,DAI22896:1:0.1,ELE23393:1:0.1,SNA90258:1:0.1,ELE66810:1:0.1,ELE94711:1:0.1,DAI46755:1:0.1,ELE59935:1:0.1,GRO73461:1:0.1\n",
      "DAI22896\tSNA99873:2:0.2,DAI91535:1:0.1,DAI49199:1:0.1,FRO18919:1:0.1,DAI50921:1:0.1,SNA80192:2:0.2,SNA85662:1:0.1,ELE17451:3:0.3,FRO81176:1:0.1,FRO86643:1:0.1,SNA90258:1:0.1,ELE66810:1:0.1,ELE94711:1:0.1,DAI46755:1:0.1,GRO94758:1:0.1,GRO75578:1:0.1,GRO73461:3:0.3\n",
      "DAI46755\tDAI91535:1:0.1,DAI49199:1:0.1,SNA80192:1:0.1,SNA85662:1:0.1,ELE17451:1:0.1,FRO81176:1:0.1,SNA90258:1:0.1,ELE66810:1:0.1,ELE94711:1:0.1,GRO94758:1:0.1,GRO73461:1:0.1\n",
      "DAI49199\tDAI91535:1:0.1,SNA80192:1:0.1,SNA85662:1:0.1,ELE17451:1:0.1,FRO81176:1:0.1,SNA90258:1:0.1,ELE66810:1:0.1,ELE94711:1:0.1,GRO94758:1:0.1,GRO73461:1:0.1\n",
      "DAI50921\tSNA99873:1:0.1,FRO18919:1:0.1,SNA80192:1:0.1,ELE17451:1:0.1,GRO75578:1:0.1,GRO73461:1:0.1\n",
      "DAI54444\tFRO86643:1:0.1,GRO39357:1:0.1,ELE11375:1:0.1,ELE17451:1:0.1,SNA11465:1:0.1,FRO78087:1:0.1,ELE28573:1:0.1,SNA69641:1:0.1\n",
      "DAI54690\tDAI91535:1:0.1,GRO56989:1:0.1,SNA80192:1:0.1,ELE17451:1:0.1,FRO81176:1:0.1,FRO76833:1:0.1,SNA69641:1:0.1,GRO99222:1:0.1,GRO94758:1:0.1,ELE37798:1:0.1\n",
      "DAI91535\tGRO56989:1:0.1,SNA80192:2:0.2,SNA85662:1:0.1,ELE17451:2:0.2,FRO81176:2:0.2,SNA90258:1:0.1,FRO76833:1:0.1,ELE66810:1:0.1,ELE37798:1:0.1,ELE94711:1:0.1,GRO99222:1:0.1,GRO94758:2:0.2,SNA69641:1:0.1,GRO73461:1:0.1\n",
      "ELE11375\tFRO86643:1:0.1,FRO18919:1:0.1,GRO39357:1:0.1,SNA90258:1:0.1,ELE28573:2:0.2,ELE17451:2:0.2,SNA11465:1:0.1,FRO78087:2:0.2,SNA69641:2:0.2\n",
      "ELE17451\tSNA91554:1:0.1,GRO56989:2:0.2,FRO81176:2:0.2,ELE89019:1:0.1,SNA11465:3:0.3,ELE91550:1:0.1,FRO84225:1:0.1,SNA69641:3:0.3,FRO12685:1:0.1,FRO86643:3:0.3,FRO18919:3:0.3,ELE59935:1:0.1,ELE23393:2:0.2,FRO76833:1:0.1,FRO90334:1:0.1,ELE52966:1:0.1,ELE37798:2:0.2,GRO73461:3:0.3,GRO39357:1:0.1,SNA90258:3:0.3,ELE28573:2:0.2,SNA30755:1:0.1,FRO78087:2:0.2,GRO94758:2:0.2,GRO12298:1:0.1,GRO75578:1:0.1,SNA99873:2:0.2,ELE26917:1:0.1,FRO11987:1:0.1,SNA80192:5:0.5,SNA85662:2:0.2,ELE66810:1:0.1,ELE94711:1:0.1,GRO99222:3:0.3\n",
      "ELE23393\tFRO86643:1:0.1,FRO18919:1:0.1,SNA91554:1:0.1,GRO56989:1:0.1,SNA80192:1:0.1,SNA85662:1:0.1,ELE59935:1:0.1,SNA11465:1:0.1,ELE37798:1:0.1\n",
      "ELE26917\tFRO12685:1:0.1,SNA80192:1:0.1,GRO12298:1:0.1,SNA11465:1:0.1,SNA30755:1:0.1,ELE52966:1:0.1,ELE91550:1:0.1,GRO99222:1:0.1,FRO84225:1:0.1,FRO90334:1:0.1\n",
      "ELE28573\tFRO86643:1:0.1,FRO18919:1:0.1,GRO39357:1:0.1,SNA90258:1:0.1,SNA11465:1:0.1,FRO78087:2:0.2,SNA69641:2:0.2\n",
      "ELE37798\tFRO86643:1:0.1,GRO56989:2:0.2,SNA80192:1:0.1,FRO81176:1:0.1,SNA11465:1:0.1,FRO76833:1:0.1,GRO99222:1:0.1,GRO94758:1:0.1,SNA69641:1:0.1\n",
      "ELE52966\tFRO12685:1:0.1,SNA80192:1:0.1,GRO12298:1:0.1,SNA11465:1:0.1,SNA30755:1:0.1,ELE91550:1:0.1,GRO99222:1:0.1,FRO84225:1:0.1,FRO90334:1:0.1\n",
      "ELE59935\tSNA80192:1:0.1,SNA85662:1:0.1,FRO18919:1:0.1,SNA91554:1:0.1\n",
      "ELE66810\tSNA80192:1:0.1,SNA85662:1:0.1,FRO81176:1:0.1,SNA90258:1:0.1,ELE94711:1:0.1,GRO94758:1:0.1,GRO73461:1:0.1\n",
      "ELE89019\tSNA90258:1:0.1,FRO11987:1:0.1,GRO99222:1:0.1\n",
      "ELE91550\tFRO12685:1:0.1,SNA80192:1:0.1,GRO12298:1:0.1,SNA11465:1:0.1,SNA30755:1:0.1,GRO99222:1:0.1,FRO84225:1:0.1,FRO90334:1:0.1\n",
      "ELE94711\tSNA80192:1:0.1,SNA85662:1:0.1,FRO81176:1:0.1,SNA90258:1:0.1,GRO94758:1:0.1,GRO73461:1:0.1\n",
      "FRO11987\tSNA90258:1:0.1,GRO99222:1:0.1\n",
      "FRO12685\tSNA80192:1:0.1,FRO84225:1:0.1,SNA11465:1:0.1,SNA30755:1:0.1,GRO99222:1:0.1,GRO12298:1:0.1,FRO90334:1:0.1\n",
      "FRO18919\tSNA99873:1:0.1,FRO78087:1:0.1,SNA91554:1:0.1,SNA90258:1:0.1,SNA85662:1:0.1,SNA80192:2:0.2,GRO75578:1:0.1,SNA69641:1:0.1,GRO73461:1:0.1\n",
      "FRO76833\tGRO56989:1:0.1,SNA80192:1:0.1,FRO81176:1:0.1,GRO99222:1:0.1,GRO94758:1:0.1,SNA69641:1:0.1\n",
      "FRO78087\tSNA90258:1:0.1,SNA11465:1:0.1,FRO86643:1:0.1,GRO39357:1:0.1,SNA69641:2:0.2\n",
      "FRO81176\tGRO56989:1:0.1,SNA80192:2:0.2,SNA85662:1:0.1,SNA90258:1:0.1,GRO99222:1:0.1,GRO94758:2:0.2,SNA69641:1:0.1,GRO73461:1:0.1\n",
      "FRO84225\tSNA80192:1:0.1,SNA11465:1:0.1,SNA30755:1:0.1,GRO99222:1:0.1,GRO12298:1:0.1,FRO90334:1:0.1\n",
      "FRO86643\tSNA99873:1:0.1,GRO39357:1:0.1,GRO56989:1:0.1,SNA11465:2:0.2,SNA69641:1:0.1,GRO73461:1:0.1\n",
      "FRO90334\tSNA80192:1:0.1,SNA11465:1:0.1,GRO99222:1:0.1,GRO12298:1:0.1,SNA30755:1:0.1\n",
      "GRO12298\tSNA80192:1:0.1,SNA11465:1:0.1,GRO99222:1:0.1,SNA30755:1:0.1\n",
      "GRO39357\tSNA11465:1:0.1,SNA69641:1:0.1\n",
      "GRO56989\tSNA80192:1:0.1,SNA11465:1:0.1,GRO99222:1:0.1,GRO94758:1:0.1,SNA69641:1:0.1\n",
      "GRO73461\tSNA99873:2:0.2,SNA80192:2:0.2,SNA85662:1:0.1,SNA90258:1:0.1,GRO94758:1:0.1,GRO75578:1:0.1\n",
      "GRO75578\tSNA80192:1:0.1,SNA99873:1:0.1\n",
      "GRO94758\tSNA80192:2:0.2,SNA85662:1:0.1,GRO99222:1:0.1,SNA90258:1:0.1,SNA69641:1:0.1\n",
      "GRO99222\tSNA80192:2:0.2,SNA11465:1:0.1,SNA30755:1:0.1,SNA90258:1:0.1,SNA69641:1:0.1\n",
      "SNA11465\tSNA80192:1:0.1,SNA30755:1:0.1,SNA69641:1:0.1\n",
      "SNA30755\tSNA80192:1:0.1\n",
      "SNA69641\tSNA80192:1:0.1,SNA90258:1:0.1\n",
      "SNA80192\tSNA90258:1:0.1,SNA85662:2:0.2,SNA91554:1:0.1,SNA99873:1:0.1\n",
      "SNA85662\tSNA90258:1:0.1,SNA91554:1:0.1\n"
     ]
    }
   ],
   "source": [
    "# Note: have to change threshold to 1\n",
    "!head -n 10 ProductPurchaseData.txt | \\\n",
    "python mapper_3.5_1.py | \\\n",
    "sort -t$'\\t' -k1,1 | \\\n",
    "python reducer_3.5_1.py --threshold 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_3.5_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.5_2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # E.g.\n",
    "    # SNA80192\tSNA90258:1:0.1,SNA85662:2:0.2,SNA91554:1:0.1,SNA99873:1:0.1\n",
    "    (item, data) = line.strip().split(\"\\t\")\n",
    "    \n",
    "    # Output all the pair info\n",
    "    for pair in data.split(\",\"):\n",
    "        info = pair.split(\":\")\n",
    "        print \"\\t\".join([item, info[0], info[1], info[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_3.5_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_3.5_2.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys, Queue\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "\n",
    "n_max = 50\n",
    "a_max = []\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    if len(a_max) < n_max:\n",
    "        a_max.append(line)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Output the result\n",
    "print \"Top %d pairs\" % n_max\n",
    "for line in a_max:\n",
    "    rec = line.strip().split('\\t')\n",
    "    print \",\".join(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test for both mapper/reducer 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:custom,mapper_called,1\n",
      "reporter:counter:custom,mapper_called,1\n",
      "reporter:counter:custom,reducer_called,1\n",
      "reporter:counter:custom,reducer_called,1\n",
      "Top 50 pairs\n",
      "FRO40251,GRO73461,84,0.084\n",
      "DAI62779,GRO73461,79,0.079\n",
      "GRO73461,SNA80324,76,0.076\n",
      "DAI75645,FRO40251,73,0.073\n",
      "FRO40251,SNA80324,69,0.069\n",
      "DAI75645,SNA80324,68,0.068\n",
      "DAI75645,GRO73461,67,0.067\n",
      "ELE17451,GRO73461,63,0.063\n",
      "FRO73056,GRO73461,48,0.048\n",
      "ELE12792,SNA69641,46,0.046\n",
      "FRO40251,GRO85051,46,0.046\n",
      "DAI50913,ELE38289,44,0.044\n",
      "DAI62779,ELE17451,38,0.038\n",
      "FRO26482,GRO73461,38,0.038\n",
      "FRO40251,FRO92469,37,0.037\n",
      "FRO73056,GRO44993,37,0.037\n",
      "GRO56726,GRO73461,37,0.037\n",
      "DAI50913,ELE24064,36,0.036\n",
      "GRO15017,GRO73461,36,0.036\n",
      "GRO44993,GRO73461,36,0.036\n",
      "DAI50913,SNA46714,35,0.035\n",
      "ELE26917,GRO99222,34,0.034\n",
      "FRO85978,SNA95666,34,0.034\n",
      "DAI62779,SNA80324,33,0.033\n",
      "ELE73604,GRO73461,33,0.033\n",
      "DAI62779,DAI75645,32,0.032\n",
      "ELE24064,SNA46714,32,0.032\n",
      "GRO73461,SNA69641,32,0.032\n",
      "ELE17451,FRO40251,31,0.031\n",
      "GRO61133,GRO73461,31,0.031\n",
      "DAI22896,GRO73461,29,0.029\n",
      "DAI75645,GRO85051,29,0.029\n",
      "ELE17451,GRO99222,29,0.029\n",
      "ELE17451,ELE73604,28,0.028\n",
      "GRO30386,GRO73461,28,0.028\n",
      "DAI62779,FRO40251,27,0.027\n",
      "DAI87514,SNA80324,27,0.027\n",
      "ELE26917,GRO73461,27,0.027\n",
      "DAI75645,ELE17451,26,0.026\n",
      "FRO92469,GRO73461,26,0.026\n",
      "GRO73461,SNA38068,26,0.026\n",
      "DAI75645,FRO47962,25,0.025\n",
      "ELE12792,GRO73461,25,0.025\n",
      "DAI83948,FRO40251,24,0.024\n",
      "DAI88079,FRO40251,24,0.024\n",
      "ELE24064,ELE38289,24,0.024\n",
      "FRO40251,GRO56726,24,0.024\n",
      "ELE38289,SNA46714,23,0.023\n",
      "FRO66881,GRO56726,23,0.023\n",
      "FRO73056,SNA69641,23,0.023\n",
      "sort: write failed: standard output: Broken pipe\n",
      "sort: write error\n"
     ]
    }
   ],
   "source": [
    "## Note: have to change threshold to 1\n",
    "!head -n 1000 ProductPurchaseData.txt | \\\n",
    "python mapper_3.5_1.py | \\\n",
    "sort -t$'\\t' -k1,1 | \\\n",
    "python reducer_3.5_1.py --threshold 1 | \\\n",
    "python mapper_3.5_2.py | \\\n",
    "sort -t$'\\t' -k3,3rn -k1,2 | \\\n",
    "python reducer_3.5_2.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it in Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:16:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.5-part1\n",
      "16/02/03 10:16:10 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 10:16:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper_3.5_1.py, reducer_3.5_1.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar8236972170776780045/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob6091141613170820410.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "\n",
    "!hdfs dfs -rm -r hw3.5-part1\n",
    "\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapreduce.job.name='HW3.5-part1' \\\n",
    "-D mapred.map.tasks=3 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=1 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k1,1\" \\\n",
    "-file mapper_3.5_1.py -mapper mapper_3.5_1.py \\\n",
    "-file reducer_3.5_1.py -reducer reducer_3.5_1.py \\\n",
    "-input ProductPurchaseData.txt \\\n",
    "-output hw3.5-part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 10:19:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.5-part2\n",
      "16/02/03 10:19:58 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 10:19:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper_3.5_2.py, reducer_3.5_2.py, /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/hadoop-unjar8809908029816802731/] [] /var/folders/dm/nsw7wjf91f1c74hgl17ldw040000gn/T/streamjob2213618716350449340.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "\n",
    "!hdfs dfs -rm -r hw3.5-part2\n",
    "\n",
    "# For sorting, use the 1st field (count) as the primary key, in reverse order.\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapreduce.job.name='HW3.5-part2' \\\n",
    "-D mapred.map.tasks=4 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k3,3rn -k1,2\" \\\n",
    "-file mapper_3.5_2.py -mapper mapper_3.5_2.py \\\n",
    "-file reducer_3.5_2.py -reducer reducer_3.5_2.py \\\n",
    "-input hw3.5-part1/part-* \\\n",
    "-output hw3.5-part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 00:25:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Top 50 pairs\t\n",
      "DAI62779,ELE17451,1592,0.0511880646925\t\n",
      "FRO40251,SNA80324,1412,0.0454004694383\t\n",
      "DAI75645,FRO40251,1254,0.0403202469374\t\n",
      "FRO40251,GRO85051,1213,0.0390019613517\t\n",
      "DAI62779,GRO73461,1139,0.0366226166361\t\n",
      "DAI75645,SNA80324,1130,0.0363332368734\t\n",
      "DAI62779,FRO40251,1070,0.0344040384554\t\n",
      "DAI62779,SNA80324,923,0.0296775023311\t\n",
      "DAI62779,DAI85309,918,0.0295167357963\t\n",
      "ELE32164,GRO59710,911,0.0292916626475\t\n",
      "DAI62779,DAI75645,882,0.0283592167454\t\n",
      "FRO40251,GRO73461,882,0.0283592167454\t\n",
      "DAI62779,ELE92920,877,0.0281984502106\t\n",
      "FRO40251,FRO92469,835,0.026848011318\t\n",
      "DAI62779,ELE32164,832,0.0267515513971\t\n",
      "DAI75645,GRO73461,712,0.0228931545609\t\n",
      "DAI43223,ELE32164,711,0.022861001254\t\n",
      "DAI62779,GRO30386,709,0.02279669464\t\n",
      "ELE17451,FRO40251,697,0.0224108549564\t\n",
      "DAI85309,ELE99737,659,0.0211890292917\t\n",
      "DAI62779,ELE26917,650,0.020899649529\t\n",
      "GRO21487,GRO73461,631,0.0202887366966\t\n",
      "DAI62779,SNA45677,604,0.0194205974084\t\n",
      "ELE17451,SNA80324,597,0.0191955242597\t\n",
      "DAI62779,GRO71621,595,0.0191312176457\t\n",
      "DAI62779,SNA55762,593,0.0190669110318\t\n",
      "DAI62779,DAI83733,586,0.018841837883\t\n",
      "ELE17451,GRO73461,580,0.0186489180412\t\n",
      "GRO73461,SNA80324,562,0.0180701585158\t\n",
      "DAI62779,GRO59710,561,0.0180380052088\t\n",
      "DAI62779,FRO80039,550,0.0176843188322\t\n",
      "DAI75645,ELE17451,547,0.0175878589113\t\n",
      "DAI62779,SNA93860,537,0.0172663258416\t\n",
      "DAI55148,DAI62779,526,0.016912639465\t\n",
      "DAI43223,GRO59710,512,0.0164624931674\t\n",
      "ELE17451,ELE32164,511,0.0164303398605\t\n",
      "DAI62779,SNA18336,506,0.0162695733256\t\n",
      "ELE32164,GRO73461,486,0.0156265071863\t\n",
      "DAI62779,FRO78087,482,0.0154978939584\t\n",
      "DAI85309,ELE17451,482,0.0154978939584\t\n",
      "DAI62779,GRO94758,479,0.0154014340375\t\n",
      "DAI62779,GRO21487,471,0.0151442075817\t\n",
      "GRO85051,SNA80324,471,0.0151442075817\t\n",
      "ELE17451,GRO30386,468,0.0150477476608\t\n",
      "FRO85978,SNA95666,463,0.014886981126\t\n",
      "DAI62779,FRO19221,462,0.014854827819\t\n",
      "DAI62779,GRO46854,461,0.0148226745121\t\n",
      "DAI43223,DAI62779,459,0.0147583678981\t\n",
      "ELE92920,SNA18336,455,0.0146297546703\t\n",
      "DAI88079,FRO40251,446,0.0143403749076\t\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "!hdfs dfs -cat hw3.5-part2/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Report \n",
    "\n",
    "Compute time: 3min 40sec+ 18sec = 3min 58sec   \n",
    "Setup: Quad Core, OS X, 4 mappers, 1 reducer  \n",
    "Mapper called: 3 + 3 = 6 times  \n",
    "Reducer called: 1 + 1 = 2 times  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
