{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 87483  100 87483    0     0  79720      0  0:00:01  0:00:01 --:--:-- 79674\n"
     ]
    }
   ],
   "source": [
    "#run any unix command\n",
    "!curl http://www.gutenberg.org/cache/epub/48054/pg48054.txt > WordCount/historical_tours.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PING www.bbc.net.uk (212.58.246.54): 56 data bytes\n",
      "64 bytes from 212.58.246.54: icmp_seq=0 ttl=52 time=152.135 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=1 ttl=52 time=153.965 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=2 ttl=52 time=170.487 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=3 ttl=52 time=164.168 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=4 ttl=52 time=152.358 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=5 ttl=52 time=153.545 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=6 ttl=52 time=154.728 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=7 ttl=52 time=149.378 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=8 ttl=52 time=156.088 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=9 ttl=52 time=152.145 ms\n",
      "64 bytes from 212.58.246.54: icmp_seq=10 ttl=52 time=153.094 ms\n",
      "^C\n",
      "--- www.bbc.net.uk ping statistics ---\n",
      "12 packets transmitted, 11 packets received, 8.3% packet loss\n",
      "round-trip min/avg/max/stddev = 149.378/155.645/170.487/5.896 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ping www.bbc.co.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcount Example in map reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the following a detailed presentation of the word count example\n",
    "#    http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/mapper.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "#sys.stderr.write(\"reporter:counter:Tokens,Total,1\") # NOTE missing the carriage return so wont work\n",
    "sys.stderr.write(\"reporter:counter:Mapper Counters,Calls,1\\n\")\n",
    "sys.stderr.write(\"reporter:status:processing my message...how are you\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    for word in line.split():\n",
    "        print '%s\\t%s' % (word, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount/reducer.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load WordCount/reducer.py  #Load code into the current frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, value = line.split()\n",
    "    if key == cur_key:\n",
    "        cur_count += int(value)\n",
    "    else:\n",
    "        if cur_key:\n",
    "            print '%s\\t%s' % (cur_key, cur_count)\n",
    "        cur_key = key\n",
    "        cur_count = int(value)\n",
    "\n",
    "print '%s\\t%s' % (cur_key, cur_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your mapper.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "foo\t1\r\n",
      "foo\t1\r\n",
      "quux\t1\r\n",
      "labs\t1\r\n",
      "foo\t1\r\n",
      "bar\t1\r\n",
      "quux\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper Counters,Calls,1\r\n",
      "reporter:status:processing my message...how are you\r\n",
      "reporter:counter:Reducer Counters,Calls,1\r\n",
      "foo\t2\r\n",
      "bar\t1\r\n",
      "foo\t1\r\n",
      "labs\t1\r\n",
      "quux\t1\r\n",
      "quux\t1\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"foo foo quux labs foo bar quux\" | WordCount/mapper.py | WordCount/reducer.py| sort -k2,2nr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export HADOOP_HOME=/usr/local/Cellar/hadoop/2.6.0\r\n",
      "export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\r\n",
      "export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib\"\r\n",
      "alias hstart=\"/usr/local/Cellar/hadoop/2.6.0/sbin/start-dfs.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/start-yarn.sh\"\r\n",
      "alias hstop=\"/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh\"\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# MUST start hadoop server\n",
    "!cat ~/.profile\n",
    "#!hstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "16/01/30 09:22:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/30 09:22:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 09:22:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-jshanahan-namenode-JAMES-SHANAHANs-Desktop-Pro-2.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-jshanahan-datanode-JAMES-SHANAHANs-Desktop-Pro-2.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-jshanahan-secondarynamenode-JAMES-SHANAHANs-Desktop-Pro-2.local.out\n",
      "16/01/30 09:23:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-jshanahan-resourcemanager-JAMES-SHANAHANs-Desktop-Pro-2.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-jshanahan-nodemanager-JAMES-SHANAHANs-Desktop-Pro-2.local.out\n"
     ]
    }
   ],
   "source": [
    "#stop old cluster\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh\n",
    "#hstart Cluster\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/start-dfs.sh; /usr/local/Cellar/hadoop/2.6.0/sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting historyserver, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/mapred-jshanahan-historyserver-JAMES-SHANAHANs-Desktop-Pro-2.local.out\r\n"
     ]
    }
   ],
   "source": [
    "#this may not work for all distributions\n",
    "\n",
    "#STEP 0 Kill ill existing server using kill -9 \"insert your PID here\"\n",
    "# STEP 1:\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/mr-jobhistory-daemon.sh  start historyserver\n",
    "#!/usr/local/Cellar/hadoop/2.6.0/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2.6.0/etc/hadoop/ start historyserver\n",
    "\n",
    "#If you are running Hadoop locally, you need to start the JobHistoryServer by running (assuming in hadoop directory)\n",
    "#./sbin/mr-jobhistory-daemon.sh --config ./etc/hadoop/ start historyserver \n",
    "\n",
    "# STEP 2: Then you will be able to access the job information by navigating to:\n",
    "#     \n",
    "# http://192.168.0.10:19888/jobhistory/app\n",
    "# http://127.0.0.1:19888/jobhistory/app\n",
    "\n",
    "# STEP 3: Click your job and then click the Counter tab on the left:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat /usr/local/Cellar/hadoop/2.6.0/libexec/logs/mapred-jshanahan-historyserver-JAMES-SHANAHANs-Desktop-Pro-2.local.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/27 20:44:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2015-02-26 21:03 gutenberg-output\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2015-02-26 19:36 historical_tours.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/02/26 21:02:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "copyFromLocal: `historical_tours.txt': File exists\n",
      "15/02/26 21:02:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 1 items\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2015-02-26 19:36 historical_tours.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -copyFromLocal WordCount/historical_tours.txt \n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count example with sorting (one reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting testWordCountInput.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile testWordCountInput.txt\n",
    "hello this is Jimi\n",
    "jimi who Jimi three Jimi \n",
    "Hello\n",
    "hello\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 18:09:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 18:09:24 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted testWordCountInput.txt\n",
      "16/02/01 18:09:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 18:09:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 18:09:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted wordcount-output\n",
      "16/02/01 18:09:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 18:09:28 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 18:09:28 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 18:09:28 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 18:09:29 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 18:09:29 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 18:09:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local576587423_0001\n",
      "16/02/01 18:09:29 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 18:09:29 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 18:09:29 INFO mapreduce.Job: Running job: job_local576587423_0001\n",
      "16/02/01 18:09:29 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 18:09:29 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 18:09:29 INFO mapred.LocalJobRunner: Starting task: attempt_local576587423_0001_m_000000_0\n",
      "16/02/01 18:09:29 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 18:09:29 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/testWordCountInput.txt:0+56\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/mapper.py]\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:09:29 INFO mapred.LocalJobRunner: \n",
      "16/02/01 18:09:29 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: bufstart = 0; bufend = 78; bufvoid = 104857600\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214356(104857424); length = 41/6553600\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 18:09:29 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:09:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:09:29 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 18:09:29 INFO mapred.Task: Task:attempt_local576587423_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 18:09:29 INFO mapred.LocalJobRunner: Records R/W=4/1\n",
      "16/02/01 18:09:29 INFO mapred.Task: Task 'attempt_local576587423_0001_m_000000_0' done.\n",
      "16/02/01 18:09:29 INFO mapred.LocalJobRunner: Finishing task: attempt_local576587423_0001_m_000000_0\n",
      "16/02/01 18:09:29 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 18:09:29 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 18:09:29 INFO mapred.LocalJobRunner: Starting task: attempt_local576587423_0001_r_000000_0\n",
      "16/02/01 18:09:29 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 18:09:29 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 18:09:30 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@39e13ba1\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 18:09:30 INFO reduce.EventFetcher: attempt_local576587423_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 18:09:30 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local576587423_0001_m_000000_0 decomp: 40 len: 44 to MEMORY\n",
      "16/02/01 18:09:30 INFO reduce.InMemoryMapOutput: Read 40 bytes from map-output for attempt_local576587423_0001_m_000000_0\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 40, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->40\n",
      "16/02/01 18:09:30 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: Merged 1 segments, 40 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: Merging 1 files, 44 bytes from disk\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 18:09:30 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 18:09:30 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:09:30 INFO mapred.Task: Task:attempt_local576587423_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:09:30 INFO mapred.Task: Task attempt_local576587423_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 18:09:30 INFO output.FileOutputCommitter: Saved output of task 'attempt_local576587423_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/wordcount-output/_temporary/0/task_local576587423_0001_r_000000\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: Records R/W=4/1 > reduce\n",
      "16/02/01 18:09:30 INFO mapred.Task: Task 'attempt_local576587423_0001_r_000000_0' done.\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: Finishing task: attempt_local576587423_0001_r_000000_0\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: Starting task: attempt_local576587423_0001_r_000001_0\n",
      "16/02/01 18:09:30 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 18:09:30 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 18:09:30 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@40230546\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 18:09:30 INFO reduce.EventFetcher: attempt_local576587423_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 18:09:30 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local576587423_0001_m_000000_0 decomp: 11 len: 15 to MEMORY\n",
      "16/02/01 18:09:30 INFO reduce.InMemoryMapOutput: Read 11 bytes from map-output for attempt_local576587423_0001_m_000000_0\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 11, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->11\n",
      "16/02/01 18:09:30 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: Merged 1 segments, 11 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: Merging 1 files, 15 bytes from disk\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 18:09:30 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:09:30 INFO mapred.Task: Task:attempt_local576587423_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:09:30 INFO mapred.Task: Task attempt_local576587423_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 18:09:30 INFO output.FileOutputCommitter: Saved output of task 'attempt_local576587423_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/wordcount-output/_temporary/0/task_local576587423_0001_r_000001\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 18:09:30 INFO mapred.Task: Task 'attempt_local576587423_0001_r_000001_0' done.\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: Finishing task: attempt_local576587423_0001_r_000001_0\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: Starting task: attempt_local576587423_0001_r_000002_0\n",
      "16/02/01 18:09:30 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 18:09:30 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 18:09:30 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1cdbcc35\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 18:09:30 INFO reduce.EventFetcher: attempt_local576587423_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 18:09:30 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local576587423_0001_m_000000_0 decomp: 27 len: 31 to MEMORY\n",
      "16/02/01 18:09:30 INFO reduce.InMemoryMapOutput: Read 27 bytes from map-output for attempt_local576587423_0001_m_000000_0\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 27, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->27\n",
      "16/02/01 18:09:30 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 19 bytes\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: Merged 1 segments, 27 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: Merging 1 files, 31 bytes from disk\n",
      "16/02/01 18:09:30 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 18:09:30 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 19 bytes\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:09:30 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:09:30 INFO mapred.Task: Task:attempt_local576587423_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 18:09:30 INFO mapred.Task: Task attempt_local576587423_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 18:09:30 INFO output.FileOutputCommitter: Saved output of task 'attempt_local576587423_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/wordcount-output/_temporary/0/task_local576587423_0001_r_000002\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/01 18:09:30 INFO mapred.Task: Task 'attempt_local576587423_0001_r_000002_0' done.\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: Finishing task: attempt_local576587423_0001_r_000002_0\n",
      "16/02/01 18:09:30 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 18:09:30 INFO mapreduce.Job: Job job_local576587423_0001 running in uber mode : false\n",
      "16/02/01 18:09:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 18:09:30 INFO mapreduce.Job: Job job_local576587423_0001 completed successfully\n",
      "16/02/01 18:09:30 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=421634\n",
      "\t\tFILE: Number of bytes written=1502345\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=224\n",
      "\t\tHDFS: Number of bytes written=123\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=11\n",
      "\t\tMap output bytes=78\n",
      "\t\tMap output materialized bytes=90\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=11\n",
      "\t\tCombine output records=8\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=90\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1442840576\n",
      "\tMapper Counters\n",
      "\t\tCalls=1\n",
      "\tReducer Counters\n",
      "\t\tCalls=6\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=56\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56\n",
      "16/02/01 18:09:30 INFO streaming.StreamJob: Output directory: wordcount-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm testWordCountInput.txt \n",
    "!hdfs dfs -copyFromLocal testWordCountInput.txt \n",
    "!hdfs dfs -rm -r wordcount-output\n",
    "#usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib\n",
    "dataDir = \"/Users/jshanahan/Dropbox/lectures-uc-berkeley-ml-class-2015/Notebooks/WordCount\"\n",
    "\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -mapper WordCount/mapper.py \\\n",
    "   -reducer WordCount/reducer.py \\\n",
    "   -combiner WordCount/reducer.py \\\n",
    "   -input testWordCountInput.txt \\\n",
    "   -output wordcount-output  \\\n",
    "   -numReduceTasks 3\n",
    "   #--D mapreduce.job.reduces=2\n",
    "#-input historical_tours.txt  file on Hadoop\n",
    "\n",
    "\n",
    "#output directory on Hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 18:09:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "hello this is Jimi\n",
      "jimi who Jimi three Jimi \n",
      "Hello\n",
      "hello16/02/01 18:09:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Hello\t1\n",
      "jimi\t1\n",
      "this\t1\n",
      "three\t1\n",
      "Jimi\t3\n",
      "hello\t2\n",
      "is\t1\n",
      "who\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat testWordCountInput.txt\n",
    "\n",
    "!hdfs dfs -cat wordcount-output/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 17:20:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:20:33 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted wordcount-output-sorted\n",
      "16/02/01 17:20:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 17:20:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 17:20:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 17:20:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 17:20:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 17:20:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local836241912_0001\n",
      "16/02/01 17:20:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 17:20:35 INFO mapreduce.Job: Running job: job_local836241912_0001\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: Starting task: attempt_local836241912_0001_m_000000_0\n",
      "16/02/01 17:20:35 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 17:20:35 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00000:0+49\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 17:20:35 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/mapper.py]\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 17:20:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:20:35 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 17:20:35 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/01 17:20:35 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: \n",
      "16/02/01 17:20:35 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: bufstart = 0; bufend = 77; bufvoid = 104857600\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214344(104857376); length = 53/6553600\n",
      "16/02/01 17:20:35 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 17:20:35 INFO mapred.Task: Task:attempt_local836241912_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: processing my message...how are you\n",
      "16/02/01 17:20:35 INFO mapred.Task: Task 'attempt_local836241912_0001_m_000000_0' done.\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: Finishing task: attempt_local836241912_0001_m_000000_0\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: Starting task: attempt_local836241912_0001_r_000000_0\n",
      "16/02/01 17:20:35 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 17:20:35 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 17:20:35 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@249083dc\n",
      "16/02/01 17:20:35 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 17:20:35 INFO reduce.EventFetcher: attempt_local836241912_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 17:20:35 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local836241912_0001_m_000000_0 decomp: 107 len: 111 to MEMORY\n",
      "16/02/01 17:20:35 INFO reduce.InMemoryMapOutput: Read 107 bytes from map-output for attempt_local836241912_0001_m_000000_0\n",
      "16/02/01 17:20:35 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 107, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->107\n",
      "16/02/01 17:20:35 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:20:35 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 17:20:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:20:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 99 bytes\n",
      "16/02/01 17:20:35 INFO reduce.MergeManagerImpl: Merged 1 segments, 107 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 17:20:35 INFO reduce.MergeManagerImpl: Merging 1 files, 111 bytes from disk\n",
      "16/02/01 17:20:35 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 17:20:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 17:20:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 99 bytes\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:20:35 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 17:20:35 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 17:20:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:20:35 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 17:20:35 INFO streaming.PipeMapRed: Records R/W=14/1\n",
      "16/02/01 17:20:35 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 17:20:35 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 17:20:35 INFO mapred.Task: Task:attempt_local836241912_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 17:20:35 INFO mapred.Task: Task attempt_local836241912_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 17:20:35 INFO output.FileOutputCommitter: Saved output of task 'attempt_local836241912_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/wordcount-output-sorted/_temporary/0/task_local836241912_0001_r_000000\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: Records R/W=14/1 > reduce\n",
      "16/02/01 17:20:35 INFO mapred.Task: Task 'attempt_local836241912_0001_r_000000_0' done.\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: Finishing task: attempt_local836241912_0001_r_000000_0\n",
      "16/02/01 17:20:35 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 17:20:36 INFO mapreduce.Job: Job job_local836241912_0001 running in uber mode : false\n",
      "16/02/01 17:20:36 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 17:20:36 INFO mapreduce.Job: Job job_local836241912_0001 completed successfully\n",
      "16/02/01 17:20:36 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=210548\n",
      "\t\tFILE: Number of bytes written=751275\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=98\n",
      "\t\tHDFS: Number of bytes written=77\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=14\n",
      "\t\tMap output bytes=77\n",
      "\t\tMap output materialized bytes=111\n",
      "\t\tInput split bytes=116\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=14\n",
      "\t\tReduce shuffle bytes=111\n",
      "\t\tReduce input records=14\n",
      "\t\tReduce output records=14\n",
      "\t\tSpilled Records=28\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=724566016\n",
      "\tMapper Counters\n",
      "\t\tCalls=1\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=49\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=77\n",
      "16/02/01 17:20:36 INFO streaming.StreamJob: Output directory: wordcount-output-sorted\n"
     ]
    }
   ],
   "source": [
    "#This will NOT work IdentityMapper, IdentityReducer\n",
    "\n",
    "!hdfs dfs -rm -r wordcount-output-sorted\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr\" \\\n",
    "   -mapper WordCount/mapper.py \\\n",
    "   -reducer WordCount/reducer.py \\\n",
    "   -input wordcount-output \\\n",
    "   -output wordcount-output-sorted  \\\n",
    "   -numReduceTasks 1 \\\n",
    "   -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "  -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sortWordCountMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sortWordCountMapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 18:10:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 18:10:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted wordcount-output-sorted\n",
      "16/02/01 18:10:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 18:10:43 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 18:10:43 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 18:10:43 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 18:10:44 INFO mapred.FileInputFormat: Total input paths to process : 3\n",
      "16/02/01 18:10:44 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/02/01 18:10:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local26226907_0001\n",
      "16/02/01 18:10:44 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 18:10:44 INFO mapreduce.Job: Running job: job_local26226907_0001\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Starting task: attempt_local26226907_0001_m_000000_0\n",
      "16/02/01 18:10:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 18:10:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00000:0+30\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./sortWordCountMapper.py]\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: \n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: bufstart = 0; bufend = 38; bufvoid = 104857600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 18:10:44 INFO mapred.Task: Task:attempt_local26226907_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Records R/W=4/1\n",
      "16/02/01 18:10:44 INFO mapred.Task: Task 'attempt_local26226907_0001_m_000000_0' done.\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local26226907_0001_m_000000_0\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Starting task: attempt_local26226907_0001_m_000001_0\n",
      "16/02/01 18:10:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 18:10:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00002:0+19\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./sortWordCountMapper.py]\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: \n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: bufstart = 0; bufend = 25; bufvoid = 104857600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214376(104857504); length = 21/6553600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 18:10:44 INFO mapred.Task: Task:attempt_local26226907_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Records R/W=3/1\n",
      "16/02/01 18:10:44 INFO mapred.Task: Task 'attempt_local26226907_0001_m_000001_0' done.\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local26226907_0001_m_000001_0\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Starting task: attempt_local26226907_0001_m_000002_0\n",
      "16/02/01 18:10:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 18:10:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/wordcount-output/part-00001:0+7\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./sortWordCountMapper.py]\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: \n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: bufstart = 0; bufend = 9; bufvoid = 104857600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n",
      "16/02/01 18:10:44 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 18:10:44 INFO mapred.Task: Task:attempt_local26226907_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "16/02/01 18:10:44 INFO mapred.Task: Task 'attempt_local26226907_0001_m_000002_0' done.\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local26226907_0001_m_000002_0\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: Starting task: attempt_local26226907_0001_r_000000_0\n",
      "16/02/01 18:10:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 18:10:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 18:10:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2dd0fbe4\n",
      "16/02/01 18:10:44 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=352321536, maxSingleShuffleLimit=88080384, mergeThreshold=232532224, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 18:10:44 INFO reduce.EventFetcher: attempt_local26226907_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 18:10:44 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local26226907_0001_m_000000_0 decomp: 56 len: 60 to MEMORY\n",
      "16/02/01 18:10:44 INFO reduce.InMemoryMapOutput: Read 56 bytes from map-output for attempt_local26226907_0001_m_000000_0\n",
      "16/02/01 18:10:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 56, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->56\n",
      "16/02/01 18:10:44 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local26226907_0001_m_000002_0 decomp: 15 len: 19 to MEMORY\n",
      "16/02/01 18:10:44 INFO reduce.InMemoryMapOutput: Read 15 bytes from map-output for attempt_local26226907_0001_m_000002_0\n",
      "16/02/01 18:10:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 15, inMemoryMapOutputs.size() -> 2, commitMemory -> 56, usedMemory ->71\n",
      "16/02/01 18:10:44 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local26226907_0001_m_000001_0 decomp: 39 len: 43 to MEMORY\n",
      "16/02/01 18:10:44 INFO reduce.InMemoryMapOutput: Read 39 bytes from map-output for attempt_local26226907_0001_m_000001_0\n",
      "16/02/01 18:10:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 39, inMemoryMapOutputs.size() -> 3, commitMemory -> 71, usedMemory ->110\n",
      "16/02/01 18:10:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 18:10:44 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 18:10:44 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/01 18:10:44 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 87 bytes\n",
      "16/02/01 18:10:44 INFO reduce.MergeManagerImpl: Merged 3 segments, 110 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 18:10:44 INFO reduce.MergeManagerImpl: Merging 1 files, 110 bytes from disk\n",
      "16/02/01 18:10:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 18:10:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 18:10:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 99 bytes\n",
      "16/02/01 18:10:44 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 18:10:44 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./sortWordCountMapper.py]\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 18:10:44 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 18:10:45 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:10:45 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 18:10:45 INFO streaming.PipeMapRed: Records R/W=16/1\n",
      "16/02/01 18:10:45 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 18:10:45 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 18:10:45 INFO mapred.Task: Task:attempt_local26226907_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 18:10:45 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 18:10:45 INFO mapred.Task: Task attempt_local26226907_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 18:10:45 INFO output.FileOutputCommitter: Saved output of task 'attempt_local26226907_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/wordcount-output-sorted/_temporary/0/task_local26226907_0001_r_000000\n",
      "16/02/01 18:10:45 INFO mapred.LocalJobRunner: Records R/W=16/1 > reduce\n",
      "16/02/01 18:10:45 INFO mapred.Task: Task 'attempt_local26226907_0001_r_000000_0' done.\n",
      "16/02/01 18:10:45 INFO mapred.LocalJobRunner: Finishing task: attempt_local26226907_0001_r_000000_0\n",
      "16/02/01 18:10:45 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 18:10:45 INFO mapreduce.Job: Job job_local26226907_0001 running in uber mode : false\n",
      "16/02/01 18:10:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 18:10:45 INFO mapreduce.Job: Job job_local26226907_0001 completed successfully\n",
      "16/02/01 18:10:45 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=423787\n",
      "\t\tFILE: Number of bytes written=1498069\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=191\n",
      "\t\tHDFS: Number of bytes written=104\n",
      "\t\tHDFS: Number of read operations=37\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=6\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=72\n",
      "\t\tMap output materialized bytes=122\n",
      "\t\tInput split bytes=348\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=16\n",
      "\t\tReduce shuffle bytes=122\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=32\n",
      "\t\tSpilled Records=32\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1831337984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=56\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=104\n",
      "16/02/01 18:10:45 INFO streaming.StreamJob: Output directory: wordcount-output-sorted\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r wordcount-output-sorted\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr\" \\\n",
    "   -mapper sortWordCountMapper.py \\\n",
    "   -reducer sortWordCountMapper.py \\\n",
    "   -input wordcount-output \\\n",
    "   -output wordcount-output-sorted  \\\n",
    "   -numReduceTasks 1 \\\n",
    "#DOES not work in streaming mode\n",
    "#   -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "#  -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 18:10:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 18:10 wordcount-output-sorted/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup        104 2016-02-01 18:10 wordcount-output-sorted/part-00000\n",
      "ls: `wordcount-output-sorted/part-00000.SORTED.txt': No such file or directory\n",
      "16/02/01 18:10:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Jimi\t3\n",
      "hello\t2\n",
      "Hello\t1\n",
      "is\t1\n",
      "jimi\t1\n",
      "this\t1\n",
      "three\t1\n",
      "who\t1\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls wordcount-output-sorted/*\n",
    "!rm -r wordcount-output-sorted\n",
    "!hdfs dfs -copyToLocal wordcount-output-sorted \n",
    "!sort -k2,2nr <wordcount-output-sorted/part-00000 >wordcount-output-sorted/part-00000.SORTED.txt\n",
    "!head -n 100 wordcount-output-sorted/part-00000.SORTED.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r gutenberg-output-sorted\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "   -mapper WordCount/mapper.py \\\n",
    "   -reducer WordCount/reducer.py \\\n",
    "   -input gutenberg-output \\\n",
    "   -output gutenberg-output-sorted  \\\n",
    "   -numReduceTasks 1 \\\n",
    "   -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    " -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:57:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:57:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted gutenberg-output\n",
      "16/02/01 16:57:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:57:30 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:57:30 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:57:30 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:57:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:57:31 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:57:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local439453704_0001\n",
      "16/02/01 16:57:31 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:57:31 INFO mapreduce.Job: Running job: job_local439453704_0001\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Starting task: attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:31 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:57:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/historical_tours.txt:0+87483\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/mapper.py]\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:57:31 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: Records R/W=1941/1\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:57:31 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: bufstart = 0; bufend = 108364; bufvoid = 104857600\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26159436(104637744); length = 54961/6553600\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task:attempt_local439453704_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: processing my message...how are you\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task 'attempt_local439453704_0001_m_000000_0' done.\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Starting task: attempt_local439453704_0001_r_000000_0\n",
      "16/02/01 16:57:31 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:57:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:57:31 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@de86e16\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: attempt_local439453704_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:57:31 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local439453704_0001_m_000000_0 decomp: 52641 len: 52645 to MEMORY\n",
      "16/02/01 16:57:31 INFO reduce.InMemoryMapOutput: Read 52641 bytes from map-output for attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 52641, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->52641\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 52628 bytes\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merged 1 segments, 52641 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 1 files, 52645 bytes from disk\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 52628 bytes\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: Records R/W=5699/1\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task:attempt_local439453704_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task attempt_local439453704_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:57:31 INFO output.FileOutputCommitter: Saved output of task 'attempt_local439453704_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-output/_temporary/0/task_local439453704_0001_r_000000\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Records R/W=5699/1 > reduce\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task 'attempt_local439453704_0001_r_000000_0' done.\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local439453704_0001_r_000000_0\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Starting task: attempt_local439453704_0001_r_000001_0\n",
      "16/02/01 16:57:31 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:57:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:57:31 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6b88e91\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: attempt_local439453704_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:57:31 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local439453704_0001_m_000000_0 decomp: 22956 len: 22960 to MEMORY\n",
      "16/02/01 16:57:31 INFO reduce.InMemoryMapOutput: Read 22956 bytes from map-output for attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 22956, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->22956\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 22944 bytes\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merged 1 segments, 22956 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 1 files, 22960 bytes from disk\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 22944 bytes\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: Records R/W=2056/1\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task:attempt_local439453704_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task attempt_local439453704_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 16:57:31 INFO output.FileOutputCommitter: Saved output of task 'attempt_local439453704_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-output/_temporary/0/task_local439453704_0001_r_000001\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Records R/W=2056/1 > reduce\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task 'attempt_local439453704_0001_r_000001_0' done.\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local439453704_0001_r_000001_0\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Starting task: attempt_local439453704_0001_r_000002_0\n",
      "16/02/01 16:57:31 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:57:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:57:31 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6c826707\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: attempt_local439453704_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:57:31 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local439453704_0001_m_000000_0 decomp: 31499 len: 31503 to MEMORY\n",
      "16/02/01 16:57:31 INFO reduce.InMemoryMapOutput: Read 31499 bytes from map-output for attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 31499, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31499\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 31489 bytes\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merged 1 segments, 31499 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 1 files, 31503 bytes from disk\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 31489 bytes\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: Records R/W=3172/1\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task:attempt_local439453704_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task attempt_local439453704_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 16:57:32 INFO output.FileOutputCommitter: Saved output of task 'attempt_local439453704_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-output/_temporary/0/task_local439453704_0001_r_000002\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: Records R/W=3172/1 > reduce\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task 'attempt_local439453704_0001_r_000002_0' done.\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: Finishing task: attempt_local439453704_0001_r_000002_0\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: Starting task: attempt_local439453704_0001_r_000003_0\n",
      "16/02/01 16:57:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:57:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:57:32 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6d0bbc23\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:57:32 INFO reduce.EventFetcher: attempt_local439453704_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:57:32 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local439453704_0001_m_000000_0 decomp: 28758 len: 28762 to MEMORY\n",
      "16/02/01 16:57:32 INFO reduce.InMemoryMapOutput: Read 28758 bytes from map-output for attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 28758, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->28758\n",
      "16/02/01 16:57:32 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:57:32 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:32 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28747 bytes\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: Merged 1 segments, 28758 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: Merging 1 files, 28762 bytes from disk\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:57:32 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:32 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28747 bytes\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: Records R/W=2814/1\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task:attempt_local439453704_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task attempt_local439453704_0001_r_000003_0 is allowed to commit now\n",
      "16/02/01 16:57:32 INFO output.FileOutputCommitter: Saved output of task 'attempt_local439453704_0001_r_000003_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-output/_temporary/0/task_local439453704_0001_r_000003\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: Records R/W=2814/1 > reduce\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task 'attempt_local439453704_0001_r_000003_0' done.\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: Finishing task: attempt_local439453704_0001_r_000003_0\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:57:32 INFO mapreduce.Job: Job job_local439453704_0001 running in uber mode : false\n",
      "16/02/01 16:57:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:57:32 INFO mapreduce.Job: Job job_local439453704_0001 completed successfully\n",
      "16/02/01 16:57:32 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1306039\n",
      "\t\tFILE: Number of bytes written=2923813\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=437415\n",
      "\t\tHDFS: Number of bytes written=92049\n",
      "\t\tHDFS: Number of read operations=55\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=25\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1941\n",
      "\t\tMap output records=13741\n",
      "\t\tMap output bytes=108364\n",
      "\t\tMap output materialized bytes=135870\n",
      "\t\tInput split bytes=109\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3736\n",
      "\t\tReduce shuffle bytes=135870\n",
      "\t\tReduce input records=13741\n",
      "\t\tReduce output records=3736\n",
      "\t\tSpilled Records=27482\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=28\n",
      "\t\tTotal committed heap usage (bytes)=1394081792\n",
      "\tMapper Counters\n",
      "\t\tCalls=1\n",
      "\tReducer Counters\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=87483\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=36548\n",
      "16/02/01 16:57:32 INFO streaming.StreamJob: Output directory: gutenberg-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r gutenberg-output\n",
    "#usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib\n",
    "dataDir = \"/Users/jshanahan/Dropbox/lectures-uc-berkeley-ml-class-2015/Notebooks/WordCount\"\n",
    "\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -mapper WordCount/mapper.py \\\n",
    "   -reducer WordCount/reducer.py \\\n",
    "   -input historical_tours.txt \\\n",
    "   -output gutenberg-output  \\\n",
    "   -numReduceTasks 4\n",
    "   #--D mapreduce.job.reduces=2\n",
    "#-input historical_tours.txt  file on Hadoop\n",
    "\n",
    "\n",
    "#output directory on Hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:57:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:57:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted gutenberg-output\n",
      "16/02/01 16:57:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:57:30 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:57:30 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:57:30 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:57:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 16:57:31 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 16:57:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local439453704_0001\n",
      "16/02/01 16:57:31 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:57:31 INFO mapreduce.Job: Running job: job_local439453704_0001\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Starting task: attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:31 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:57:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/historical_tours.txt:0+87483\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/mapper.py]\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:57:31 INFO mapred.LineRecordReader: Found UTF-8 BOM and skipped it\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: Records R/W=1941/1\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:57:31 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: bufstart = 0; bufend = 108364; bufvoid = 104857600\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26159436(104637744); length = 54961/6553600\n",
      "16/02/01 16:57:31 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task:attempt_local439453704_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: processing my message...how are you\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task 'attempt_local439453704_0001_m_000000_0' done.\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Starting task: attempt_local439453704_0001_r_000000_0\n",
      "16/02/01 16:57:31 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:57:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:57:31 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@de86e16\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: attempt_local439453704_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:57:31 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local439453704_0001_m_000000_0 decomp: 52641 len: 52645 to MEMORY\n",
      "16/02/01 16:57:31 INFO reduce.InMemoryMapOutput: Read 52641 bytes from map-output for attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 52641, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->52641\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 52628 bytes\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merged 1 segments, 52641 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 1 files, 52645 bytes from disk\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 52628 bytes\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: Records R/W=5699/1\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task:attempt_local439453704_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task attempt_local439453704_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:57:31 INFO output.FileOutputCommitter: Saved output of task 'attempt_local439453704_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-output/_temporary/0/task_local439453704_0001_r_000000\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Records R/W=5699/1 > reduce\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task 'attempt_local439453704_0001_r_000000_0' done.\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local439453704_0001_r_000000_0\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Starting task: attempt_local439453704_0001_r_000001_0\n",
      "16/02/01 16:57:31 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:57:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:57:31 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6b88e91\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: attempt_local439453704_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:57:31 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local439453704_0001_m_000000_0 decomp: 22956 len: 22960 to MEMORY\n",
      "16/02/01 16:57:31 INFO reduce.InMemoryMapOutput: Read 22956 bytes from map-output for attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 22956, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->22956\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 22944 bytes\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merged 1 segments, 22956 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 1 files, 22960 bytes from disk\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 22944 bytes\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 16:57:31 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: Records R/W=2056/1\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:57:31 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task:attempt_local439453704_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task attempt_local439453704_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 16:57:31 INFO output.FileOutputCommitter: Saved output of task 'attempt_local439453704_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-output/_temporary/0/task_local439453704_0001_r_000001\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Records R/W=2056/1 > reduce\n",
      "16/02/01 16:57:31 INFO mapred.Task: Task 'attempt_local439453704_0001_r_000001_0' done.\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Finishing task: attempt_local439453704_0001_r_000001_0\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: Starting task: attempt_local439453704_0001_r_000002_0\n",
      "16/02/01 16:57:31 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:57:31 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:57:31 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6c826707\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: attempt_local439453704_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:57:31 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local439453704_0001_m_000000_0 decomp: 31499 len: 31503 to MEMORY\n",
      "16/02/01 16:57:31 INFO reduce.InMemoryMapOutput: Read 31499 bytes from map-output for attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 31499, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31499\n",
      "16/02/01 16:57:31 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:57:31 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 31489 bytes\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merged 1 segments, 31499 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 1 files, 31503 bytes from disk\n",
      "16/02/01 16:57:31 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:31 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 31489 bytes\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: Records R/W=3172/1\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task:attempt_local439453704_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task attempt_local439453704_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 16:57:32 INFO output.FileOutputCommitter: Saved output of task 'attempt_local439453704_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-output/_temporary/0/task_local439453704_0001_r_000002\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: Records R/W=3172/1 > reduce\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task 'attempt_local439453704_0001_r_000002_0' done.\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: Finishing task: attempt_local439453704_0001_r_000002_0\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: Starting task: attempt_local439453704_0001_r_000003_0\n",
      "16/02/01 16:57:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:57:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:57:32 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6d0bbc23\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:57:32 INFO reduce.EventFetcher: attempt_local439453704_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:57:32 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local439453704_0001_m_000000_0 decomp: 28758 len: 28762 to MEMORY\n",
      "16/02/01 16:57:32 INFO reduce.InMemoryMapOutput: Read 28758 bytes from map-output for attempt_local439453704_0001_m_000000_0\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 28758, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->28758\n",
      "16/02/01 16:57:32 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:57:32 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:32 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28747 bytes\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: Merged 1 segments, 28758 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: Merging 1 files, 28762 bytes from disk\n",
      "16/02/01 16:57:32 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:57:32 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:57:32 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28747 bytes\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: Records R/W=2814/1\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:57:32 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task:attempt_local439453704_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task attempt_local439453704_0001_r_000003_0 is allowed to commit now\n",
      "16/02/01 16:57:32 INFO output.FileOutputCommitter: Saved output of task 'attempt_local439453704_0001_r_000003_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-output/_temporary/0/task_local439453704_0001_r_000003\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: Records R/W=2814/1 > reduce\n",
      "16/02/01 16:57:32 INFO mapred.Task: Task 'attempt_local439453704_0001_r_000003_0' done.\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: Finishing task: attempt_local439453704_0001_r_000003_0\n",
      "16/02/01 16:57:32 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:57:32 INFO mapreduce.Job: Job job_local439453704_0001 running in uber mode : false\n",
      "16/02/01 16:57:32 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:57:32 INFO mapreduce.Job: Job job_local439453704_0001 completed successfully\n",
      "16/02/01 16:57:32 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1306039\n",
      "\t\tFILE: Number of bytes written=2923813\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=437415\n",
      "\t\tHDFS: Number of bytes written=92049\n",
      "\t\tHDFS: Number of read operations=55\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=25\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1941\n",
      "\t\tMap output records=13741\n",
      "\t\tMap output bytes=108364\n",
      "\t\tMap output materialized bytes=135870\n",
      "\t\tInput split bytes=109\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3736\n",
      "\t\tReduce shuffle bytes=135870\n",
      "\t\tReduce input records=13741\n",
      "\t\tReduce output records=3736\n",
      "\t\tSpilled Records=27482\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=28\n",
      "\t\tTotal committed heap usage (bytes)=1394081792\n",
      "\tMapper Counters\n",
      "\t\tCalls=1\n",
      "\tReducer Counters\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=87483\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=36548\n",
      "16/02/01 16:57:32 INFO streaming.StreamJob: Output directory: gutenberg-output\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r gutenberg-output\n",
    "#usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib\n",
    "dataDir = \"/Users/jshanahan/Dropbox/lectures-uc-berkeley-ml-class-2015/Notebooks/WordCount\"\n",
    "\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "   -mapper WordCount/mapper.py \\\n",
    "   -reducer WordCount/reducer.py \\\n",
    "   -input historical_tours.txt \\\n",
    "   -output gutenberg-output  \\\n",
    "   -numReduceTasks 4\n",
    "   #--D mapreduce.job.reduces=2\n",
    "#-input historical_tours.txt  file on Hadoop\n",
    "\n",
    "\n",
    "#output directory on Hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in your browser launch \n",
    "# \"http://127.0.0.1:19888/jobhistory/app\"\n",
    "\n",
    "#look at the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-57-35b0e624b3a5>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-57-35b0e624b3a5>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ./sbin/mr-jobhistory-daemon.sh --config ./etc/hadoop/ start historyserver\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#this may not work for all distributions\n",
    "\n",
    "#If you are running Hadoop locally, you need to start the JobHistoryServer by running (assuming in hadoop directory)\n",
    "./sbin/mr-jobhistory-daemon.sh --config ./etc/hadoop/ start historyserver \n",
    "\n",
    "#Then you will be able to access the job information by navigating to:\n",
    "#http://192.168.0.10:19888/jobhistory/app\n",
    "\n",
    "#Click your job and then click the Counter tab on the left:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/jshanahan/Dropbox/lectures-uc-berkeley-ml-class-2015/Notebooks'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/27 20:49:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 2 items\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-01-27 20:49 gutenberg-output\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2015-02-26 19:36 historical_tours.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 15:24:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 5 items\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 15:24 gutenberg-output/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup       9528 2016-02-01 15:24 gutenberg-output/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup       8970 2016-02-01 15:24 gutenberg-output/part-00001\n",
      "-rw-r--r--   1 jshanahan supergroup       8977 2016-02-01 15:24 gutenberg-output/part-00002\n",
      "-rw-r--r--   1 jshanahan supergroup       9073 2016-02-01 15:24 gutenberg-output/part-00003\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls gutenberg-output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:58:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "windows.\t1\n",
      "winter.\t1\n",
      "wish\t2\n",
      "within\t6\n",
      "words\t1\n",
      "work.\t5\n",
      "would\t2\n",
      "wrote\t3\n",
      "your\t52\n",
      "yourself\t1\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail  gutenberg-output/part-00000 |tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  Sort the words in descreasing order of frequency, but break ties with alphabetical sorting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:58:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:58:14 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted gutenberg-output-sorted\n",
      "16/02/01 16:58:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 16:58:16 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 16:58:16 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 16:58:16 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "16/02/01 16:58:16 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "16/02/01 16:58:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local866421202_0001\n",
      "16/02/01 16:58:16 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 16:58:16 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 16:58:16 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 16:58:16 INFO mapreduce.Job: Running job: job_local866421202_0001\n",
      "16/02/01 16:58:16 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 16:58:16 INFO mapred.LocalJobRunner: Starting task: attempt_local866421202_0001_m_000000_0\n",
      "16/02/01 16:58:16 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:58:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00000:0+9528\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/mapper.py]\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 16:58:16 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: Records R/W=986/1\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:58:16 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:58:16 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: bufstart = 0; bufend = 13472; bufvoid = 104857600\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206512(104826048); length = 7885/6553600\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:58:16 INFO mapred.Task: Task:attempt_local866421202_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:58:16 INFO mapred.LocalJobRunner: processing my message...how are you\n",
      "16/02/01 16:58:16 INFO mapred.Task: Task 'attempt_local866421202_0001_m_000000_0' done.\n",
      "16/02/01 16:58:16 INFO mapred.LocalJobRunner: Finishing task: attempt_local866421202_0001_m_000000_0\n",
      "16/02/01 16:58:16 INFO mapred.LocalJobRunner: Starting task: attempt_local866421202_0001_m_000001_0\n",
      "16/02/01 16:58:16 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:58:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00003:0+9073\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/mapper.py]\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: Records R/W=931/1\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:58:16 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:58:16 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:58:16 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: bufstart = 0; bufend = 12797; bufvoid = 104857600\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206952(104827808); length = 7445/6553600\n",
      "16/02/01 16:58:16 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:58:16 INFO mapred.Task: Task:attempt_local866421202_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: processing my message...how are you\n",
      "16/02/01 16:58:17 INFO mapred.Task: Task 'attempt_local866421202_0001_m_000001_0' done.\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local866421202_0001_m_000001_0\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: Starting task: attempt_local866421202_0001_m_000002_0\n",
      "16/02/01 16:58:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:58:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00002:0+8977\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/mapper.py]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: Records R/W=921/1\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:58:17 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: bufstart = 0; bufend = 12661; bufvoid = 104857600\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26207032(104828128); length = 7365/6553600\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:58:17 INFO mapred.Task: Task:attempt_local866421202_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: processing my message...how are you\n",
      "16/02/01 16:58:17 INFO mapred.Task: Task 'attempt_local866421202_0001_m_000002_0' done.\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local866421202_0001_m_000002_0\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: Starting task: attempt_local866421202_0001_m_000003_0\n",
      "16/02/01 16:58:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:58:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/gutenberg-output/part-00001:0+8970\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/mapper.py]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: Records R/W=898/1\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: \n",
      "16/02/01 16:58:17 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: bufstart = 0; bufend = 12562; bufvoid = 104857600\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26207216(104828864); length = 7181/6553600\n",
      "16/02/01 16:58:17 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 16:58:17 INFO mapred.Task: Task:attempt_local866421202_0001_m_000003_0 is done. And is in the process of committing\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: processing my message...how are you\n",
      "16/02/01 16:58:17 INFO mapred.Task: Task 'attempt_local866421202_0001_m_000003_0' done.\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local866421202_0001_m_000003_0\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: Starting task: attempt_local866421202_0001_r_000000_0\n",
      "16/02/01 16:58:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 16:58:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 16:58:17 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@515b7cf\n",
      "16/02/01 16:58:17 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 16:58:17 INFO reduce.EventFetcher: attempt_local866421202_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 16:58:17 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local866421202_0001_m_000001_0 decomp: 16523 len: 16527 to MEMORY\n",
      "16/02/01 16:58:17 INFO reduce.InMemoryMapOutput: Read 16523 bytes from map-output for attempt_local866421202_0001_m_000001_0\n",
      "16/02/01 16:58:17 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 16523, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->16523\n",
      "16/02/01 16:58:17 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local866421202_0001_m_000000_0 decomp: 17418 len: 17422 to MEMORY\n",
      "16/02/01 16:58:17 INFO reduce.InMemoryMapOutput: Read 17418 bytes from map-output for attempt_local866421202_0001_m_000000_0\n",
      "16/02/01 16:58:17 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 17418, inMemoryMapOutputs.size() -> 2, commitMemory -> 16523, usedMemory ->33941\n",
      "16/02/01 16:58:17 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local866421202_0001_m_000003_0 decomp: 16156 len: 16160 to MEMORY\n",
      "16/02/01 16:58:17 INFO reduce.InMemoryMapOutput: Read 16156 bytes from map-output for attempt_local866421202_0001_m_000003_0\n",
      "16/02/01 16:58:17 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 16156, inMemoryMapOutputs.size() -> 3, commitMemory -> 33941, usedMemory ->50097\n",
      "16/02/01 16:58:17 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local866421202_0001_m_000002_0 decomp: 16347 len: 16351 to MEMORY\n",
      "16/02/01 16:58:17 INFO reduce.InMemoryMapOutput: Read 16347 bytes from map-output for attempt_local866421202_0001_m_000002_0\n",
      "16/02/01 16:58:17 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 16347, inMemoryMapOutputs.size() -> 4, commitMemory -> 50097, usedMemory ->66444\n",
      "16/02/01 16:58:17 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/01 16:58:17 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 16:58:17 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/01 16:58:17 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 66411 bytes\n",
      "16/02/01 16:58:17 INFO reduce.MergeManagerImpl: Merged 4 segments, 66444 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 16:58:17 INFO reduce.MergeManagerImpl: Merging 1 files, 66442 bytes from disk\n",
      "16/02/01 16:58:17 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 16:58:17 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 16:58:17 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 66427 bytes\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./WordCount/reducer.py]\n",
      "16/02/01 16:58:17 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 16:58:17 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: Records R/W=7472/1\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 16:58:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 16:58:17 INFO mapred.Task: Task:attempt_local866421202_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/01 16:58:17 INFO mapred.Task: Task attempt_local866421202_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 16:58:17 INFO output.FileOutputCommitter: Saved output of task 'attempt_local866421202_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/gutenberg-output-sorted/_temporary/0/task_local866421202_0001_r_000000\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: Records R/W=7472/1 > reduce\n",
      "16/02/01 16:58:17 INFO mapred.Task: Task 'attempt_local866421202_0001_r_000000_0' done.\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local866421202_0001_r_000000_0\n",
      "16/02/01 16:58:17 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 16:58:17 INFO mapreduce.Job: Job job_local866421202_0001 running in uber mode : false\n",
      "16/02/01 16:58:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 16:58:17 INFO mapreduce.Job: Job job_local866421202_0001 completed successfully\n",
      "16/02/01 16:58:17 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=665102\n",
      "\t\tFILE: Number of bytes written=2180726\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=128803\n",
      "\t\tHDFS: Number of bytes written=50048\n",
      "\t\tHDFS: Number of read operations=51\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=7\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3736\n",
      "\t\tMap output records=7472\n",
      "\t\tMap output bytes=51492\n",
      "\t\tMap output materialized bytes=66460\n",
      "\t\tInput split bytes=464\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7111\n",
      "\t\tReduce shuffle bytes=66460\n",
      "\t\tReduce input records=7472\n",
      "\t\tReduce output records=7111\n",
      "\t\tSpilled Records=14944\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=42\n",
      "\t\tTotal committed heap usage (bytes)=1716518912\n",
      "\tMapper Counters\n",
      "\t\tCalls=4\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=36548\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=50048\n",
      "16/02/01 16:58:17 INFO streaming.StreamJob: Output directory: gutenberg-output-sorted\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r gutenberg-output-sorted\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "   -mapper WordCount/mapper.py \\\n",
    "   -reducer WordCount/reducer.py \\\n",
    "   -input gutenberg-output \\\n",
    "   -output gutenberg-output-sorted  \\\n",
    "   -numReduceTasks 1 \\\n",
    "   -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    " -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 16:59:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 16:58 gutenberg-output-sorted/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup      50048 2016-02-01 16:58 gutenberg-output-sorted/part-00000\n",
      "ls: `gutenberg-output-sorted/part-00000.SORTED.txt': No such file or directory\n",
      "16/02/01 16:59:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "1\t4\n",
      "1\t4\n",
      "1\t3\n",
      "1\t3\n",
      "1\t3\n",
      "1\t3\n",
      "1\t3\n",
      "1\t3\n",
      "1\t3\n",
      "1\t3\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls gutenberg-output-sorted/*\n",
    "!rm -r gutenberg-output-sorted\n",
    "!hdfs dfs -copyToLocal gutenberg-output-sorted \n",
    "!sort -k2,2nr <gutenberg-output-sorted/part-00000 >gutenberg-output-sorted/part-00000.SORTED.txt\n",
    "!head -n 10 gutenberg-output-sorted/part-00000.SORTED.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sort -k2,2nr <gutenberg-output-sorted/part-00000 >gutenberg-output-sorted/part-00000.SORTED.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\t1\n",
      "3\t1\n",
      "States,\t1\n",
      "6\t1\n",
      "Start:\t1\n",
      "1\t1\n",
      "2\t1\n",
      "Students\t1\n",
      "Standish's\t1\n",
      "1\t1\n",
      "/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 gutenberg-output-sorted/part-00000\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: hadoop fs [generic options]\r\n",
      "\t[-appendToFile <localsrc> ... <dst>]\r\n",
      "\t[-cat [-ignoreCrc] <src> ...]\r\n",
      "\t[-checksum <src> ...]\r\n",
      "\t[-chgrp [-R] GROUP PATH...]\r\n",
      "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\r\n",
      "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\r\n",
      "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\r\n",
      "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\r\n",
      "\t[-count [-q] [-h] <path> ...]\r\n",
      "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\r\n",
      "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\r\n",
      "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\r\n",
      "\t[-df [-h] [<path> ...]]\r\n",
      "\t[-du [-s] [-h] <path> ...]\r\n",
      "\t[-expunge]\r\n",
      "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\r\n",
      "\t[-getfacl [-R] <path>]\r\n",
      "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\r\n",
      "\t[-getmerge [-nl] <src> <localdst>]\r\n",
      "\t[-help [cmd ...]]\r\n",
      "\t[-ls [-d] [-h] [-R] [<path> ...]]\r\n",
      "\t[-mkdir [-p] <path> ...]\r\n",
      "\t[-moveFromLocal <localsrc> ... <dst>]\r\n",
      "\t[-moveToLocal <src> <localdst>]\r\n",
      "\t[-mv <src> ... <dst>]\r\n",
      "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\r\n",
      "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\r\n",
      "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\r\n",
      "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\r\n",
      "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\r\n",
      "\t[-setfattr {-n name [-v value] | -x name} <path>]\r\n",
      "\t[-setrep [-R] [-w] <rep> <path> ...]\r\n",
      "\t[-stat [format] <path> ...]\r\n",
      "\t[-tail [-f] <file>]\r\n",
      "\t[-test -[defsz] <path>]\r\n",
      "\t[-text [-ignoreCrc] <src> ...]\r\n",
      "\t[-touchz <path> ...]\r\n",
      "\t[-usage [cmd ...]]\r\n",
      "\r\n",
      "Generic options supported are\r\n",
      "-conf <configuration file>     specify an application configuration file\r\n",
      "-D <property=value>            use value for given property\r\n",
      "-fs <local|namenode:port>      specify a namenode\r\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\r\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\r\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\r\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\r\n",
      "\r\n",
      "The general command line syntax is\r\n",
      "bin/hadoop command [genericOptions] [commandOptions]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IP Address secondary sort and paritioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ipAddresses.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ipAddresses.txt\n",
    "11.12.1.2\n",
    "11.14.2.3\n",
    "11.11.4.1\n",
    "11.12.1.1\n",
    "11.14.2.2\n",
    "11.12.2.5222\n",
    "11.9999999.2.5222\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:53:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:53:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted ipAddresses.txt\n",
      "16/02/01 12:53:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:53:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 10 items\n",
      "-rw-r--r--   1 jshanahan supergroup     888190 2016-02-01 08:32 1901\n",
      "-rw-r--r--   1 jshanahan supergroup     888978 2016-02-01 08:43 1902\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 SecondarySort\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-01-30 09:23 gutenberg-output\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2015-02-26 19:36 historical_tours.txt\n",
      "-rw-r--r--   1 jshanahan supergroup         80 2016-02-01 12:53 ipAddresses.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 12:50 myOutputDirForIPAddresses\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:36 output-secondarysort-streaming\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 12:46 stockprice\n",
      "-rw-r--r--   1 jshanahan supergroup        400 2016-02-01 12:46 stockprice.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm ipAddresses.txt \n",
    "!hdfs dfs -copyFromLocal ipAddresses.txt \n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#on AWS\n",
    "echo \"11.12.1.2\" > ipAddresses.txt\n",
    "echo \"11.14.2.3\" >> ipAddresses.txt\n",
    "echo \"11.11.4.1\" >> ipAddresses.txt\n",
    "echo \"11.12.1.1\" >> ipAddresses.txt\n",
    "echo \"11.14.2.2\" >> ipAddresses.txt\n",
    "echo \"11.12.2.5222\" >> ipAddresses.txt\n",
    "echo \"11.9999999.2.5222\" >> ipAddresses.txt\n",
    "\n",
    "hadoop jar /usr/lib/hadoop/hadoop-streaming*.jar \\\n",
    "  -input ipAddresses.txt \\\n",
    "  -output myOutputDir \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /usr/bin/wc\n",
    "hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.1-amzn-0.jar -input ipAddresses.txt -output myOutputDir -mapper /bin/cat -reducer /usr/bin/wc\n",
    "#start hadoop\n",
    "\n",
    "mapreduce.partition.keypartitioner.options\n",
    "hdfs dfs -rm -r myOutputDirForIPAddresses\n",
    "hadoop jar /usr/lib/hadoop/hadoop-streaming-2.7.1-amzn-0.jar \\\n",
    "    -D stream.map.output.field.separator=. \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "    -numReduceTasks 3 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -inputformat org.apache.hadoop.mapred.KeyValueTextInputFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.11.4.1\r\n",
      "11.12.1.1\r\n",
      "11.12.1.2\r\n",
      "11.12.2.5222\r\n",
      "11.14.2.2\r\n",
      "11.14.2.3\r\n",
      "11.9999999.2.5222\r\n"
     ]
    }
   ],
   "source": [
    "scp ipAddresses.txt\n",
    "!sort -k1,1 -k2,2 <ipAddresses.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:43:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:43:29 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted ipAddresses.txt\n",
      "16/02/01 12:43:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:43:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 10 items\n",
      "-rw-r--r--   1 jshanahan supergroup     888190 2016-02-01 08:32 1901\n",
      "-rw-r--r--   1 jshanahan supergroup     888978 2016-02-01 08:43 1902\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 SecondarySort\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-01-30 09:23 gutenberg-output\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2015-02-26 19:36 historical_tours.txt\n",
      "-rw-r--r--   1 jshanahan supergroup         87 2016-02-01 12:43 ipAddresses.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 12:32 myOutputDirForIPAddresses\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:36 output-secondarysort-streaming\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:38 stockprice\n",
      "-rw-r--r--   1 jshanahan supergroup        365 2016-02-01 09:37 stockprice.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm ipAddresses.txt \n",
    "!hdfs dfs -copyFromLocal ipAddresses.txt \n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ipAddressMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ipAddressMapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    f1, f2, f3, f4 = line.split('.')\n",
    "    # get date from unix time\n",
    "    print '%s\\t%s\\t%s\\t%s mapper' % (f1, f2, f3, f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ipAddressReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ipAddressReducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    f1, f2, f3, f4 = line.split('\\t')\n",
    "    # get date from unix time\n",
    "    print '%s.%s.%s.%s jimi' % (f1, f2, f3, f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.12.1.2 jimi\r\n",
      "11.14.2.3 jimi\r\n",
      "11.11.4.1 jimi\r\n",
      "11.12.1.1 jimi\r\n",
      "11.14.2.2 jimi\r\n",
      "11.12.2.5222 jimi\r\n",
      "11.9999999.2.5222 jimi\r\n"
     ]
    }
   ],
   "source": [
    "!cat ipAddresses.txt| python ipAddressReducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group1\n"
     ]
    }
   ],
   "source": [
    "group1 = \"abcdefghijklm\"\n",
    "group2 = \"nopqrstuvwxyz\"\n",
    "\n",
    "chars=\"Mnabcb\"\n",
    "firstChar = chars[0].lower()\n",
    "if firstChar in group1:\n",
    "    print \"group1\"\n",
    "elif firstChar in group2:\n",
    "    print \"group2\"\n",
    "else:\n",
    "    print \"group3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:53:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:53:46 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 12:53:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:53:48 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:53:48 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:53:48 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:53:49 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:53:49 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:53:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1733440059_0001\n",
      "16/02/01 12:53:49 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:53:49 INFO mapreduce.Job: Running job: job_local1733440059_0001\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: \n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: bufstart = 0; bufend = 137; bufvoid = 104857600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 12:53:49 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_m_000000_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_r_000000_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4dcd971d\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: attempt_local1733440059_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:53:49 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1733440059_0001_m_000000_0 decomp: 70 len: 74 to MEMORY\n",
      "16/02/01 12:53:49 INFO reduce.InMemoryMapOutput: Read 70 bytes from map-output for attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 70, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->70\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 60 bytes\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 70 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 1 files, 74 bytes from disk\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 60 bytes\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task attempt_local1733440059_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 12:53:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1733440059_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1733440059_0001_r_000000\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_r_000000_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_r_000000_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_r_000001_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6d946c52\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: attempt_local1733440059_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:53:49 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1733440059_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 12:53:49 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task attempt_local1733440059_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 12:53:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1733440059_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1733440059_0001_r_000001\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_r_000001_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_r_000001_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Starting task: attempt_local1733440059_0001_r_000002_0\n",
      "16/02/01 12:53:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:53:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:53:49 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@38a63406\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: attempt_local1733440059_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:53:49 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1733440059_0001_m_000000_0 decomp: 45 len: 49 to MEMORY\n",
      "16/02/01 12:53:49 INFO reduce.InMemoryMapOutput: Read 45 bytes from map-output for attempt_local1733440059_0001_m_000000_0\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 45, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->45\n",
      "16/02/01 12:53:49 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 35 bytes\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merged 1 segments, 45 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 1 files, 49 bytes from disk\n",
      "16/02/01 12:53:49 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:53:49 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 35 bytes\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task:attempt_local1733440059_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task attempt_local1733440059_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 12:53:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1733440059_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1733440059_0001_r_000002\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:53:49 INFO mapred.Task: Task 'attempt_local1733440059_0001_r_000002_0' done.\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local1733440059_0001_r_000002_0\n",
      "16/02/01 12:53:49 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 12:53:50 INFO mapreduce.Job: Job job_local1733440059_0001 running in uber mode : false\n",
      "16/02/01 12:53:50 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 12:53:50 INFO mapreduce.Job: Job job_local1733440059_0001 completed successfully\n",
      "16/02/01 12:53:50 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=422129\n",
      "\t\tFILE: Number of bytes written=1498547\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=320\n",
      "\t\tHDFS: Number of bytes written=219\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=137\n",
      "\t\tMap output materialized bytes=169\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=169\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1440743424\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=80\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=101\n",
      "16/02/01 12:53:50 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "#Partition into 3 reducers (the first 2 fields are used as keys for partition)\n",
    "#Sorting within each partition for the reducer(all 4 fields used for sorting)\n",
    "#In the above example, \"-D stream.map.output.field.separator=.\" specifies \".\" as the field separator \n",
    "#for the map outputs, and the prefix up to the fourth \".\" in a line will be the key and the rest of \n",
    "#the line (excluding the fourth \".\") will be the value. If a line has less than four \".\"s, then the \n",
    "#whole line will be the key and the value will be an empty Text object (like the one created by new Text(\"\")).\n",
    "\n",
    "#Similarly, you can use \"-D stream.reduce.output.field.separator=SEP\" and \"-D stream.num.reduce.output.fields=NUM\"\n",
    "#to specify the nth field separator in a line of the reduce outputs as the separator between the key and the value.\n",
    "\n",
    "#Similarly, you can specify \"stream.map.input.field.separator\" and \"stream.reduce.input.field.separator\" as \n",
    "#the input separator for Map/Reduce inputs. By default the separator is the tab character.\n",
    "\n",
    "#record 11.12.1.2 is treated as a key only with no value\n",
    "\n",
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,2 \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "    -numReduceTasks 3 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:56:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:56:59 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 12:57:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:57:00 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:57:00 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:57:00 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:57:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:57:00 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "16/02/01 12:57:01 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1377900324_0001\n",
      "16/02/01 12:57:01 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:57:01 INFO mapreduce.Job: Running job: job_local1377900324_0001\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: numReduceTasks: 5\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressMapper.py]\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: \n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: bufstart = 0; bufend = 137; bufvoid = 104857600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 12:57:01 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=7/1\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_m_000000_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000000_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@450849b9\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 22 len: 26 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 22 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 22, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->22\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 22 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 26 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000000\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000000_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000000_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000001_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1c2c6e85\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 65 len: 69 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 65 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 65, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->65\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 46 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 65 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 69 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 46 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000001\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000001_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000001_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000002_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@449a3b50\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 30 len: 34 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 30 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 30, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->30\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 30 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 34 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000002\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000002_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000002_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000003_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6665f62e\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 23 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 23 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: Records R/W=2/1\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000003_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000003_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000003\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000003_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000003_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1377900324_0001_r_000004_0\n",
      "16/02/01 12:57:01 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:57:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:57:01 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3b5a3cf4\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: attempt_local1377900324_0001_r_000004_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:57:01 INFO reduce.LocalFetcher: localfetcher#5 about to shuffle output of map attempt_local1377900324_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 12:57:01 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1377900324_0001_m_000000_0\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/01 12:57:01 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk\n",
      "16/02/01 12:57:01 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:57:01 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./ipAddressReducer.py]\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:57:01 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task:attempt_local1377900324_0001_r_000004_0 is done. And is in the process of committing\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task attempt_local1377900324_0001_r_000004_0 is allowed to commit now\n",
      "16/02/01 12:57:01 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1377900324_0001_r_000004_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local1377900324_0001_r_000004\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 12:57:01 INFO mapred.Task: Task 'attempt_local1377900324_0001_r_000004_0' done.\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: Finishing task: attempt_local1377900324_0001_r_000004_0\n",
      "16/02/01 12:57:01 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 12:57:02 INFO mapreduce.Job: Job job_local1377900324_0001 running in uber mode : false\n",
      "16/02/01 12:57:02 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 12:57:02 INFO mapreduce.Job: Job job_local1377900324_0001 completed successfully\n",
      "16/02/01 12:57:02 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=635229\n",
      "\t\tFILE: Number of bytes written=2265774\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=480\n",
      "\t\tHDFS: Number of bytes written=588\n",
      "\t\tHDFS: Number of read operations=75\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=36\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=137\n",
      "\t\tMap output materialized bytes=181\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=7\n",
      "\t\tReduce shuffle bytes=181\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=7\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=4\n",
      "\t\tTotal committed heap usage (bytes)=2205679616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=80\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=172\n",
      "16/02/01 12:57:02 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D mapred.text.key.partitioner.options=-k1,2 \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper ipAddressMapper.py \\\n",
    "    -reducer ipAddressReducer.py \\\n",
    "    -numReduceTasks 5 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sorting within each partition for the reducer(all 4 fields used for sorting)\n",
    "# desired output \n",
    "11.11.4.1\n",
    "-----------\n",
    "11.12.1.1\n",
    "11.12.1.2\n",
    "-----------\n",
    "11.14.2.2\n",
    "11.14.2.3\n",
    "11.14.2.5222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:57:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 6 items\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 12:57 myOutputDirForIPAddresses/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup         23 2016-02-01 12:57 myOutputDirForIPAddresses/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup         72 2016-02-01 12:57 myOutputDirForIPAddresses/part-00001\n",
      "-rw-r--r--   1 jshanahan supergroup         31 2016-02-01 12:57 myOutputDirForIPAddresses/part-00002\n",
      "-rw-r--r--   1 jshanahan supergroup         46 2016-02-01 12:57 myOutputDirForIPAddresses/part-00003\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 12:57 myOutputDirForIPAddresses/part-00004\n",
      "16/02/01 12:57:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.11.4.1 mapper jimi\t\n",
      "----\n",
      "16/02/01 12:57:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.12.1.1 mapper jimi\t\n",
      "11.12.1.2 mapper jimi\t\n",
      "11.12.2.5222 mapper jimi\t\n",
      "----\n",
      "16/02/01 12:57:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "11.9999999.2.5222 mapper jimi\t\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls myOutputDirForIPAddresses\n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00000\n",
    "!echo \"----\" \n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00001\n",
    "!echo \"----\" \n",
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 10:08:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 10:08:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 10:08:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 10:08:43 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 10:08:43 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 10:08:43 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 10:08:43 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 10:08:43 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 10:08:43 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/02/01 10:08:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local145025840_0001\n",
      "16/02/01 10:08:44 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 10:08:44 INFO mapreduce.Job: Running job: job_local145025840_0001\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+59\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: \n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: bufstart = 0; bufend = 108; bufvoid = 104857600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214376(104857504); length = 21/6553600\n",
      "16/02/01 10:08:44 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+59\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_m_000000_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_r_000000_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5748fba6\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: attempt_local145025840_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 10:08:44 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local145025840_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 10:08:44 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task attempt_local145025840_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 10:08:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local145025840_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local145025840_0001_r_000000\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_r_000000_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_r_000000_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_r_000001_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@576ca50f\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: attempt_local145025840_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 10:08:44 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local145025840_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 10:08:44 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task attempt_local145025840_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 10:08:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local145025840_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local145025840_0001_r_000001\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_r_000001_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_r_000001_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Starting task: attempt_local145025840_0001_r_000002_0\n",
      "16/02/01 10:08:44 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 10:08:44 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 10:08:44 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@50bda288\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: attempt_local145025840_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 10:08:44 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local145025840_0001_m_000000_0 decomp: 42 len: 46 to MEMORY\n",
      "16/02/01 10:08:44 INFO reduce.InMemoryMapOutput: Read 42 bytes from map-output for attempt_local145025840_0001_m_000000_0\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 42, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->42\n",
      "16/02/01 10:08:44 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merged 1 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 1 files, 46 bytes from disk\n",
      "16/02/01 10:08:44 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 10:08:44 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 32 bytes\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task:attempt_local145025840_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task attempt_local145025840_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 10:08:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_local145025840_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/myOutputDirForIPAddresses/_temporary/0/task_local145025840_0001_r_000002\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/01 10:08:44 INFO mapred.Task: Task 'attempt_local145025840_0001_r_000002_0' done.\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: Finishing task: attempt_local145025840_0001_r_000002_0\n",
      "16/02/01 10:08:44 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 10:08:45 INFO mapreduce.Job: Job job_local145025840_0001 running in uber mode : false\n",
      "16/02/01 10:08:45 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 10:08:45 INFO mapreduce.Job: Job job_local145025840_0001 completed successfully\n",
      "16/02/01 10:08:45 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=421940\n",
      "\t\tFILE: Number of bytes written=1498172\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=236\n",
      "\t\tHDFS: Number of bytes written=153\n",
      "\t\tHDFS: Number of read operations=38\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=16\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=6\n",
      "\t\tMap output bytes=108\n",
      "\t\tMap output materialized bytes=138\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=138\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=12\n",
      "\t\tShuffled Maps =3\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=1333788672\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=59\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=77\n",
      "16/02/01 10:08:45 INFO streaming.StreamJob: Output directory: myOutputDirForIPAddresses\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    " -D stream.map.output.field.separator=. \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "    -D map.output.key.field.separator=. \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1n -k2nr\" \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer org.apache.hadoop.mapred.lib.IdentityReducer \\\n",
    "    -numReduceTasks 3 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:55:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:55:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted myOutputDirForIPAddresses\n",
      "16/02/01 12:55:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:55:35 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:55:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:55:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:55:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:55:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:55:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local72918067_0001\n",
      "16/02/01 12:55:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:55:35 INFO mapreduce.Job: Running job: job_local72918067_0001\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:55:35 INFO mapred.LocalJobRunner: Starting task: attempt_local72918067_0001_m_000000_0\n",
      "16/02/01 12:55:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:55:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/ipAddresses.txt:0+80\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:55:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:55:36 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:55:36 WARN mapred.LocalJobRunner: job_local72918067_0001\n",
      "java.lang.Exception: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\n",
      "Caused by: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.Text, received org.apache.hadoop.io.LongWritable\n",
      "\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1069)\n",
      "\tat org.apache.hadoop.mapred.MapTask$OldOutputCollector.collect(MapTask.java:607)\n",
      "\tat org.apache.hadoop.mapred.lib.IdentityMapper.map(IdentityMapper.java:43)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "16/02/01 12:55:36 INFO mapreduce.Job: Job job_local72918067_0001 running in uber mode : false\n",
      "16/02/01 12:55:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/01 12:55:36 INFO mapreduce.Job: Job job_local72918067_0001 failed with state FAILED due to: NA\n",
      "16/02/01 12:55:36 INFO mapreduce.Job: Counters: 0\n",
      "16/02/01 12:55:36 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r myOutputDirForIPAddresses \n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    " -D stream.map.output.field.separator=. \\\n",
    "    -D stream.num.map.output.key.fields=4 \\\n",
    "     -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1n -k2nr\" \\\n",
    "    -input ipAddresses.txt \\\n",
    "    -output myOutputDirForIPAddresses \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer ipAddressReducer.py \\\n",
    "    -numReduceTasks 3 \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \n",
    "\n",
    "   -D map.output.key.field.separator=. \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 10:09:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "0\t11.12.1.2\n",
      "30\t11.12.1.1\n",
      "10\t11.14.2.3\n",
      "40\t11.14.2.2\n",
      "20\t11.11.4.1\n",
      "50\t11.14.2.5\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat myOutputDirForIPAddresses/part-0000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondary Sort: Stock Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stockprice.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile stockprice.txt\n",
    "Google,2015-08-06,647.32\n",
    "Apple,2015-08-01,117.83\n",
    "Facebook,2015-08-05,92.67\n",
    "Facebook,2015-08-01,90.96\n",
    "Oracle,2015-08-04,38.55\n",
    "Apple,2015-08-04,113.77\n",
    "Google,2015-08-05,677.95\n",
    "Facebook,2015-08-08,90.43\n",
    "Oracle,2015-08-03,35.78\n",
    "Apple,2015-08-11,110.09\n",
    "Oracle,2015-08-07,39.67\n",
    "Google,2015-08-09,656.63\n",
    "ABXXXX,2015-08-07,39.67\n",
    "ABXXXX,2015-08-09,656.63\n",
    "Google,2000-08-09,0\n",
    "ABXXXX,2015-08-08,6569999999999.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stockPriceMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stockPriceMapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line\n",
    "    name, date, price= line.split(\",\")\n",
    "    # unix time is for secondary sort\n",
    "    unix_time = datetime.datetime.strptime(date, '%Y-%m-%d').strftime(\"%s\")\n",
    "    # output each record\n",
    "    print '%s\\t%s\\t%s' % (name, unix_time, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stockPriceReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile stockPriceReducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # parse the input we got from mapper.py\n",
    "    name, unix_time, price = line.split('\\t')\n",
    "    # get date from unix time\n",
    "    date = datetime.datetime.fromtimestamp(int(unix_time)).strftime('%Y-%m-%d')\n",
    "    print '%s\\t%s\\t%s' % (name, date, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABXXXX\t1438930800\t39.67\r\n",
      "ABXXXX\t1439103600\t656.63\r\n",
      "Apple\t1438412400\t117.83\r\n",
      "Apple\t1438671600\t113.77\r\n",
      "Apple\t1439276400\t110.09\r\n",
      "Facebook\t1438412400\t90.96\r\n",
      "Facebook\t1438758000\t92.67\r\n",
      "Facebook\t1439017200\t90.43\r\n",
      "Google\t1438758000\t677.95\r\n",
      "Google\t1438844400\t647.32\r\n",
      "Google\t1439103600\t656.63\r\n",
      "Google\t965804400\t0\r\n",
      "Oracle\t1438585200\t35.78\r\n",
      "Oracle\t1438671600\t38.55\r\n",
      "Oracle\t1438930800\t39.67\r\n"
     ]
    }
   ],
   "source": [
    "#sort stock name increasing, and the do a secondary sort on key2=unix date, NUMERICALLY\n",
    "!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2n   #Unix sort\n",
    "#WHY does it NOT work? It takes the whole as a key and sorts alphanumerically\n",
    "\n",
    "#very different to the following line -k1,1 sort \n",
    "#!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2,2n   #Unix sort BAD SORT key for key 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABXXXX\t1438930800\t39.67\r\n",
      "ABXXXX\t1439103600\t656.63\r\n",
      "Apple\t1438412400\t117.83\r\n",
      "Apple\t1438671600\t113.77\r\n",
      "Apple\t1439276400\t110.09\r\n",
      "Facebook\t1438412400\t90.96\r\n",
      "Facebook\t1438758000\t92.67\r\n",
      "Facebook\t1439017200\t90.43\r\n",
      "Google\t965804400\t0\r\n",
      "Google\t1438758000\t677.95\r\n",
      "Google\t1438844400\t647.32\r\n",
      "Google\t1439103600\t656.63\r\n",
      "Oracle\t1438585200\t35.78\r\n",
      "Oracle\t1438671600\t38.55\r\n",
      "Oracle\t1438930800\t39.67\r\n"
     ]
    }
   ],
   "source": [
    "#sort stock name increasing, and the do a secondary sort on key2=unix date, NUMERICALLY\n",
    "!cat stockprice.txt| python stockPriceMapper.py |sort -k1,1 -k2,2n   #Unix sort\n",
    "#this is CORRECT\n",
    "#very different to the following line -k1,1 sort \n",
    "#!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2,2n   #Unix sort BAD SORT key for key 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABXXXX\t1438930800\t39.67\r\n",
      "ABXXXX\t1439103600\t656.63\r\n",
      "Apple\t1438412400\t117.83\r\n",
      "Apple\t1438671600\t113.77\r\n",
      "Apple\t1439276400\t110.09\r\n",
      "Facebook\t1438412400\t90.96\r\n",
      "Facebook\t1438758000\t92.67\r\n",
      "Facebook\t1439017200\t90.43\r\n",
      "Google\t1438758000\t677.95\r\n",
      "Google\t1438844400\t647.32\r\n",
      "Google\t1439103600\t656.63\r\n",
      "Google\t965804400\t0\r\n",
      "Oracle\t1438585200\t35.78\r\n",
      "Oracle\t1438671600\t38.55\r\n",
      "Oracle\t1438930800\t39.67\r\n"
     ]
    }
   ],
   "source": [
    "#!cat stockprice.txt| python stockPriceMapper.py |sort -k1,1 -k2,2n   #Unix sort\n",
    "#very different to the following line -k1,1 sort \n",
    "!cat stockprice.txt| python stockPriceMapper.py |sort -k1 -k2,2n   #Unix sort BAD SORT key for key 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:46:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:46:19 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted stockprice.txt\n",
      "16/02/01 12:46:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm stockprice.txt\n",
    "!hdfs dfs -copyFromLocal stockprice.txt /user/jshanahan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:46:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:46:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted stockprice\n",
      "16/02/01 12:46:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 12:46:42 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 12:46:42 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 12:46:42 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 12:46:42 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/01 12:46:42 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/01 12:46:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local249202583_0001\n",
      "16/02/01 12:46:43 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 12:46:43 INFO mapreduce.Job: Running job: job_local249202583_0001\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Starting task: attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:46:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/stockprice.txt:0+400\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: numReduceTasks: 2\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./stockPriceMapper.py]\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: Records R/W=16/1\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: \n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: bufstart = 0; bufend = 416; bufvoid = 104857600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214336(104857344); length = 61/6553600\n",
      "16/02/01 12:46:43 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task:attempt_local249202583_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Records R/W=16/1\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task 'attempt_local249202583_0001_m_000000_0' done.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Starting task: attempt_local249202583_0001_r_000000_0\n",
      "16/02/01 12:46:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:46:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:46:43 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5ebfefd\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: attempt_local249202583_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:46:43 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local249202583_0001_m_000000_0 decomp: 251 len: 255 to MEMORY\n",
      "16/02/01 12:46:43 INFO reduce.InMemoryMapOutput: Read 251 bytes from map-output for attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 251, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->251\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 225 bytes\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merged 1 segments, 251 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 1 files, 255 bytes from disk\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 225 bytes\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./stockPriceReducer.py]\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: Records R/W=9/1\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task:attempt_local249202583_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task attempt_local249202583_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 12:46:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_local249202583_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/stockprice/_temporary/0/task_local249202583_0001_r_000000\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Records R/W=9/1 > reduce\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task 'attempt_local249202583_0001_r_000000_0' done.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local249202583_0001_r_000000_0\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Starting task: attempt_local249202583_0001_r_000001_0\n",
      "16/02/01 12:46:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 12:46:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 12:46:43 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7b31deb3\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: attempt_local249202583_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 12:46:43 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local249202583_0001_m_000000_0 decomp: 201 len: 205 to MEMORY\n",
      "16/02/01 12:46:43 INFO reduce.InMemoryMapOutput: Read 201 bytes from map-output for attempt_local249202583_0001_m_000000_0\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 201, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->201\n",
      "16/02/01 12:46:43 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 174 bytes\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merged 1 segments, 201 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 1 files, 205 bytes from disk\n",
      "16/02/01 12:46:43 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 12:46:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 174 bytes\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./stockPriceReducer.py]\n",
      "16/02/01 12:46:43 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 12:46:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task:attempt_local249202583_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task attempt_local249202583_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 12:46:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_local249202583_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/stockprice/_temporary/0/task_local249202583_0001_r_000001\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Records R/W=7/1 > reduce\n",
      "16/02/01 12:46:43 INFO mapred.Task: Task 'attempt_local249202583_0001_r_000001_0' done.\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local249202583_0001_r_000001_0\n",
      "16/02/01 12:46:43 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 12:46:44 INFO mapreduce.Job: Job job_local249202583_0001 running in uber mode : false\n",
      "16/02/01 12:46:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 12:46:44 INFO mapreduce.Job: Job job_local249202583_0001 completed successfully\n",
      "16/02/01 12:46:44 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=317416\n",
      "\t\tFILE: Number of bytes written=1132117\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1200\n",
      "\t\tHDFS: Number of bytes written=623\n",
      "\t\tHDFS: Number of read operations=24\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=16\n",
      "\t\tMap output records=16\n",
      "\t\tMap output bytes=416\n",
      "\t\tMap output materialized bytes=460\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=16\n",
      "\t\tReduce shuffle bytes=460\n",
      "\t\tReduce input records=16\n",
      "\t\tReduce output records=16\n",
      "\t\tSpilled Records=32\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=781713408\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=400\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=401\n",
      "16/02/01 12:46:44 INFO streaming.StreamJob: Output directory: stockprice\n"
     ]
    }
   ],
   "source": [
    "#NOTE \"-k1,1 -k2,2nr\" -k1,1  is redundanct\n",
    "!hdfs dfs -rm -r stockprice\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "        -D stream.num.map.output.key.fields=3 \\\n",
    "        -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "        -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "        -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2nr\" \\\n",
    "        -mapper stockPriceMapper.py \\\n",
    "        -reducer stockPriceReducer.py \\\n",
    "        -input stockprice.txt -output stockprice \\\n",
    "        -numReduceTasks 2 \\\n",
    "        -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 12:46:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 12:46 stockprice/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup        222 2016-02-01 12:46 stockprice/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup        179 2016-02-01 12:46 stockprice/part-00001\n",
      "16/02/01 12:46:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Apple\t2015-08-11\t110.09\n",
      "Apple\t2015-08-04\t113.77\n",
      "Apple\t2015-08-01\t117.83\n",
      "Facebook\t2015-08-08\t90.43\n",
      "Facebook\t2015-08-05\t92.67\n",
      "Facebook\t2015-08-01\t90.96\n",
      "Oracle\t2015-08-07\t39.67\n",
      "Oracle\t2015-08-04\t38.55\n",
      "Oracle\t2015-08-03\t35.78\n",
      "ABXXXX\t2015-08-09\t656.63\n",
      "ABXXXX\t2015-08-08\t6569999999999.63\n",
      "ABXXXX\t2015-08-07\t39.67\n",
      "Google\t2015-08-09\t656.63\n",
      "Google\t2015-08-06\t647.32\n",
      "Google\t2015-08-05\t677.95\n",
      "Google\t2000-08-09\t0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls stockprice\n",
    "!hdfs dfs -cat stockprice/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:39:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "ABXXXX\t2015-08-09\t656.63\n",
      "ABXXXX\t2015-08-07\t39.67\n",
      "Google\t2015-08-09\t656.63\n",
      "Google\t2015-08-06\t647.32\n",
      "Google\t2015-08-05\t677.95\n",
      "Google\t2000-08-09\t0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat stockprice/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/27 22:39:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/27 22:39:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted stockprice\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r stockprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -r stockprice\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1n -k2nr\" \\\n",
    "    -files secondary_sort_map.py,secondary_sort_reduce.py \\\n",
    "    -input SecondarySort \\\n",
    "    -output output-secondarysort-streaming \\\n",
    "    -mapper secondary_sort_map.py \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -reducer secondary_sort_reduce.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/27 22:39:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 3 items\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-01-27 22:39 stockprice/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup         75 2016-01-27 22:39 stockprice/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup        222 2016-01-27 22:39 stockprice/part-00001\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls stockprice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/27 22:40:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Apple\t2015-08-11\t110.09\n",
      "Apple\t2015-08-04\t113.77\n",
      "Apple\t2015-08-01\t117.83\n",
      "Facebook\t2015-08-08\t90.43\n",
      "Facebook\t2015-08-01\t90.96\n",
      "Facebook\t2015-08-05\t92.67\n",
      "Oracle\t2015-08-07\t39.67\n",
      "Oracle\t2015-08-03\t35.78\n",
      "Oracle\t2015-08-04\t38.55\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat stockprice/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary Sort from Tom White (Chapter 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SecondarySort/otherYears.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile SecondarySort/otherYears.txt\n",
    "0029029070999991903010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991904010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n",
    "0029029070999991907010120004+64333+023450FM-12+000599999V0209991C000019999999N0000001N9-00941+99999102001ADDGF108991999999999999999999\n",
    "002902907099999190610106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991906010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n",
    "0029029070999991906010120004+64333+023450FM-12+000599999V0209991C000019999999N0000001N9-00941+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991907010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "0029029070999991907010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:23:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "copyFromLocal: `SecondarySort/1901': File exists\n",
      "copyFromLocal: `SecondarySort/1902': File exists\n",
      "copyFromLocal: `SecondarySort/otherYears.txt': File exists\n",
      "16/02/01 09:23:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Found 9 items\n",
      "-rw-r--r--   1 jshanahan supergroup     888190 2016-02-01 08:32 1901\n",
      "-rw-r--r--   1 jshanahan supergroup     888978 2016-02-01 08:43 1902\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 SecondarySort\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-01-30 09:23 gutenberg-output\n",
      "-rw-r--r--   1 jshanahan supergroup      87483 2015-02-26 19:36 historical_tours.txt\n",
      "-rw-r--r--   1 jshanahan supergroup         59 2016-01-31 20:01 ipAddresses.txt\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-01-31 20:36 myOutputDirForIPAddresses\n",
      "drwxr-xr-x   - jshanahan supergroup          0 2016-02-01 09:21 output-secondarysort-streaming\n",
      "-rw-r--r--   1 jshanahan supergroup        365 2016-02-01 08:55 stockprice.txt\n"
     ]
    }
   ],
   "source": [
    "#copy year 1901 and 1902 to HDFS\n",
    "#val[15:19], int(val[87:92]), val[92:93]\n",
    "# year, temp, q\n",
    "#002902907099999  1901  010106004+64333+023450FM-12+000599999V0202701N015919999999N0000001N9-00781+99999102001ADDGF108991999999999999999999\n",
    "#0029029070999991901010113004+64333+023450FM-12+000599999V0202901N008219999999N0000001N9-00721+99999102001ADDGF104991999999999999999999\n",
    "#0029029070999991901010120004+64333+023450FM-12+000599999V0209991C000019999999N0000001N9-00941+99999102001ADDGF108991999999999999999999\n",
    "\n",
    "\n",
    "!hdfs dfs -copyFromLocal SecondarySort\n",
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting secondary_sort_map.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile secondary_sort_map.py\n",
    "#!/usr/bin/env python\n",
    "#Example 9-7. Map function for secondary sort in Python\n",
    "import re\n",
    "import sys\n",
    "for line in sys.stdin:\n",
    "    val = line.strip()\n",
    "    (year, temp, q) = (val[15:19], int(val[87:92]), val[92:93])\n",
    "    if temp == 9999:\n",
    "        sys.stderr.write(\"reporter:counter:Temperature,Missing,1\\n\")\n",
    "    elif re.match(\"[01459]\", q):\n",
    "        print \"%s\\t%s\" % (year, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting secondary_sort_reduce.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile secondary_sort_reduce.py\n",
    "#!/usr/bin/env python\n",
    "#Example 9-8. Reducer function for secondary sort in Python\n",
    "import sys\n",
    "last_group = None\n",
    "for line in sys.stdin:\n",
    "    val = line.strip()\n",
    "    (year, temp) = val.split(\"\\t\")\n",
    "    group = year\n",
    "    if last_group != group:   #print the first record ONLY for each year; skip all other records for that year\n",
    "        print val\n",
    "        last_group = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1902\t-100\r\n"
     ]
    }
   ],
   "source": [
    "!cat SecondarySort/1902| python secondary_sort_map.py |sort|python secondary_sort_reduce.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:36:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 09:36:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted output-secondarysort-streaming\n",
      "16/02/01 09:36:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 09:36:11 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/01 09:36:11 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/01 09:36:11 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/01 09:36:11 INFO mapred.FileInputFormat: Total input paths to process : 3\n",
      "16/02/01 09:36:11 INFO mapreduce.JobSubmitter: number of splits:3\n",
      "16/02/01 09:36:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local806998882_0001\n",
      "16/02/01 09:36:11 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/secondary_sort_map.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454348171875/secondary_sort_map.py\n",
      "16/02/01 09:36:11 INFO mapred.LocalDistributedCacheManager: Localized file:/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/secondary_sort_reduce.py as file:/usr/local/Cellar/hadoop/hdfs/tmp/mapred/local/1454348171876/secondary_sort_reduce.py\n",
      "16/02/01 09:36:12 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/01 09:36:12 INFO mapreduce.Job: Running job: job_local806998882_0001\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/SecondarySort/1902:0+888978\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_map.py]\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=2903/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: \n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufend = 63107; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26188140(104752560); length = 26257/6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=2903/1\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_m_000000_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/SecondarySort/1901:0+888190\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_map.py]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=2936/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: \n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufend = 63794; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26188144(104752576); length = 26253/6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=2936/1\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_m_000001_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/jshanahan/SecondarySort/otherYears.txt:0+1078\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: numReduceTasks: 3\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_map.py]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=8/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: \n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Spilling map output\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: bufstart = 0; bufend = 70; bufvoid = 104857600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/02/01 09:36:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=8/1\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_m_000002_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_r_000000_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@344c22e1\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=352321536, maxSingleShuffleLimit=88080384, mergeThreshold=232532224, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: attempt_local806998882_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local806998882_0001_m_000001_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local806998882_0001_m_000002_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->4\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local806998882_0001_m_000000_0 decomp: 76239 len: 76243 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 76239 bytes from map-output for attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 76239, inMemoryMapOutputs.size() -> 3, commitMemory -> 4, usedMemory ->76243\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76227 bytes\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merged 3 segments, 76243 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 1 files, 76243 bytes from disk\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76227 bytes\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_reduce.py]\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=6565/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task attempt_local806998882_0001_r_000000_0 is allowed to commit now\n",
      "16/02/01 09:36:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local806998882_0001_r_000000_0' to hdfs://localhost:9000/user/jshanahan/output-secondarysort-streaming/_temporary/0/task_local806998882_0001_r_000000\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=6565/1 > reduce\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_r_000000_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_r_000000_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_r_000001_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@625b6997\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=368102592, maxSingleShuffleLimit=92025648, mergeThreshold=242947728, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: attempt_local806998882_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local806998882_0001_m_000001_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local806998882_0001_m_000002_0 decomp: 38 len: 42 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 38 bytes from map-output for attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 38, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->40\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local806998882_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 40, usedMemory ->42\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merged 3 segments, 42 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 1 files, 42 bytes from disk\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 27 bytes\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_reduce.py]\n",
      "16/02/01 09:36:12 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task attempt_local806998882_0001_r_000001_0 is allowed to commit now\n",
      "16/02/01 09:36:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local806998882_0001_r_000001_0' to hdfs://localhost:9000/user/jshanahan/output-secondarysort-streaming/_temporary/0/task_local806998882_0001_r_000001\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_r_000001_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_r_000001_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Starting task: attempt_local806998882_0001_r_000002_0\n",
      "16/02/01 09:36:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/01 09:36:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/01 09:36:12 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@34986bf\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=368102592, maxSingleShuffleLimit=92025648, mergeThreshold=242947728, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: attempt_local806998882_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local806998882_0001_m_000001_0 decomp: 76924 len: 76928 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 76924 bytes from map-output for attempt_local806998882_0001_m_000001_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 76924, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->76924\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local806998882_0001_m_000002_0 decomp: 50 len: 54 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 50 bytes from map-output for attempt_local806998882_0001_m_000002_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 50, inMemoryMapOutputs.size() -> 2, commitMemory -> 76924, usedMemory ->76974\n",
      "16/02/01 09:36:12 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local806998882_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/01 09:36:12 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local806998882_0001_m_000000_0\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 76974, usedMemory ->76976\n",
      "16/02/01 09:36:12 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 76951 bytes\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merged 3 segments, 76976 bytes to disk to satisfy reduce memory limit\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 1 files, 76976 bytes from disk\n",
      "16/02/01 09:36:12 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/01 09:36:12 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76960 bytes\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/jshanahan/Dropbox/Lectures-UC-Berkeley-ML-Class-2015/Notebooks/./secondary_sort_reduce.py]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: Records R/W=6568/1\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/01 09:36:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task:attempt_local806998882_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: 3 / 3 copied.\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task attempt_local806998882_0001_r_000002_0 is allowed to commit now\n",
      "16/02/01 09:36:12 INFO output.FileOutputCommitter: Saved output of task 'attempt_local806998882_0001_r_000002_0' to hdfs://localhost:9000/user/jshanahan/output-secondarysort-streaming/_temporary/0/task_local806998882_0001_r_000002\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Records R/W=6568/1 > reduce\n",
      "16/02/01 09:36:12 INFO mapred.Task: Task 'attempt_local806998882_0001_r_000002_0' done.\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local806998882_0001_r_000002_0\n",
      "16/02/01 09:36:12 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/01 09:36:13 INFO mapreduce.Job: Job job_local806998882_0001 running in uber mode : false\n",
      "16/02/01 09:36:13 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/01 09:36:13 INFO mapreduce.Job: Job job_local806998882_0001 completed successfully\n",
      "16/02/01 09:36:13 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1273416\n",
      "\t\tFILE: Number of bytes written=3433695\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=9779130\n",
      "\t\tHDFS: Number of bytes written=94\n",
      "\t\tHDFS: Number of read operations=72\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=18\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=13138\n",
      "\t\tMap output records=13136\n",
      "\t\tMap output bytes=126971\n",
      "\t\tMap output materialized bytes=153297\n",
      "\t\tInput split bytes=331\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=11809\n",
      "\t\tReduce shuffle bytes=153297\n",
      "\t\tReduce input records=13136\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=26272\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=2905604096\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tTemperature\n",
      "\t\tMissing=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1778246\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=56\n",
      "16/02/01 09:36:13 INFO streaming.StreamJob: Output directory: output-secondarysort-streaming\n"
     ]
    }
   ],
   "source": [
    "#To do a secondary sort in Streaming, we can take advantage of a couple of library classes\n",
    "#that Hadoop provides. Here’s the driver that we can use to do a secondary sort:\n",
    "!hdfs dfs -rm -r output-secondarysort-streaming\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.6.0/libexec/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1n -k2,1nr\" \\\n",
    "    -files secondary_sort_map.py,secondary_sort_reduce.py \\\n",
    "    -input SecondarySort \\\n",
    "    -output output-secondarysort-streaming \\\n",
    "    -mapper secondary_sort_map.py \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "    -reducer secondary_sort_reduce.py \\\n",
    "    -numReduceTasks 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 09:25:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "-rw-r--r--   1 jshanahan supergroup          0 2016-02-01 09:24 output-secondarysort-streaming/_SUCCESS\n",
      "-rw-r--r--   1 jshanahan supergroup          9 2016-02-01 09:24 output-secondarysort-streaming/part-00000\n",
      "-rw-r--r--   1 jshanahan supergroup         18 2016-02-01 09:24 output-secondarysort-streaming/part-00001\n",
      "-rw-r--r--   1 jshanahan supergroup         27 2016-02-01 09:24 output-secondarysort-streaming/part-00002\n",
      "16/02/01 09:25:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "1902\t244\n",
      "1903\t-78\n",
      "1906\t-72\n",
      "1901\t317\n",
      "1904\t-72\n",
      "1907\t-72\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls output-secondarysort-streaming/*\n",
    "!hdfs dfs -cat output-secondarysort-streaming/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/30 09:12:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/30 09:13:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#stop hadoop/yarn cluster on my local machine (in this case)\n",
    "#!alias hstop=\"/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh\"\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
