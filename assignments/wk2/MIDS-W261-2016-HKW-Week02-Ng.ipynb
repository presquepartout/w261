{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Email: patng@ischool.berkeley.edu  \n",
    "Class: W261-2  \n",
    "Week: 01  \n",
    "Date of submission: Jan 26, 2016\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.1. Sort in Hadoop MapReduce\n",
    "Given as input: Records of the form < integer, “NA” >, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form < integer, “NA” > in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "Write code to generate N  random records of the form < integer, “NA” >. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting genrand.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile genrand.py\n",
    "#!/usr/bin/python\n",
    "import random\n",
    "import sys\n",
    "\n",
    "nums = 10000\n",
    "if len(sys.argv) > 1:\n",
    "    nums = int(sys.argv[1])\n",
    "\n",
    "random.seed(0)\n",
    "for i in range(nums):\n",
    "    print '< %d, \"NA\" >' % random.randint(-1000000, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x genrand.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# The regex which captures the integer from a line in the format < integer, \"NA\" >\n",
    "regex = re.compile(r'\\<\\s*(-?\\d+)\\s*,\\s*\\\"NA\\\"\\s*\\>')\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    # Get the integer from the line\n",
    "    result = regex.findall(line)\n",
    "    if len(result) == 0:\n",
    "        # Cannot find any integer. Could be a corrupted input line.  Skip it.\n",
    "        continue\n",
    "    \n",
    "    # print the integer as the key of the output.  Absence of value means there is no value.\n",
    "    print result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    print '<%s, \"NA\">' % line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<965571, \"NA\">\r\n",
      "<819493, \"NA\">\r\n",
      "<816226, \"NA\">\r\n",
      "<804332, \"NA\">\r\n",
      "<688844, \"NA\">\r\n",
      "<620435, \"NA\">\r\n",
      "<567597, \"NA\">\r\n",
      "<515909, \"NA\">\r\n",
      "<511609, \"NA\">\r\n",
      "<236738, \"NA\">\r\n",
      "<166764, \"NA\">\r\n",
      "<22549, \"NA\">\r\n",
      "<9374, \"NA\">\r\n",
      "<-46806, \"NA\">\r\n",
      "<-158857, \"NA\">\r\n",
      "<-190132, \"NA\">\r\n",
      "<-393375, \"NA\">\r\n",
      "<-436325, \"NA\">\r\n",
      "<-482167, \"NA\">\r\n",
      "<-498988, \"NA\">\r\n"
     ]
    }
   ],
   "source": [
    "!python genrand.py 20 | python mapper.py | sort -g -r | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run it in hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-patrickng-resourcemanager-Patricks-MacBook-Pro.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-patrickng-nodemanager-Patricks-MacBook-Pro.local.out\n",
      "16/01/23 12:32:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-namenode-Patricks-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-datanode-Patricks-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-secondarynamenode-Patricks-MacBook-Pro.local.out\n",
      "16/01/23 12:33:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random numbers, each in the range [-1000000, 1000000].\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"Generating random numbers, each in the range [-1000000, 1000000].\"\n",
    "!rm -f randomNums.txt\n",
    "!./genrand.py 10000 >> randomNums.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:20:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted randomNums.txt\n",
      "16/01/23 13:21:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# upload input file to hdfs\n",
    "!hdfs dfs -rm -f randomNums.txt\n",
    "!hdfs dfs -put randomNums.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:21:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted sortRandomNums\n",
      "16/01/23 13:21:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r sortRandomNums\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options=\"-nr\" -mapper mapper.py -reducer reducer.py -input randomNums.txt -output sortRandomNums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:21:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 13:21:17 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "\n",
      "10 biggest numbers:\n",
      "<999806, \"NA\">\t\n",
      "<999764, \"NA\">\t\n",
      "<999727, \"NA\">\t\n",
      "<999663, \"NA\">\t\n",
      "<999371, \"NA\">\t\n",
      "<998888, \"NA\">\t\n",
      "<998841, \"NA\">\t\n",
      "<998388, \"NA\">\t\n",
      "<997707, \"NA\">\t\n",
      "<997613, \"NA\">\t\n",
      "\n",
      "10 smallest numbers:\n",
      "<-997715, \"NA\">\t\n",
      "<-997902, \"NA\">\t\n",
      "<-997975, \"NA\">\t\n",
      "<-998040, \"NA\">\t\n",
      "<-998770, \"NA\">\t\n",
      "<-998808, \"NA\">\t\n",
      "<-999519, \"NA\">\t\n",
      "<-999672, \"NA\">\t\n",
      "<-999732, \"NA\">\t\n",
      "<-999954, \"NA\">\t\n"
     ]
    }
   ],
   "source": [
    "# Show the reults\n",
    "!rm -f w2.1.result\n",
    "!hdfs dfs -get sortRandomNums/part-00000 w2.1.result\n",
    "!echo\n",
    "!echo \"10 biggest numbers:\"\n",
    "!head -n 10 w2.1.result\n",
    "!echo\n",
    "!echo \"10 smallest numbers:\"\n",
    "!tail -n 10 w2.1.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if you have multiple reducers? Do you need additional steps?\n",
    "\n",
    "If I have multiple reducers, then I have multiple sorted results.  I need to merge these sorted lists into a single sorted list, either by writing my own code, or by passing these results to a single reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2 WORDCOUNT\n",
    "\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "\n",
    " \n",
    "CROSSCHECK: \n",
    "> grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "> 8    \n",
    "\n",
    "#NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Delimiters are: <spaces> , .\n",
    "regex = re.compile(r\"[\\s,\\.]+\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    parts = re.split(\"\\t\", line)\n",
    "\n",
    "    # Extract the text parts\n",
    "    subject = \"\" if parts[2].strip() == \"NA\" else parts[2]\n",
    "    body = \"\" if parts[3].strip() == \"NA\" else parts[3]\n",
    "    text = subject + \" \" + body\n",
    "    \n",
    "    words = filter(None, regex.split(text))\n",
    "    for word in words:\n",
    "        print \"%s\\t1\" % word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "totalCount = 0\n",
    "prev = None # the word previously seen\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    parts = line.split('\\t')\n",
    "    word = parts[0]\n",
    "    count = int(parts[1])\n",
    "    \n",
    "    # If we have encountered a new word, output the answer of the previous word\n",
    "    if prev != word:\n",
    "        if prev is not None:\n",
    "            print \"%s\\t%d\" % (prev, totalCount)\n",
    "            totalCount = 0\n",
    "            \n",
    "    totalCount += 1\n",
    "    prev = word\n",
    "\n",
    "# Output for the last word seen\n",
    "if prev is not None:\n",
    "    print \"%s\\t%d\" % (prev, totalCount)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\t1\r\n",
      "\"\"\t4\r\n",
      "&\t1\r\n",
      "----------------------forwarded\t1\r\n",
      "01/17/2000\t2\r\n",
      "03:22\t1\r\n",
      "06:44\t1\r\n",
      "1\t1\r\n",
      "1-5\t3\r\n",
      "10\t1\r\n",
      "3-7394\t1\r\n",
      "33597\t1\r\n",
      "560\t1\r\n",
      "6\t1\r\n",
      "8\t1\r\n",
      "8-10\t1\r\n",
      "8-12\t3\r\n",
      "9\t1\r\n",
      "@\t21\r\n",
      "a\t8\r\n",
      "all\t2\r\n",
      "allen/hou/ect\t1\r\n",
      "also\t1\r\n",
      "am\t3\r\n",
      "an\t2\r\n",
      "and\t11\r\n",
      "any\t9\r\n",
      "appropriate\t1\r\n",
      "are\t8\r\n",
      "armstrong/corp/enron\t2\r\n",
      "as\t3\r\n",
      "ask\t1\r\n",
      "asking\t1\r\n",
      "at\t2\r\n",
      "attached\t1\r\n",
      "attend\t2\r\n",
      "attendance\t1\r\n",
      "attending\t1\r\n",
      "audience\t2\r\n",
      "available\t2\r\n",
      "back\t2\r\n",
      "be\t6\r\n",
      "being\t1\r\n",
      "below\t3\r\n",
      "benefit\t1\r\n",
      "brad\t1\r\n",
      "buck/hou/ect\t1\r\n",
      "by\t2\r\n",
      "call\t1\r\n",
      "carrera/hou/ect\t1\r\n",
      "cc:\t1\r\n",
      "challenges\t1\r\n",
      "charge\t1\r\n",
      "chosen\t1\r\n",
      "christine\t1\r\n",
      "christmas\t1\r\n",
      "cindy\t1\r\n",
      "classrom\t1\r\n",
      "client\t1\r\n",
      "clients\t1\r\n",
      "coaching\t1\r\n",
      "communicating\t2\r\n",
      "completing\t1\r\n",
      "conduct\t1\r\n",
      "conn/corp/enron\t1\r\n",
      "contact\t1\r\n",
      "cost\t1\r\n",
      "courses\t1\r\n",
      "cross-section\t1\r\n",
      "currently\t1\r\n",
      "curriculum\t4\r\n",
      "curriculum!\t1\r\n",
      "date\t1\r\n",
      "david\t1\r\n",
      "delegating\t1\r\n",
      "depending\t1\r\n",
      "description\t1\r\n",
      "design\t1\r\n",
      "designed\t1\r\n",
      "development\t3\r\n",
      "directing\t1\r\n",
      "discussion\t1\r\n",
      "ect\t17\r\n",
      "effectively\t2\r\n",
      "employee\t2\r\n",
      "ena\t2\r\n",
      "energy\t1\r\n",
      "enron\t3\r\n",
      "enron_development\t1\r\n",
      "eops\t1\r\n",
      "evaluate\t2\r\n",
      "even\t1\r\n",
      "exception\t1\r\n",
      "excited\t1\r\n",
      "experience\t1\r\n",
      "facilitators\t1\r\n",
      "farm\t1\r\n",
      "feb\t3\r\n",
      "february\t3\r\n",
      "find\t1\r\n",
      "fine-tuning\t1\r\n",
      "focus\t1\r\n",
      "following\t1\r\n",
      "for\t9\r\n",
      "fran\t1\r\n",
      "from\t2\r\n",
      "from:\t1\r\n",
      "full\t1\r\n",
      "further\t1\r\n",
      "gary\t1\r\n",
      "get\t1\r\n",
      "good\t1\r\n",
      "gracie\t2\r\n",
      "great\t1\r\n",
      "group\t2\r\n",
      "groups\t1\r\n",
      "half-day\t1\r\n",
      "have\t4\r\n",
      "held\t1\r\n",
      "help\t1\r\n",
      "helpful\t1\r\n",
      "hope\t1\r\n",
      "hope/hou/ect\t1\r\n",
      "i\t1\r\n",
      "if\t3\r\n",
      "in\t6\r\n",
      "include\t1\r\n",
      "information\t2\r\n",
      "invite\t1\r\n",
      "is\t4\r\n",
      "it\t3\r\n",
      "jane\t1\r\n",
      "janice\t1\r\n",
      "jones/corp/enron\t1\r\n",
      "julie\t2\r\n",
      "just\t1\r\n",
      "kathryn\t1\r\n",
      "kim\t1\r\n",
      "kimberly\t1\r\n",
      "l\t1\r\n",
      "later\t1\r\n",
      "leadership\t7\r\n",
      "learn\t1\r\n",
      "learning\t2\r\n",
      "less\t1\r\n",
      "listed\t3\r\n",
      "lunch\t1\r\n",
      "mary\t1\r\n",
      "materials\t2\r\n",
      "may\t1\r\n",
      "mayes/hou/ect\t1\r\n",
      "mclean/hou/ect\t1\r\n",
      "mcsherry/hou/ect\t1\r\n",
      "me\t2\r\n",
      "meeting\t1\r\n",
      "melodick/hou/ect\t1\r\n",
      "minimum\t1\r\n",
      "module\t1\r\n",
      "modules\t3\r\n",
      "months\t1\r\n",
      "more\t1\r\n",
      "motivating\t1\r\n",
      "names\t1\r\n",
      "need\t1\r\n",
      "news\t1\r\n",
      "no\t1\r\n",
      "norma\t1\r\n",
      "of\t9\r\n",
      "on\t6\r\n",
      "one\t1\r\n",
      "only\t1\r\n",
      "open\t1\r\n",
      "operations\t1\r\n",
      "options\t1\r\n",
      "or\t1\r\n",
      "order\t1\r\n",
      "other\t1\r\n",
      "our\t2\r\n",
      "overgaard/pdx/ect\t1\r\n",
      "oxley/hou/ect\t1\r\n",
      "participate\t1\r\n",
      "per\t1\r\n",
      "performance\t2\r\n",
      "philip\t1\r\n",
      "pick\t1\r\n",
      "pictures\t1\r\n",
      "pilot\t6\r\n",
      "please\t3\r\n",
      "pm\t4\r\n",
      "pm---------------------------\t1\r\n",
      "portion\t1\r\n",
      "presas\t1\r\n",
      "presas/hou/ect\t1\r\n",
      "present\t1\r\n",
      "primary\t1\r\n",
      "products\t1\r\n",
      "programs\t1\r\n",
      "purpose\t1\r\n",
      "questions\t2\r\n",
      "rankings\t1\r\n",
      "re:\t1\r\n",
      "ready\t2\r\n",
      "really\t1\r\n",
      "receive\t2\r\n",
      "regarding\t1\r\n",
      "respond\t1\r\n",
      "results\t1\r\n",
      "riedel/hou/ect\t1\r\n",
      "rizzi/hou/ect\t1\r\n",
      "robert\t1\r\n",
      "room\t1\r\n",
      "runkel\t1\r\n",
      "s\t1\r\n",
      "sally:\t1\r\n",
      "selection\t2\r\n",
      "sessions\t2\r\n",
      "setting\t1\r\n",
      "several\t1\r\n",
      "shall\t1\r\n",
      "sheila\t1\r\n",
      "shenkman/enron_development\t1\r\n",
      "sign\t1\r\n",
      "six\t1\r\n",
      "skinner/hou/ect\t1\r\n",
      "so\t1\r\n",
      "southwest\t1\r\n",
      "start\t1\r\n",
      "styles\t1\r\n",
      "subject:\t1\r\n",
      "supervisor\t3\r\n",
      "supervisor\"\t1\r\n",
      "supervisors\t5\r\n",
      "susan\t2\r\n",
      "target\t1\r\n",
      "team\t2\r\n",
      "than\t2\r\n",
      "thank\t2\r\n",
      "that\t3\r\n",
      "the\t21\r\n",
      "their\t2\r\n",
      "there\t1\r\n",
      "this\t3\r\n",
      "thoroughly\t1\r\n",
      "through\t1\r\n",
      "time\t2\r\n",
      "times\t1\r\n",
      "timing\t1\r\n",
      "to\t12\r\n",
      "to:\t1\r\n",
      "today\t1\r\n",
      "tree\t1\r\n",
      "two\t1\r\n",
      "up\t3\r\n",
      "update\t2\r\n",
      "valeria\t1\r\n",
      "valuable\t1\r\n",
      "vendor\t2\r\n",
      "vendors\t1\r\n",
      "villarreal/hou/ect\t1\r\n",
      "walton/hou/ect\t1\r\n",
      "we\t6\r\n",
      "we've\t1\r\n",
      "week\t1\r\n",
      "what\t1\r\n",
      "when\t1\r\n",
      "will\t8\r\n",
      "wilson\t2\r\n",
      "with\t3\r\n",
      "working\t1\r\n",
      "would\t2\r\n",
      "x\t1\r\n",
      "you\t6\r\n",
      "your\t6\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 enronemail_1h.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:03:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 15:03:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Upload input file to HDFS\n",
    "!hdfs dfs -rm -f enronemail_1h.txt\n",
    "!hdfs dfs -put enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:04:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `wordCount': No such file or directory\n",
      "16/01/23 15:04:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Run the hadoop streaming command\n",
    "!hdfs dfs -rm -r wordCount\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:13:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 15:13:36 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "\n",
      "Occurrence count of 'assistance':\n",
      "assistance\t10\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!rm -f w2.2.result\n",
    "!hdfs dfs -get wordCount/part-00000 w2.2.result\n",
    "!echo\n",
    "!echo \"Occurrence count of 'assistance':\"\n",
    "!grep 'assistance' w2.2.result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2.1  \n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    parts = line.split('\\t')\n",
    "    \n",
    "    # Output is: count, and then word\n",
    "    print \"%s\\t%s\" % (parts[1], parts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "count = 0\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    print line\n",
    "    \n",
    "    # Display only the top 10 words\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240\tthe\r\n",
      "914\tto\r\n",
      "659\tand\r\n",
      "556\tof\r\n",
      "527\ta\r\n",
      "415\tin\r\n",
      "407\tyou\r\n",
      "389\tyour\r\n",
      "369\tfor\r\n",
      "361\t@\r\n"
     ]
    }
   ],
   "source": [
    "!cat w2.2.result | python mapper.py | sort -g -r | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:32:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted top10\n",
      "16/01/23 15:32:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "# Please note that we use the output from HW2.2 as input.\n",
    "!hdfs dfs -rm -r top10\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options=\"-nr\" -mapper mapper.py -reducer reducer.py -input wordCount -output top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 occurring words:\n",
      "16/01/23 15:32:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "the\n",
      "to\n",
      "and\n",
      "of\n",
      "a\n",
      "in\n",
      "you\n",
      "your\n",
      "for\n",
      "@\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!echo 'Top 10 occurring words:'\n",
    "!hdfs dfs -cat top10/part-00000 | cut -d$'\\t' -f 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/23 12:23:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/23 12:23:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
