{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Email: patng@ischool.berkeley.edu  \n",
    "Class: W261-2  \n",
    "Week: 01  \n",
    "Date of submission: Jan 26, 2016\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.1. Sort in Hadoop MapReduce\n",
    "Given as input: Records of the form < integer, “NA” >, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form < integer, “NA” > in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "Write code to generate N  random records of the form < integer, “NA” >. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting genrand.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile genrand.py\n",
    "#!/usr/bin/python\n",
    "import random\n",
    "import sys\n",
    "\n",
    "nums = 10000\n",
    "if len(sys.argv) > 1:\n",
    "    nums = int(sys.argv[1])\n",
    "\n",
    "random.seed(0)\n",
    "for i in range(nums):\n",
    "    print '< %d, \"NA\" >' % random.randint(-1000000, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x genrand.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# The regex which captures the integer from a line in the format < integer, \"NA\" >\n",
    "regex = re.compile(r'\\<\\s*(-?\\d+)\\s*,\\s*\\\"NA\\\"\\s*\\>')\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    # Get the integer from the line\n",
    "    result = regex.findall(line)\n",
    "    if len(result) == 0:\n",
    "        # Cannot find any integer. Could be a corrupted input line.  Skip it.\n",
    "        continue\n",
    "    \n",
    "    # print the integer as the key of the output.  Absence of value means there is no value.\n",
    "    print result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    print '<%s, \"NA\">' % line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<965571, \"NA\">\r\n",
      "<819493, \"NA\">\r\n",
      "<816226, \"NA\">\r\n",
      "<804332, \"NA\">\r\n",
      "<688844, \"NA\">\r\n",
      "<620435, \"NA\">\r\n",
      "<567597, \"NA\">\r\n",
      "<515909, \"NA\">\r\n",
      "<511609, \"NA\">\r\n",
      "<236738, \"NA\">\r\n",
      "<166764, \"NA\">\r\n",
      "<22549, \"NA\">\r\n",
      "<9374, \"NA\">\r\n",
      "<-46806, \"NA\">\r\n",
      "<-158857, \"NA\">\r\n",
      "<-190132, \"NA\">\r\n",
      "<-393375, \"NA\">\r\n",
      "<-436325, \"NA\">\r\n",
      "<-482167, \"NA\">\r\n",
      "<-498988, \"NA\">\r\n"
     ]
    }
   ],
   "source": [
    "!python genrand.py 20 | python mapper.py | sort -g -r | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run it in hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-patrickng-resourcemanager-Patricks-MacBook-Pro.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-patrickng-nodemanager-Patricks-MacBook-Pro.local.out\n",
      "16/01/23 12:32:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-namenode-Patricks-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-datanode-Patricks-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-secondarynamenode-Patricks-MacBook-Pro.local.out\n",
      "16/01/23 12:33:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random numbers, each in the range [-1000000, 1000000].\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"Generating random numbers, each in the range [-1000000, 1000000].\"\n",
    "!rm -f randomNums.txt\n",
    "!./genrand.py 10000 >> randomNums.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:20:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted randomNums.txt\n",
      "16/01/23 13:21:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# upload input file to hdfs\n",
    "!hdfs dfs -rm -f randomNums.txt\n",
    "!hdfs dfs -put randomNums.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:21:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted sortRandomNums\n",
      "16/01/23 13:21:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r sortRandomNums\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options=\"-nr\" -mapper mapper.py -reducer reducer.py -input randomNums.txt -output sortRandomNums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:21:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 13:21:17 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "\n",
      "10 biggest numbers:\n",
      "<999806, \"NA\">\t\n",
      "<999764, \"NA\">\t\n",
      "<999727, \"NA\">\t\n",
      "<999663, \"NA\">\t\n",
      "<999371, \"NA\">\t\n",
      "<998888, \"NA\">\t\n",
      "<998841, \"NA\">\t\n",
      "<998388, \"NA\">\t\n",
      "<997707, \"NA\">\t\n",
      "<997613, \"NA\">\t\n",
      "\n",
      "10 smallest numbers:\n",
      "<-997715, \"NA\">\t\n",
      "<-997902, \"NA\">\t\n",
      "<-997975, \"NA\">\t\n",
      "<-998040, \"NA\">\t\n",
      "<-998770, \"NA\">\t\n",
      "<-998808, \"NA\">\t\n",
      "<-999519, \"NA\">\t\n",
      "<-999672, \"NA\">\t\n",
      "<-999732, \"NA\">\t\n",
      "<-999954, \"NA\">\t\n"
     ]
    }
   ],
   "source": [
    "# Show the reults\n",
    "!rm -f w2.1.result\n",
    "!hdfs dfs -get sortRandomNums/part-00000 w2.1.result\n",
    "!echo\n",
    "!echo \"10 biggest numbers:\"\n",
    "!head -n 10 w2.1.result\n",
    "!echo\n",
    "!echo \"10 smallest numbers:\"\n",
    "!tail -n 10 w2.1.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if you have multiple reducers? Do you need additional steps?\n",
    "\n",
    "If I have multiple reducers, then I have multiple sorted results.  I need to merge these sorted lists into a single sorted list, either by writing my own code, or by passing these results to a single reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2 WORDCOUNT\n",
    "\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "\n",
    " \n",
    "CROSSCHECK: \n",
    "> grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "> 8    \n",
    "\n",
    "#NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Regex for splitting the words.  Delimiters are: <spaces> , .\n",
    "regex = re.compile(r\"[\\s,\\.]+\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    parts = re.split(\"\\t\", line)\n",
    "\n",
    "    # Extract the text parts\n",
    "    subject = \"\" if parts[2].strip() == \"NA\" else parts[2]\n",
    "    body = \"\" if parts[3].strip() == \"NA\" else parts[3]\n",
    "    text = subject + \" \" + body\n",
    "    \n",
    "    words = filter(None, regex.split(text))\n",
    "    for word in words:\n",
    "        print \"%s\\t1\" % word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "totalCount = 0\n",
    "prev = None # the word previously seen\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    parts = line.split('\\t')\n",
    "    word = parts[0]\n",
    "    count = int(parts[1])\n",
    "    \n",
    "    # If we have encountered a new word, output the answer of the previous word\n",
    "    if prev != word:\n",
    "        if prev is not None:\n",
    "            print \"%s\\t%d\" % (prev, totalCount)\n",
    "            totalCount = 0\n",
    "            \n",
    "    totalCount += 1\n",
    "    prev = word\n",
    "\n",
    "# Output for the last word seen\n",
    "if prev is not None:\n",
    "    print \"%s\\t%d\" % (prev, totalCount)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\t1\r\n",
      "\"\"\t4\r\n",
      "&\t1\r\n",
      "----------------------forwarded\t1\r\n",
      "01/17/2000\t2\r\n",
      "03:22\t1\r\n",
      "06:44\t1\r\n",
      "1\t1\r\n",
      "1-5\t3\r\n",
      "10\t1\r\n",
      "3-7394\t1\r\n",
      "33597\t1\r\n",
      "560\t1\r\n",
      "6\t1\r\n",
      "8\t1\r\n",
      "8-10\t1\r\n",
      "8-12\t3\r\n",
      "9\t1\r\n",
      "@\t21\r\n",
      "a\t8\r\n",
      "all\t2\r\n",
      "allen/hou/ect\t1\r\n",
      "also\t1\r\n",
      "am\t3\r\n",
      "an\t2\r\n",
      "and\t11\r\n",
      "any\t9\r\n",
      "appropriate\t1\r\n",
      "are\t8\r\n",
      "armstrong/corp/enron\t2\r\n",
      "as\t3\r\n",
      "ask\t1\r\n",
      "asking\t1\r\n",
      "at\t2\r\n",
      "attached\t1\r\n",
      "attend\t2\r\n",
      "attendance\t1\r\n",
      "attending\t1\r\n",
      "audience\t2\r\n",
      "available\t2\r\n",
      "back\t2\r\n",
      "be\t6\r\n",
      "being\t1\r\n",
      "below\t3\r\n",
      "benefit\t1\r\n",
      "brad\t1\r\n",
      "buck/hou/ect\t1\r\n",
      "by\t2\r\n",
      "call\t1\r\n",
      "carrera/hou/ect\t1\r\n",
      "cc:\t1\r\n",
      "challenges\t1\r\n",
      "charge\t1\r\n",
      "chosen\t1\r\n",
      "christine\t1\r\n",
      "christmas\t1\r\n",
      "cindy\t1\r\n",
      "classrom\t1\r\n",
      "client\t1\r\n",
      "clients\t1\r\n",
      "coaching\t1\r\n",
      "communicating\t2\r\n",
      "completing\t1\r\n",
      "conduct\t1\r\n",
      "conn/corp/enron\t1\r\n",
      "contact\t1\r\n",
      "cost\t1\r\n",
      "courses\t1\r\n",
      "cross-section\t1\r\n",
      "currently\t1\r\n",
      "curriculum\t4\r\n",
      "curriculum!\t1\r\n",
      "date\t1\r\n",
      "david\t1\r\n",
      "delegating\t1\r\n",
      "depending\t1\r\n",
      "description\t1\r\n",
      "design\t1\r\n",
      "designed\t1\r\n",
      "development\t3\r\n",
      "directing\t1\r\n",
      "discussion\t1\r\n",
      "ect\t17\r\n",
      "effectively\t2\r\n",
      "employee\t2\r\n",
      "ena\t2\r\n",
      "energy\t1\r\n",
      "enron\t3\r\n",
      "enron_development\t1\r\n",
      "eops\t1\r\n",
      "evaluate\t2\r\n",
      "even\t1\r\n",
      "exception\t1\r\n",
      "excited\t1\r\n",
      "experience\t1\r\n",
      "facilitators\t1\r\n",
      "farm\t1\r\n",
      "feb\t3\r\n",
      "february\t3\r\n",
      "find\t1\r\n",
      "fine-tuning\t1\r\n",
      "focus\t1\r\n",
      "following\t1\r\n",
      "for\t9\r\n",
      "fran\t1\r\n",
      "from\t2\r\n",
      "from:\t1\r\n",
      "full\t1\r\n",
      "further\t1\r\n",
      "gary\t1\r\n",
      "get\t1\r\n",
      "good\t1\r\n",
      "gracie\t2\r\n",
      "great\t1\r\n",
      "group\t2\r\n",
      "groups\t1\r\n",
      "half-day\t1\r\n",
      "have\t4\r\n",
      "held\t1\r\n",
      "help\t1\r\n",
      "helpful\t1\r\n",
      "hope\t1\r\n",
      "hope/hou/ect\t1\r\n",
      "i\t1\r\n",
      "if\t3\r\n",
      "in\t6\r\n",
      "include\t1\r\n",
      "information\t2\r\n",
      "invite\t1\r\n",
      "is\t4\r\n",
      "it\t3\r\n",
      "jane\t1\r\n",
      "janice\t1\r\n",
      "jones/corp/enron\t1\r\n",
      "julie\t2\r\n",
      "just\t1\r\n",
      "kathryn\t1\r\n",
      "kim\t1\r\n",
      "kimberly\t1\r\n",
      "l\t1\r\n",
      "later\t1\r\n",
      "leadership\t7\r\n",
      "learn\t1\r\n",
      "learning\t2\r\n",
      "less\t1\r\n",
      "listed\t3\r\n",
      "lunch\t1\r\n",
      "mary\t1\r\n",
      "materials\t2\r\n",
      "may\t1\r\n",
      "mayes/hou/ect\t1\r\n",
      "mclean/hou/ect\t1\r\n",
      "mcsherry/hou/ect\t1\r\n",
      "me\t2\r\n",
      "meeting\t1\r\n",
      "melodick/hou/ect\t1\r\n",
      "minimum\t1\r\n",
      "module\t1\r\n",
      "modules\t3\r\n",
      "months\t1\r\n",
      "more\t1\r\n",
      "motivating\t1\r\n",
      "names\t1\r\n",
      "need\t1\r\n",
      "news\t1\r\n",
      "no\t1\r\n",
      "norma\t1\r\n",
      "of\t9\r\n",
      "on\t6\r\n",
      "one\t1\r\n",
      "only\t1\r\n",
      "open\t1\r\n",
      "operations\t1\r\n",
      "options\t1\r\n",
      "or\t1\r\n",
      "order\t1\r\n",
      "other\t1\r\n",
      "our\t2\r\n",
      "overgaard/pdx/ect\t1\r\n",
      "oxley/hou/ect\t1\r\n",
      "participate\t1\r\n",
      "per\t1\r\n",
      "performance\t2\r\n",
      "philip\t1\r\n",
      "pick\t1\r\n",
      "pictures\t1\r\n",
      "pilot\t6\r\n",
      "please\t3\r\n",
      "pm\t4\r\n",
      "pm---------------------------\t1\r\n",
      "portion\t1\r\n",
      "presas\t1\r\n",
      "presas/hou/ect\t1\r\n",
      "present\t1\r\n",
      "primary\t1\r\n",
      "products\t1\r\n",
      "programs\t1\r\n",
      "purpose\t1\r\n",
      "questions\t2\r\n",
      "rankings\t1\r\n",
      "re:\t1\r\n",
      "ready\t2\r\n",
      "really\t1\r\n",
      "receive\t2\r\n",
      "regarding\t1\r\n",
      "respond\t1\r\n",
      "results\t1\r\n",
      "riedel/hou/ect\t1\r\n",
      "rizzi/hou/ect\t1\r\n",
      "robert\t1\r\n",
      "room\t1\r\n",
      "runkel\t1\r\n",
      "s\t1\r\n",
      "sally:\t1\r\n",
      "selection\t2\r\n",
      "sessions\t2\r\n",
      "setting\t1\r\n",
      "several\t1\r\n",
      "shall\t1\r\n",
      "sheila\t1\r\n",
      "shenkman/enron_development\t1\r\n",
      "sign\t1\r\n",
      "six\t1\r\n",
      "skinner/hou/ect\t1\r\n",
      "so\t1\r\n",
      "southwest\t1\r\n",
      "start\t1\r\n",
      "styles\t1\r\n",
      "subject:\t1\r\n",
      "supervisor\t3\r\n",
      "supervisor\"\t1\r\n",
      "supervisors\t5\r\n",
      "susan\t2\r\n",
      "target\t1\r\n",
      "team\t2\r\n",
      "than\t2\r\n",
      "thank\t2\r\n",
      "that\t3\r\n",
      "the\t21\r\n",
      "their\t2\r\n",
      "there\t1\r\n",
      "this\t3\r\n",
      "thoroughly\t1\r\n",
      "through\t1\r\n",
      "time\t2\r\n",
      "times\t1\r\n",
      "timing\t1\r\n",
      "to\t12\r\n",
      "to:\t1\r\n",
      "today\t1\r\n",
      "tree\t1\r\n",
      "two\t1\r\n",
      "up\t3\r\n",
      "update\t2\r\n",
      "valeria\t1\r\n",
      "valuable\t1\r\n",
      "vendor\t2\r\n",
      "vendors\t1\r\n",
      "villarreal/hou/ect\t1\r\n",
      "walton/hou/ect\t1\r\n",
      "we\t6\r\n",
      "we've\t1\r\n",
      "week\t1\r\n",
      "what\t1\r\n",
      "when\t1\r\n",
      "will\t8\r\n",
      "wilson\t2\r\n",
      "with\t3\r\n",
      "working\t1\r\n",
      "would\t2\r\n",
      "x\t1\r\n",
      "you\t6\r\n",
      "your\t6\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 enronemail_1h.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:03:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 15:03:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Upload input file to HDFS\n",
    "!hdfs dfs -rm -f enronemail_1h.txt\n",
    "!hdfs dfs -put enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:04:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `wordCount': No such file or directory\n",
      "16/01/23 15:04:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Run the hadoop streaming command\n",
    "!hdfs dfs -rm -r wordCount\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:13:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 15:13:36 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "\n",
      "Occurrence count of 'assistance':\n",
      "assistance\t10\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!rm -f w2.2.result\n",
    "!hdfs dfs -get wordCount/part-00000 w2.2.result\n",
    "!echo\n",
    "!echo \"Occurrence count of 'assistance':\"\n",
    "!grep 'assistance' w2.2.result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2.1  \n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    parts = line.split('\\t')\n",
    "    \n",
    "    # Output is: count, and then word\n",
    "    print \"%s\\t%s\" % (parts[1], parts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "count = 0\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    print line\n",
    "    \n",
    "    # Display only the top 10 words\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240\tthe\r\n",
      "914\tto\r\n",
      "659\tand\r\n",
      "556\tof\r\n",
      "527\ta\r\n",
      "415\tin\r\n",
      "407\tyou\r\n",
      "389\tyour\r\n",
      "369\tfor\r\n",
      "361\t@\r\n"
     ]
    }
   ],
   "source": [
    "!cat w2.2.result | python mapper.py | sort -g -r | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:32:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted top10\n",
      "16/01/23 15:32:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "# Please note that we use the output from HW2.2 as input.\n",
    "!hdfs dfs -rm -r top10\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options=\"-nr\" -mapper mapper.py -reducer reducer.py -input wordCount -output top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 occurring words:\n",
      "16/01/23 15:32:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "the\n",
      "to\n",
      "and\n",
      "of\n",
      "a\n",
      "in\n",
      "you\n",
      "your\n",
      "for\n",
      "@\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!echo 'Top 10 occurring words:'\n",
    "!hdfs dfs -cat top10/part-00000 | cut -d$'\\t' -f 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "> the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. **Count up how many times you need to process a zero probabilty for each class and report.** \n",
    "\n",
    "   Report the performance of your learnt classifier in terms of **misclassifcation error rate** of your multinomial Naive Bayes Classifier.  \n",
    "   \n",
    "   Plot a histogram of the log posterior probabilities (i.e., log(Pr(Class|Doc))) for each class over the training set. \n",
    "   \n",
    "   Summarize what you see. \n",
    "\n",
    "> Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    ">> Let DF represent the evalution set in the following:\n",
    ">> Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    ">> Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Regex for splitting the words.  Delimiters are: <spaces> , .\n",
    "regex = re.compile(r\"[\\s,\\.]+\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    parts = re.split(\"\\t\", line) # parse line into separate fields\n",
    "    \n",
    "    msgId = parts[0].strip()\n",
    "    isSpam = parts[1].strip()\n",
    "\n",
    "    # Extract the text parts\n",
    "    subject = parts[2].strip()\n",
    "    if subject == \"NA\":\n",
    "        subject = \"\"\n",
    "        \n",
    "    body = parts[3].strip()\n",
    "    if body == \"NA\":\n",
    "        body = \"\"\n",
    "\n",
    "    text = subject + \" \" + body\n",
    "    \n",
    "    # Create list of words\n",
    "    words = filter(None, regex.split(text))\n",
    "    \n",
    "    for word in words:\n",
    "        # Send one row for every word instance to the reducer.\n",
    "        print msgId + '\\t' + isSpam + '\\t' + word + '\\t' + '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys\n",
    "from math import log\n",
    " \n",
    "per_word_counts = {} # ham and spam counts about each word\n",
    "emails = {} # data about each email \n",
    "email_counts = [0,0] # number of emails in each class\n",
    "class_word_counts = [0,0] # number of total (not unique) words in each class\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    \n",
    "    #parse the incoming line\n",
    "    parts=line.split(\"\\t\")\n",
    "    email=parts[0]\n",
    "    spam=int(parts[1])\n",
    "    word=parts[2]\n",
    "    \n",
    "    # initialize storage for word/email data\n",
    "    if word not in per_word_counts.keys():\n",
    "        per_word_counts[word] = [0,0] # ham count and spam count\n",
    "        \n",
    "    if email not in emails.keys():\n",
    "        emails[email] = {'spam':spam, 'words':[]}\n",
    "        email_counts[spam] += 1\n",
    "\n",
    "    # update per-class word count for this word\n",
    "    per_word_counts[word][spam] += 1\n",
    "    class_word_counts[spam] += 1\n",
    "\n",
    "    # update email data \n",
    "    emails[email]['words'].append(word)\n",
    "\n",
    "\n",
    "# train the model\n",
    "\n",
    "priors = {} # priors for the two classes\n",
    "condProbs = {} # conditional probabilties for each word\n",
    "vocab_count = len(per_word_counts) # number of unique words in the total vocabulary\n",
    "\n",
    "for c in [0,1]:\n",
    "    priors[c] = email_counts[c] / len(emails)\n",
    "\n",
    "zeros = [0,0] # Remember the number of zero cond. prob. encountered in each class\n",
    "\n",
    "# Go through each class, and compute the conditional probability of each word in the vocab\n",
    "for word, counts in per_word_counts.iteritems():\n",
    "    condProbs[word] = [0,0]\n",
    "    for c in [0,1]:\n",
    "        # Calcuate the conditional probability of the word in this class without smoothing\n",
    "        condProbs[word][c] = counts[c] / class_word_counts[c]\n",
    "        \n",
    "        if counts[c] == 0:\n",
    "            zeros[c] += 1\n",
    "\n",
    "            \n",
    "# Now make the predictions\n",
    "\n",
    "misclassifiedCount = 0\n",
    "for msgId, email in emails.iteritems():\n",
    "\n",
    "    # Compute the email's score for each class\n",
    "    \n",
    "    # Initialize the score of each class\n",
    "    scores = [log(priors[0]), log(priors[1])]\n",
    "        \n",
    "    # For each word contained in this email, add up its log(condProb)\n",
    "    # Because we're not using smoothing, we have skip any word where its\n",
    "    # cond. prob. is zero is either one of the classes.\n",
    "    for word in email['words']:\n",
    "        if condProbs[word][0] == 0 or condProbs[word][1] == 0:\n",
    "            continue\n",
    "\n",
    "        for c in [0,1]:\n",
    "            scores[c] += log(condProbs[word][c])\n",
    "    \n",
    "    # The predicted class is the one which has the higher score\n",
    "    predicted = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    if predicted != email['spam']:\n",
    "        misclassifiedCount += 1\n",
    "\n",
    "# Report the result\n",
    "print \"Misclassifcation error rate:\", misclassifiedCount / len(emails)\n",
    "print \"Number of zero cond. prob. processed in ham emails:\", zeros[0]\n",
    "print \"Number of zero cond. prob. processed in spam emails:\", zeros[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifcation error rate: 0.11\r\n",
      "Number of zero cond. prob. processed in ham emails: 3246\r\n",
      "Number of zero cond. prob. processed in spam emails: 2140\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 enronemail_1h.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 22:59:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted enronemail_1h.txt\n",
      "16/01/23 22:59:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r enronemail_1h.txt\n",
    "!hdfs dfs -put enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 22:59:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `hw2.2': No such file or directory\n",
      "16/01/23 22:59:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r output_hw2.3\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output output_hw2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 23:00:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Misclassifcation error rate: 1.0\t\n",
      "Number of zero cond. prob. processed in ham emails: 6453\t\n",
      "Number of zero cond. prob. processed in spam emails: 4959\t\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!hdfs dfs -cat output_hw2.3/part-00000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Still need to plot the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.4 \n",
    "Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer (Note: will use the same mapper in HW2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys\n",
    "from math import log\n",
    " \n",
    "per_word_counts = {} # ham and spam counts about each word\n",
    "emails = {} # data about each email \n",
    "email_counts = [0,0] # number of emails in each class\n",
    "class_word_counts = [0,0] # number of total (not unique) words in each class\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    \n",
    "    #parse the incoming line\n",
    "    parts=line.split(\"\\t\")\n",
    "    email=parts[0]\n",
    "    spam=int(parts[1])\n",
    "    word=parts[2]\n",
    "    \n",
    "    # initialize storage for word/email data\n",
    "    if word not in per_word_counts.keys():\n",
    "        per_word_counts[word] = [0,0] # ham count and spam count\n",
    "        \n",
    "    if email not in emails.keys():\n",
    "        emails[email] = {'spam':spam, 'words':[]}\n",
    "        email_counts[spam] += 1\n",
    "\n",
    "    # update per-class word count for this word\n",
    "    per_word_counts[word][spam] += 1\n",
    "    class_word_counts[spam] += 1\n",
    "\n",
    "    # update email data \n",
    "    emails[email]['words'].append(word)\n",
    "\n",
    "\n",
    "# train the model\n",
    "\n",
    "priors = {} # priors for the two classes\n",
    "condProbs = {} # conditional probabilties for each word\n",
    "vocab_count = len(per_word_counts) # number of unique words in the total vocabulary\n",
    "\n",
    "for c in [0,1]:\n",
    "    priors[c] = email_counts[c] / len(emails)\n",
    "\n",
    "# Go through each class, and compute the conditional probability of each word in the vocab\n",
    "for word, counts in per_word_counts.iteritems():\n",
    "    condProbs[word] = [0,0]\n",
    "    for c in [0,1]:\n",
    "        # Calcuate the conditional probability of the word in this class with\n",
    "        # Laplace plus-one smoothing\n",
    "        condProbs[word][c] = (counts[c] + 1) / (class_word_counts[c] + vocab_count)\n",
    "\n",
    "# Now make the predictions\n",
    "\n",
    "misclassifiedCount = 0\n",
    "for msgId, email in emails.iteritems():\n",
    "\n",
    "    # Initialize the score of each class\n",
    "    scores = [log(priors[0]), log(priors[1])]\n",
    "        \n",
    "    # For each word contained in this email, add up its log(condProb)\n",
    "    # Because we're not using smoothing, we have skip any word where its\n",
    "    # cond. prob. is zero is either one of the classes.\n",
    "    for word in email['words']:\n",
    "        for c in [0,1]:\n",
    "            scores[c] += log(condProbs[word][c])\n",
    "    \n",
    "    # The predicted class is the one which has the higher score\n",
    "    predicted = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    if predicted != email['spam']:\n",
    "        misclassifiedCount += 1\n",
    "\n",
    "# Report the result\n",
    "print \"Misclassifcation error rate:\", misclassifiedCount / len(emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifcation error rate: 0.01\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 enronemail_1h.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 23:33:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted enronemail_1h.txt\n",
      "16/01/23 23:33:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r enronemail_1h.txt\n",
    "!hdfs dfs -put enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 00:00:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted output_hw2.4\n",
      "16/01/24 00:00:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r output_hw2.4\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output output_hw2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 00:00:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Misclassifcation error rate: 0.01\t\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!hdfs dfs -cat output_hw2.4/part-00000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.**\n",
    "\n",
    "After adding smoothing, the misclassification rate has dropped from 11% to 1%.  It is because in HW2.3, if the conditional probability of a word is zero, the code will ignore that word during prediction.  So it means we have less data, as there were 5386 such cases.  With Laplace plus-one smoothing we have retained those cases and thus the \"weight\" of those words are used during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.5. \n",
    "Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer (Note: we will reduce the mapper in HW2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys\n",
    "from math import log\n",
    " \n",
    "per_word_counts = {} # ham and spam counts about each word\n",
    "emails = {} # data about each email \n",
    "email_counts = [0,0] # number of emails in each class\n",
    "class_word_counts = [0,0] # number of total (not unique) words in each class\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    \n",
    "    #parse the incoming line\n",
    "    parts=line.split(\"\\t\")\n",
    "    email=parts[0]\n",
    "    spam=int(parts[1])\n",
    "    word=parts[2]\n",
    "    \n",
    "    # initialize storage for word/email data\n",
    "    if word not in per_word_counts.keys():\n",
    "        per_word_counts[word] = [0,0] # ham count and spam count\n",
    "        \n",
    "    if email not in emails.keys():\n",
    "        emails[email] = {'spam':spam, 'words':[]}\n",
    "        email_counts[spam] += 1\n",
    "\n",
    "    # update per-class word count for this word\n",
    "    per_word_counts[word][spam] += 1\n",
    "    class_word_counts[spam] += 1\n",
    "\n",
    "    # update email data \n",
    "    emails[email]['words'].append(word)\n",
    "\n",
    "\n",
    "# Remove all words with a frequency of less than three (3) in the training set.\n",
    "lowFrequencyWords = []\n",
    "for word, counts in per_word_counts.iteritems():\n",
    "    if sum(counts) < 3:\n",
    "        lowFrequencyWords.append(word)\n",
    "\n",
    "for word in lowFrequencyWords:\n",
    "    del per_word_counts[word]\n",
    "\n",
    "    \n",
    "# train the model\n",
    "\n",
    "priors = {} # priors for the two classes\n",
    "condProbs = {} # conditional probabilties for each word\n",
    "vocab_count = len(per_word_counts) # number of unique words in the total vocabulary\n",
    "\n",
    "for c in [0,1]:\n",
    "    priors[c] = email_counts[c] / len(emails)\n",
    "\n",
    "# Go through each class, and compute the conditional probability of each word in the vocab\n",
    "for word, counts in per_word_counts.iteritems():\n",
    "    condProbs[word] = [0,0]\n",
    "    for c in [0,1]:\n",
    "        # Calcuate the conditional probability of the word in this class with\n",
    "        # Laplace plus-one smoothing\n",
    "        condProbs[word][c] = (counts[c] + 1) / (class_word_counts[c] + vocab_count)\n",
    "\n",
    "\n",
    "# Now make the predictions\n",
    "\n",
    "misclassifiedCount = 0\n",
    "for msgId, email in emails.iteritems():\n",
    "\n",
    "    # Initialize the score of each class\n",
    "    scores = [log(priors[0]), log(priors[1])]\n",
    "        \n",
    "    # For each word contained in this email, add up its log(condProb)\n",
    "    # Because we're not using smoothing, we have skip any word where its\n",
    "    # cond. prob. is zero is either one of the classes.\n",
    "    for word in email['words']:\n",
    "        if word not in lowFrequencyWords:\n",
    "            for c in [0,1]:\n",
    "                scores[c] += log(condProbs[word][c])\n",
    "    \n",
    "    # The predicted class is the one which has the higher score\n",
    "    predicted = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    if predicted != email['spam']:\n",
    "        misclassifiedCount += 1\n",
    "\n",
    "# Report the result\n",
    "print \"Misclassifcation error rate:\", misclassifiedCount / len(emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifcation error rate: 0.01\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 enronemail_1h.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 00:36:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted output_hw2.5\n",
      "16/01/24 00:36:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r output_hw2.5\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output output_hw2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 00:36:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Misclassifcation error rate: 0.01\t\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!hdfs dfs -cat output_hw2.5/part-00000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset?**\n",
    "\n",
    "?????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/23 12:23:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/23 12:23:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
