{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Email: patng@ischool.berkeley.edu  \n",
    "Class: W261-2  \n",
    "Week: 01  \n",
    "Date of submission: Jan 26, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.0.  \n",
    "What is a race condition in the context of parallel computation? Give an example.  \n",
    "What is MapReduce?  \n",
    "How does it differ from Hadoop?  \n",
    "Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a race condition in the context of parallel computation? Give an example. ** \n",
    "\n",
    "A race condition happens when multiple execution entities (e.g. threads, processes, etc)\n",
    "are accessing or modifying a common resource at the same time, and the order of access can impact the result.  \n",
    "\n",
    "For example, consider the following logic, where A is a global variable:  \n",
    "```\n",
    "temp = A\n",
    "temp = temp + 1\n",
    "A = temp\n",
    "```\n",
    "It will increase the value of A by one.  If it is run twice sequentially, A will be increased by two.  However, if two threads are running it at the same time, there is a chance that A will only be increased by one instead of two.  \n",
    "<br/>\n",
    "\n",
    "**What is MapReduce?  **\n",
    "\n",
    "MapReduce can refer to three distinct but related concepts.  \n",
    "\n",
    "First, MapReduce codifies a generic recipe for processing large datasets that consists of two stages.\n",
    "- In the first stage, a user-specified computation is applied over all input records in a dataset.\n",
    "- These operations occur in parallel and yield intermediate output that is then aggregated by another user-specified computation.\n",
    "\n",
    "Second, MapReduce can refer to the execution framework (i.e., the “runtime”) that coordinates the execution of programs written in this particular style.  \n",
    "\n",
    "Finally, MapReduce can refer to the software implementation of the programming model and the execution framework.\n",
    "\n",
    "<br/>\n",
    "**How does it differ from Hadoop?**  \n",
    "\n",
    "Hadoop is the software implementation of the MapReduce programming model and the execution framework.  In Hadoop v2.0, the main components include MapReduce (the programming model), YARN (the resource manager) and HDFS (the distributed file system).\n",
    "\n",
    "<br/>\n",
    "**Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.**\n",
    "\n",
    "Hadoop is based on the MapReduce programming model.  You can find examples of its code and its running in HW2.1-HW2.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.1. Sort in Hadoop MapReduce\n",
    "Given as input: Records of the form < integer, “NA” >, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form < integer, “NA” > in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "Write code to generate N  random records of the form < integer, “NA” >. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting genrand.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile genrand.py\n",
    "#!/usr/bin/python\n",
    "import random\n",
    "import sys\n",
    "\n",
    "nums = 10000\n",
    "if len(sys.argv) > 1:\n",
    "    nums = int(sys.argv[1])\n",
    "\n",
    "random.seed(0)\n",
    "for i in range(nums):\n",
    "    print '< %d, \"NA\" >' % random.randint(-1000000, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x genrand.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# The regex which captures the integer from a line in the format < integer, \"NA\" >\n",
    "regex = re.compile(r'\\<\\s*(-?\\d+)\\s*,\\s*\\\"NA\\\"\\s*\\>')\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    # Get the integer from the line\n",
    "    result = regex.findall(line)\n",
    "    if len(result) == 0:\n",
    "        # Cannot find any integer. Could be a corrupted input line.  Skip it.\n",
    "        continue\n",
    "    \n",
    "    # print the integer as the key of the output.  Absence of value means there is no value.\n",
    "    print result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    print '<%s, \"NA\">' % line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<965571, \"NA\">\r\n",
      "<819493, \"NA\">\r\n",
      "<816226, \"NA\">\r\n",
      "<804332, \"NA\">\r\n",
      "<688844, \"NA\">\r\n",
      "<620435, \"NA\">\r\n",
      "<567597, \"NA\">\r\n",
      "<515909, \"NA\">\r\n",
      "<511609, \"NA\">\r\n",
      "<236738, \"NA\">\r\n",
      "<166764, \"NA\">\r\n",
      "<22549, \"NA\">\r\n",
      "<9374, \"NA\">\r\n",
      "<-46806, \"NA\">\r\n",
      "<-158857, \"NA\">\r\n",
      "<-190132, \"NA\">\r\n",
      "<-393375, \"NA\">\r\n",
      "<-436325, \"NA\">\r\n",
      "<-482167, \"NA\">\r\n",
      "<-498988, \"NA\">\r\n"
     ]
    }
   ],
   "source": [
    "!python genrand.py 20 | python mapper.py | sort -g -r | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run it in hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-patrickng-resourcemanager-Patricks-MacBook-Pro.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-patrickng-nodemanager-Patricks-MacBook-Pro.local.out\n",
      "16/01/23 12:32:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-namenode-Patricks-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-datanode-Patricks-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-secondarynamenode-Patricks-MacBook-Pro.local.out\n",
      "16/01/23 12:33:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random numbers, each in the range [-1000000, 1000000].\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"Generating random numbers, each in the range [-1000000, 1000000].\"\n",
    "!rm -f randomNums.txt\n",
    "!./genrand.py 10000 >> randomNums.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:20:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted randomNums.txt\n",
      "16/01/23 13:21:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# upload input file to hdfs\n",
    "!hdfs dfs -rm -f randomNums.txt\n",
    "!hdfs dfs -put randomNums.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:21:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted sortRandomNums\n",
      "16/01/23 13:21:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r sortRandomNums\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options=\"-nr\" -mapper mapper.py -reducer reducer.py -input randomNums.txt -output sortRandomNums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:21:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 13:21:17 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "\n",
      "10 biggest numbers:\n",
      "<999806, \"NA\">\t\n",
      "<999764, \"NA\">\t\n",
      "<999727, \"NA\">\t\n",
      "<999663, \"NA\">\t\n",
      "<999371, \"NA\">\t\n",
      "<998888, \"NA\">\t\n",
      "<998841, \"NA\">\t\n",
      "<998388, \"NA\">\t\n",
      "<997707, \"NA\">\t\n",
      "<997613, \"NA\">\t\n",
      "\n",
      "10 smallest numbers:\n",
      "<-997715, \"NA\">\t\n",
      "<-997902, \"NA\">\t\n",
      "<-997975, \"NA\">\t\n",
      "<-998040, \"NA\">\t\n",
      "<-998770, \"NA\">\t\n",
      "<-998808, \"NA\">\t\n",
      "<-999519, \"NA\">\t\n",
      "<-999672, \"NA\">\t\n",
      "<-999732, \"NA\">\t\n",
      "<-999954, \"NA\">\t\n"
     ]
    }
   ],
   "source": [
    "# Show the reults\n",
    "!rm -f w2.1.result\n",
    "!hdfs dfs -get sortRandomNums/part-00000 w2.1.result\n",
    "!echo\n",
    "!echo \"10 biggest numbers:\"\n",
    "!head -n 10 w2.1.result\n",
    "!echo\n",
    "!echo \"10 smallest numbers:\"\n",
    "!tail -n 10 w2.1.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if you have multiple reducers? Do you need additional steps?\n",
    "\n",
    "If I have multiple reducers, then I have multiple sorted results.  I need to merge these sorted lists into a single sorted list, either by writing my own code, or by passing these results to a single reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2 WORDCOUNT\n",
    "\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "\n",
    " \n",
    "CROSSCHECK: \n",
    "> grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "> 8    \n",
    "\n",
    "#NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Regex for splitting the words.  Delimiters are: <spaces> , .\n",
    "regex = re.compile(r\"[\\s,\\.]+\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    parts = re.split(\"\\t\", line)\n",
    "\n",
    "    # Extract the text parts\n",
    "    subject = \"\" if parts[2].strip() == \"NA\" else parts[2]\n",
    "    body = \"\" if parts[3].strip() == \"NA\" else parts[3]\n",
    "    text = subject + \" \" + body\n",
    "    \n",
    "    words = filter(None, regex.split(text))\n",
    "    for word in words:\n",
    "        print \"%s\\t1\" % word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "totalCount = 0\n",
    "prev = None # the word previously seen\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    parts = line.split('\\t')\n",
    "    word = parts[0]\n",
    "    count = int(parts[1])\n",
    "    \n",
    "    # If we have encountered a new word, output the answer of the previous word\n",
    "    if prev != word:\n",
    "        if prev is not None:\n",
    "            print \"%s\\t%d\" % (prev, totalCount)\n",
    "            totalCount = 0\n",
    "            \n",
    "    totalCount += 1\n",
    "    prev = word\n",
    "\n",
    "# Output for the last word seen\n",
    "if prev is not None:\n",
    "    print \"%s\\t%d\" % (prev, totalCount)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\t1\r\n",
      "\"\"\t4\r\n",
      "&\t1\r\n",
      "----------------------forwarded\t1\r\n",
      "01/17/2000\t2\r\n",
      "03:22\t1\r\n",
      "06:44\t1\r\n",
      "1\t1\r\n",
      "1-5\t3\r\n",
      "10\t1\r\n",
      "3-7394\t1\r\n",
      "33597\t1\r\n",
      "560\t1\r\n",
      "6\t1\r\n",
      "8\t1\r\n",
      "8-10\t1\r\n",
      "8-12\t3\r\n",
      "9\t1\r\n",
      "@\t21\r\n",
      "a\t8\r\n",
      "all\t2\r\n",
      "allen/hou/ect\t1\r\n",
      "also\t1\r\n",
      "am\t3\r\n",
      "an\t2\r\n",
      "and\t11\r\n",
      "any\t9\r\n",
      "appropriate\t1\r\n",
      "are\t8\r\n",
      "armstrong/corp/enron\t2\r\n",
      "as\t3\r\n",
      "ask\t1\r\n",
      "asking\t1\r\n",
      "at\t2\r\n",
      "attached\t1\r\n",
      "attend\t2\r\n",
      "attendance\t1\r\n",
      "attending\t1\r\n",
      "audience\t2\r\n",
      "available\t2\r\n",
      "back\t2\r\n",
      "be\t6\r\n",
      "being\t1\r\n",
      "below\t3\r\n",
      "benefit\t1\r\n",
      "brad\t1\r\n",
      "buck/hou/ect\t1\r\n",
      "by\t2\r\n",
      "call\t1\r\n",
      "carrera/hou/ect\t1\r\n",
      "cc:\t1\r\n",
      "challenges\t1\r\n",
      "charge\t1\r\n",
      "chosen\t1\r\n",
      "christine\t1\r\n",
      "christmas\t1\r\n",
      "cindy\t1\r\n",
      "classrom\t1\r\n",
      "client\t1\r\n",
      "clients\t1\r\n",
      "coaching\t1\r\n",
      "communicating\t2\r\n",
      "completing\t1\r\n",
      "conduct\t1\r\n",
      "conn/corp/enron\t1\r\n",
      "contact\t1\r\n",
      "cost\t1\r\n",
      "courses\t1\r\n",
      "cross-section\t1\r\n",
      "currently\t1\r\n",
      "curriculum\t4\r\n",
      "curriculum!\t1\r\n",
      "date\t1\r\n",
      "david\t1\r\n",
      "delegating\t1\r\n",
      "depending\t1\r\n",
      "description\t1\r\n",
      "design\t1\r\n",
      "designed\t1\r\n",
      "development\t3\r\n",
      "directing\t1\r\n",
      "discussion\t1\r\n",
      "ect\t17\r\n",
      "effectively\t2\r\n",
      "employee\t2\r\n",
      "ena\t2\r\n",
      "energy\t1\r\n",
      "enron\t3\r\n",
      "enron_development\t1\r\n",
      "eops\t1\r\n",
      "evaluate\t2\r\n",
      "even\t1\r\n",
      "exception\t1\r\n",
      "excited\t1\r\n",
      "experience\t1\r\n",
      "facilitators\t1\r\n",
      "farm\t1\r\n",
      "feb\t3\r\n",
      "february\t3\r\n",
      "find\t1\r\n",
      "fine-tuning\t1\r\n",
      "focus\t1\r\n",
      "following\t1\r\n",
      "for\t9\r\n",
      "fran\t1\r\n",
      "from\t2\r\n",
      "from:\t1\r\n",
      "full\t1\r\n",
      "further\t1\r\n",
      "gary\t1\r\n",
      "get\t1\r\n",
      "good\t1\r\n",
      "gracie\t2\r\n",
      "great\t1\r\n",
      "group\t2\r\n",
      "groups\t1\r\n",
      "half-day\t1\r\n",
      "have\t4\r\n",
      "held\t1\r\n",
      "help\t1\r\n",
      "helpful\t1\r\n",
      "hope\t1\r\n",
      "hope/hou/ect\t1\r\n",
      "i\t1\r\n",
      "if\t3\r\n",
      "in\t6\r\n",
      "include\t1\r\n",
      "information\t2\r\n",
      "invite\t1\r\n",
      "is\t4\r\n",
      "it\t3\r\n",
      "jane\t1\r\n",
      "janice\t1\r\n",
      "jones/corp/enron\t1\r\n",
      "julie\t2\r\n",
      "just\t1\r\n",
      "kathryn\t1\r\n",
      "kim\t1\r\n",
      "kimberly\t1\r\n",
      "l\t1\r\n",
      "later\t1\r\n",
      "leadership\t7\r\n",
      "learn\t1\r\n",
      "learning\t2\r\n",
      "less\t1\r\n",
      "listed\t3\r\n",
      "lunch\t1\r\n",
      "mary\t1\r\n",
      "materials\t2\r\n",
      "may\t1\r\n",
      "mayes/hou/ect\t1\r\n",
      "mclean/hou/ect\t1\r\n",
      "mcsherry/hou/ect\t1\r\n",
      "me\t2\r\n",
      "meeting\t1\r\n",
      "melodick/hou/ect\t1\r\n",
      "minimum\t1\r\n",
      "module\t1\r\n",
      "modules\t3\r\n",
      "months\t1\r\n",
      "more\t1\r\n",
      "motivating\t1\r\n",
      "names\t1\r\n",
      "need\t1\r\n",
      "news\t1\r\n",
      "no\t1\r\n",
      "norma\t1\r\n",
      "of\t9\r\n",
      "on\t6\r\n",
      "one\t1\r\n",
      "only\t1\r\n",
      "open\t1\r\n",
      "operations\t1\r\n",
      "options\t1\r\n",
      "or\t1\r\n",
      "order\t1\r\n",
      "other\t1\r\n",
      "our\t2\r\n",
      "overgaard/pdx/ect\t1\r\n",
      "oxley/hou/ect\t1\r\n",
      "participate\t1\r\n",
      "per\t1\r\n",
      "performance\t2\r\n",
      "philip\t1\r\n",
      "pick\t1\r\n",
      "pictures\t1\r\n",
      "pilot\t6\r\n",
      "please\t3\r\n",
      "pm\t4\r\n",
      "pm---------------------------\t1\r\n",
      "portion\t1\r\n",
      "presas\t1\r\n",
      "presas/hou/ect\t1\r\n",
      "present\t1\r\n",
      "primary\t1\r\n",
      "products\t1\r\n",
      "programs\t1\r\n",
      "purpose\t1\r\n",
      "questions\t2\r\n",
      "rankings\t1\r\n",
      "re:\t1\r\n",
      "ready\t2\r\n",
      "really\t1\r\n",
      "receive\t2\r\n",
      "regarding\t1\r\n",
      "respond\t1\r\n",
      "results\t1\r\n",
      "riedel/hou/ect\t1\r\n",
      "rizzi/hou/ect\t1\r\n",
      "robert\t1\r\n",
      "room\t1\r\n",
      "runkel\t1\r\n",
      "s\t1\r\n",
      "sally:\t1\r\n",
      "selection\t2\r\n",
      "sessions\t2\r\n",
      "setting\t1\r\n",
      "several\t1\r\n",
      "shall\t1\r\n",
      "sheila\t1\r\n",
      "shenkman/enron_development\t1\r\n",
      "sign\t1\r\n",
      "six\t1\r\n",
      "skinner/hou/ect\t1\r\n",
      "so\t1\r\n",
      "southwest\t1\r\n",
      "start\t1\r\n",
      "styles\t1\r\n",
      "subject:\t1\r\n",
      "supervisor\t3\r\n",
      "supervisor\"\t1\r\n",
      "supervisors\t5\r\n",
      "susan\t2\r\n",
      "target\t1\r\n",
      "team\t2\r\n",
      "than\t2\r\n",
      "thank\t2\r\n",
      "that\t3\r\n",
      "the\t21\r\n",
      "their\t2\r\n",
      "there\t1\r\n",
      "this\t3\r\n",
      "thoroughly\t1\r\n",
      "through\t1\r\n",
      "time\t2\r\n",
      "times\t1\r\n",
      "timing\t1\r\n",
      "to\t12\r\n",
      "to:\t1\r\n",
      "today\t1\r\n",
      "tree\t1\r\n",
      "two\t1\r\n",
      "up\t3\r\n",
      "update\t2\r\n",
      "valeria\t1\r\n",
      "valuable\t1\r\n",
      "vendor\t2\r\n",
      "vendors\t1\r\n",
      "villarreal/hou/ect\t1\r\n",
      "walton/hou/ect\t1\r\n",
      "we\t6\r\n",
      "we've\t1\r\n",
      "week\t1\r\n",
      "what\t1\r\n",
      "when\t1\r\n",
      "will\t8\r\n",
      "wilson\t2\r\n",
      "with\t3\r\n",
      "working\t1\r\n",
      "would\t2\r\n",
      "x\t1\r\n",
      "you\t6\r\n",
      "your\t6\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 enronemail_1h.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:03:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 15:03:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Upload input file to HDFS\n",
    "!hdfs dfs -rm -f enronemail_1h.txt\n",
    "!hdfs dfs -put enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:04:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `wordCount': No such file or directory\n",
      "16/01/23 15:04:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Run the hadoop streaming command\n",
    "!hdfs dfs -rm -r wordCount\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:13:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 15:13:36 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "\n",
      "Occurrence count of 'assistance':\n",
      "assistance\t10\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!rm -f w2.2.result\n",
    "!hdfs dfs -get wordCount/part-00000 w2.2.result\n",
    "!echo\n",
    "!echo \"Occurrence count of 'assistance':\"\n",
    "!grep 'assistance' w2.2.result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.2.1  \n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    parts = line.split('\\t')\n",
    "    \n",
    "    # Output is: count, and then word\n",
    "    print \"%s\\t%s\" % (parts[1], parts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "count = 0\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    print line\n",
    "    \n",
    "    # Display only the top 10 words\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240\tthe\r\n",
      "914\tto\r\n",
      "659\tand\r\n",
      "556\tof\r\n",
      "527\ta\r\n",
      "415\tin\r\n",
      "407\tyou\r\n",
      "389\tyour\r\n",
      "369\tfor\r\n",
      "361\t@\r\n"
     ]
    }
   ],
   "source": [
    "!cat w2.2.result | python mapper.py | sort -g -r | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run it in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:32:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted top10\n",
      "16/01/23 15:32:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "# Please note that we use the output from HW2.2 as input.\n",
    "!hdfs dfs -rm -r top10\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options=\"-nr\" -mapper mapper.py -reducer reducer.py -input wordCount -output top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 occurring words:\n",
      "16/01/23 15:32:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "the\n",
      "to\n",
      "and\n",
      "of\n",
      "a\n",
      "in\n",
      "you\n",
      "your\n",
      "for\n",
      "@\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!echo 'Top 10 occurring words:'\n",
    "!hdfs dfs -cat top10/part-00000 | cut -d$'\\t' -f 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "> the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. **Count up how many times you need to process a zero probabilty for each class and report.** \n",
    "\n",
    "   Report the performance of your learnt classifier in terms of **misclassifcation error rate** of your multinomial Naive Bayes Classifier.  \n",
    "   \n",
    "   Plot a histogram of the log posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. \n",
    "   \n",
    "   Summarize what you see. \n",
    "\n",
    "```\n",
    "Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# Regex for splitting the words.  Delimiters are: <spaces> , .\n",
    "regex = re.compile(r\"[\\s,\\.]+\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    parts = re.split(\"\\t\", line) # parse line into separate fields\n",
    "    \n",
    "    msgId = parts[0].strip()\n",
    "    isSpam = parts[1].strip()\n",
    "\n",
    "    # Extract the text parts\n",
    "    subject = parts[2].strip()\n",
    "    if subject == \"NA\":\n",
    "        subject = \"\"\n",
    "        \n",
    "    body = parts[3].strip()\n",
    "    if body == \"NA\":\n",
    "        body = \"\"\n",
    "\n",
    "    text = subject + \" \" + body\n",
    "    \n",
    "    # Create list of words\n",
    "    words = filter(None, regex.split(text))\n",
    "    \n",
    "    for word in words:\n",
    "        # Send one row for every word instance to the reducer.\n",
    "        print msgId + '\\t' + isSpam + '\\t' + word + '\\t' + '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys\n",
    "from math import log\n",
    " \n",
    "per_word_counts = {} # ham and spam counts about each word\n",
    "emails = {} # data about each email \n",
    "email_counts = [0,0] # number of emails in each class\n",
    "class_word_counts = [0,0] # number of total (not unique) words in each class\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    \n",
    "    #parse the incoming line\n",
    "    parts=line.split(\"\\t\")\n",
    "    email=parts[0]\n",
    "    spam=int(parts[1])\n",
    "    word=parts[2]\n",
    "    \n",
    "    # initialize storage for word/email data\n",
    "    if word not in per_word_counts.keys():\n",
    "        per_word_counts[word] = [0,0] # ham count and spam count\n",
    "        \n",
    "    if email not in emails.keys():\n",
    "        emails[email] = {'spam':spam, 'words':[]}\n",
    "        email_counts[spam] += 1\n",
    "\n",
    "    # update per-class word count for this word\n",
    "    per_word_counts[word][spam] += 1\n",
    "    class_word_counts[spam] += 1\n",
    "\n",
    "    # update email data \n",
    "    emails[email]['words'].append(word)\n",
    "\n",
    "\n",
    "# train the model\n",
    "\n",
    "priors = {} # priors for the two classes\n",
    "condProbs = {} # conditional probabilties for each word\n",
    "vocab_count = len(per_word_counts) # number of unique words in the total vocabulary\n",
    "email_count = len(emails)\n",
    "\n",
    "for c in [0,1]:\n",
    "    priors[c] = email_counts[c] / email_count\n",
    "\n",
    "zeros = [0,0] # Remember the number of zero cond. prob. encountered in each class\n",
    "\n",
    "# Go through each class, and compute the conditional probability of each word in the vocab\n",
    "for word, counts in per_word_counts.iteritems():\n",
    "    condProbs[word] = [0,0]\n",
    "    for c in [0,1]:\n",
    "        # Calcuate the conditional probability of the word in this class without smoothing\n",
    "        condProbs[word][c] = counts[c] / class_word_counts[c]\n",
    "        \n",
    "        if counts[c] == 0:\n",
    "            zeros[c] += 1\n",
    "\n",
    "            \n",
    "# Now make the predictions\n",
    "# And for HW2.3, for each email we also have to calculate:\n",
    "# Pr(y|X) = Pr(y) Pr(x1|y) Pr(x2|y) ... P(xn|y) / P(X)\n",
    "\n",
    "probEmails = { 0:[], 1:[] } # For each class, the list of Pr(class|Doc), one for each email\n",
    "misclassifiedCount = 0\n",
    "for msgId, email in emails.iteritems():\n",
    "\n",
    "    # Compute the email's score for each class\n",
    "    \n",
    "    # Initialize the score of each class\n",
    "    scores = [log(priors[0]), log(priors[1])]\n",
    "    \n",
    "    hitZero = None\n",
    "        \n",
    "    # For each word contained in this email, add up its log(condProb)\n",
    "    for word in email['words']:\n",
    "        if condProbs[word][0] == 0 or condProbs[word][1] == 0:\n",
    "            continue\n",
    "\n",
    "        for c in [0,1]:\n",
    "            if condProbs[word][c] == 0:\n",
    "                # Remember if we've met a zero cond. prob. for a class\n",
    "                hitZero = c\n",
    "            else:\n",
    "                scores[c] += log(condProbs[word][c])\n",
    "                \n",
    "    # if a zero cond. prob. is met, we treat the prob. of that class to be zero.\n",
    "    if hitZero is not None:\n",
    "        scores[hitZero] = 0\n",
    "    \n",
    "    # The predicted class is the one which has the higher score\n",
    "    predicted = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    if predicted != email['spam']:\n",
    "        misclassifiedCount += 1\n",
    "        \n",
    "    # Calculate Pr(y|X) = Pr(y) Pr(x1|y) Pr(x2|y) ... P(xn|y) / P(X)\n",
    "    p_x = 1 / email_count\n",
    "    for c in [0,1]:\n",
    "        p = priors[c]\n",
    "        for word in email['words']:\n",
    "            p *= condProbs[word][c]\n",
    "            \n",
    "        p = p / p_x\n",
    "        probEmails[c].append(p)\n",
    "            \n",
    "# Report the result\n",
    "print \"Misclassifcation error rate:\", misclassifiedCount / len(emails)\n",
    "print \"Number of zero cond. prob. processed in ham emails:\", zeros[0]\n",
    "print \"Number of zero cond. prob. processed in spam emails:\", zeros[1]\n",
    "\n",
    "print\n",
    "for c in [0,1]:\n",
    "    for p in probEmails[c]:\n",
    "        print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifcation error rate: 0.11\r\n",
      "Number of zero cond. prob. processed in ham emails: 3246\r\n",
      "Number of zero cond. prob. processed in spam emails: 2140\r\n",
      "\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "1.34818836861e-218\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "4.6908531808e-170\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "4.35867370769e-58\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "6.81526599242e-219\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "1.07982602754e-220\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "2.03038646833e-133\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "2.46051189364e-14\r\n",
      "0.0\r\n",
      "3.14137513038e-196\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "4.9575535035e-317\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "5.95973991751e-308\r\n",
      "2.07576968428e-182\r\n",
      "1.73281866001e-90\r\n",
      "0.0\r\n",
      "3.48977389561e-34\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "8.96205808226e-205\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "4.72404982668e-198\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "5.34713067354e-136\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "8.23199486422e-290\r\n",
      "2.88450982869e-286\r\n",
      "0.0\r\n",
      "1.43288633806e-11\r\n",
      "0.0\r\n",
      "4.63754484541e-243\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "7.07252148853e-20\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "3.88614552047e-91\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "3.75822058409e-293\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "1.76326279806e-77\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "5.72877959404e-308\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "3.66723482714e-82\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "1.36176181521e-176\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "2.46463059226e-64\r\n",
      "0.0\r\n",
      "0.0\r\n",
      "1.4369401539e-267\r\n",
      "7.37115766104e-237\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 enronemail_1h.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:06:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted enronemail_1h.txt\n",
      "16/01/26 18:06:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r enronemail_1h.txt\n",
    "!hdfs dfs -put enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:06:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `output_hw2.3': No such file or directory\n",
      "16/01/26 18:06:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r output_hw2.3\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output output_hw2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 18:06:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/26 18:06:55 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "# Get the results\n",
    "!hdfs dfs -get output_hw2.3/part-00000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifcation error rate: 0.11\t\r\n",
      "Number of zero cond. prob. processed in ham emails: 3246\t\r\n",
      "Number of zero cond. prob. processed in spam emails: 2140\t\r\n",
      "\t\r\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!head -n 4 part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEaCAYAAADg2nttAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d3f35d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEaCAYAAAAVJPDdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzCzjYDAzs8z/B8nCxr/9zbrCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d05fed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the histograms\n",
    "d = np.loadtxt('part-00000', skiprows = 4)\n",
    "\n",
    "hamData = d[0:100]\n",
    "spamData = d[100:]\n",
    "\n",
    "# Plot the Ham data\n",
    "plt.hist(hamData, color=\"blue\")\n",
    "plt.xlabel('Pr(Ham|Doc)')\n",
    "plt.ylabel('Count')\n",
    "plt.title(r'Histogram of Pr(Ham|Doc)')\n",
    "plt.show()\n",
    "\n",
    "# Plot the Spam data\n",
    "plt.hist(spamData, color=\"red\")\n",
    "plt.xlabel('Pr(Spam|Doc)')\n",
    "plt.ylabel('Count')\n",
    "plt.title(r'Histogram of Pr(Spam|Doc)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.4 \n",
    "Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer (Note: will use the same mapper in HW2.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys\n",
    "from math import log\n",
    " \n",
    "per_word_counts = {} # ham and spam counts about each word\n",
    "emails = {} # data about each email \n",
    "email_counts = [0,0] # number of emails in each class\n",
    "class_word_counts = [0,0] # number of total (not unique) words in each class\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    \n",
    "    #parse the incoming line\n",
    "    parts=line.split(\"\\t\")\n",
    "    email=parts[0]\n",
    "    spam=int(parts[1])\n",
    "    word=parts[2]\n",
    "    \n",
    "    # initialize storage for word/email data\n",
    "    if word not in per_word_counts.keys():\n",
    "        per_word_counts[word] = [0,0] # ham count and spam count\n",
    "        \n",
    "    if email not in emails.keys():\n",
    "        emails[email] = {'spam':spam, 'words':[]}\n",
    "        email_counts[spam] += 1\n",
    "\n",
    "    # update per-class word count for this word\n",
    "    per_word_counts[word][spam] += 1\n",
    "    class_word_counts[spam] += 1\n",
    "\n",
    "    # update email data \n",
    "    emails[email]['words'].append(word)\n",
    "\n",
    "\n",
    "# train the model\n",
    "\n",
    "priors = {} # priors for the two classes\n",
    "condProbs = {} # conditional probabilties for each word\n",
    "vocab_count = len(per_word_counts) # number of unique words in the total vocabulary\n",
    "\n",
    "for c in [0,1]:\n",
    "    priors[c] = email_counts[c] / len(emails)\n",
    "\n",
    "# Go through each class, and compute the conditional probability of each word in the vocab\n",
    "for word, counts in per_word_counts.iteritems():\n",
    "    condProbs[word] = [0,0]\n",
    "    for c in [0,1]:\n",
    "        # Calcuate the conditional probability of the word in this class with\n",
    "        # Laplace plus-one smoothing\n",
    "        condProbs[word][c] = (counts[c] + 1) / (class_word_counts[c] + vocab_count)\n",
    "\n",
    "# Now make the predictions\n",
    "\n",
    "misclassifiedCount = 0\n",
    "for msgId, email in emails.iteritems():\n",
    "\n",
    "    # Initialize the score of each class\n",
    "    scores = [log(priors[0]), log(priors[1])]\n",
    "        \n",
    "    # For each word contained in this email, add up its log(condProb)\n",
    "    # Because we're not using smoothing, we have skip any word where its\n",
    "    # cond. prob. is zero is either one of the classes.\n",
    "    for word in email['words']:\n",
    "        for c in [0,1]:\n",
    "            scores[c] += log(condProbs[word][c])\n",
    "    \n",
    "    # The predicted class is the one which has the higher score\n",
    "    predicted = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    if predicted != email['spam']:\n",
    "        misclassifiedCount += 1\n",
    "\n",
    "# Report the result\n",
    "print \"Misclassifcation error rate:\", misclassifiedCount / len(emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifcation error rate: 0.01\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 enronemail_1h.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 23:33:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted enronemail_1h.txt\n",
      "16/01/23 23:33:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r enronemail_1h.txt\n",
    "!hdfs dfs -put enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 00:00:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted output_hw2.4\n",
      "16/01/24 00:00:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r output_hw2.4\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output output_hw2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 00:00:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Misclassifcation error rate: 0.01\t\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!hdfs dfs -cat output_hw2.4/part-00000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.**\n",
    "\n",
    "After adding smoothing, the misclassification rate has dropped from 11% to 1%.  It is because in HW2.3, if the conditional probability of a word is zero, the code will ignore that word during prediction.  So it means we have less data, as there were 5386 such cases.  With Laplace plus-one smoothing we have retained those cases and thus the \"weight\" of those words are used during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.5. \n",
    "Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer (Note: we will reuse the mapper in HW2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys\n",
    "from math import log\n",
    " \n",
    "per_word_counts = {} # ham and spam counts about each word\n",
    "emails = {} # data about each email \n",
    "email_counts = [0,0] # number of emails in each class\n",
    "class_word_counts = [0,0] # number of total (not unique) words in each class\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    \n",
    "    #parse the incoming line\n",
    "    parts=line.split(\"\\t\")\n",
    "    email=parts[0]\n",
    "    spam=int(parts[1])\n",
    "    word=parts[2]\n",
    "    \n",
    "    # initialize storage for word/email data\n",
    "    if word not in per_word_counts.keys():\n",
    "        per_word_counts[word] = [0,0] # ham count and spam count\n",
    "        \n",
    "    if email not in emails.keys():\n",
    "        emails[email] = {'spam':spam, 'words':[]}\n",
    "        email_counts[spam] += 1\n",
    "\n",
    "    # update per-class word count for this word\n",
    "    per_word_counts[word][spam] += 1\n",
    "    class_word_counts[spam] += 1\n",
    "\n",
    "    # update email data \n",
    "    emails[email]['words'].append(word)\n",
    "\n",
    "\n",
    "# Remove all words with a frequency of less than three (3) in the training set.\n",
    "lowFrequencyWords = []\n",
    "for word, counts in per_word_counts.iteritems():\n",
    "    if sum(counts) < 3:\n",
    "        lowFrequencyWords.append(word)\n",
    "\n",
    "for word in lowFrequencyWords:\n",
    "    del per_word_counts[word]\n",
    "\n",
    "    \n",
    "# train the model\n",
    "\n",
    "priors = {} # priors for the two classes\n",
    "condProbs = {} # conditional probabilties for each word\n",
    "vocab_count = len(per_word_counts) # number of unique words in the total vocabulary\n",
    "\n",
    "for c in [0,1]:\n",
    "    priors[c] = email_counts[c] / len(emails)\n",
    "\n",
    "# Go through each class, and compute the conditional probability of each word in the vocab\n",
    "for word, counts in per_word_counts.iteritems():\n",
    "    condProbs[word] = [0,0]\n",
    "    for c in [0,1]:\n",
    "        # Calcuate the conditional probability of the word in this class with\n",
    "        # Laplace plus-one smoothing\n",
    "        condProbs[word][c] = (counts[c] + 1) / (class_word_counts[c] + vocab_count)\n",
    "\n",
    "\n",
    "# Now make the predictions\n",
    "\n",
    "misclassifiedCount = 0\n",
    "for msgId, email in emails.iteritems():\n",
    "\n",
    "    # Initialize the score of each class\n",
    "    scores = [log(priors[0]), log(priors[1])]\n",
    "        \n",
    "    # For each word contained in this email, add up its log(condProb)\n",
    "    # Because we're not using smoothing, we have skip any word where its\n",
    "    # cond. prob. is zero is either one of the classes.\n",
    "    for word in email['words']:\n",
    "        if word not in lowFrequencyWords:\n",
    "            for c in [0,1]:\n",
    "                scores[c] += log(condProbs[word][c])\n",
    "    \n",
    "    # The predicted class is the one which has the higher score\n",
    "    predicted = 0 if scores[0] > scores[1] else 1\n",
    "    \n",
    "    if predicted != email['spam']:\n",
    "        misclassifiedCount += 1\n",
    "\n",
    "# Report the result\n",
    "print \"Misclassifcation error rate:\", misclassifiedCount / len(emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifcation error rate: 0.01\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 enronemail_1h.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running in hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 00:36:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted output_hw2.5\n",
      "16/01/24 00:36:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Hadoop streaming command\n",
    "!hdfs dfs -rm -r output_hw2.5\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -mapper mapper.py -reducer reducer.py -input enronemail_1h.txt -output output_hw2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 00:36:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Misclassifcation error rate: 0.01\t\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "!hdfs dfs -cat output_hw2.5/part-00000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset?**\n",
    "\n",
    "Now all low frequency words are filtered out and have no impact on the prediction.  In theory that could reduce the misclassification error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW2.6 \n",
    "\n",
    "Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "\n",
    "It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "— Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sklearn MultinomialNB, error rate = 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Own hadoop code</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sklearn MultinomialNB</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "emails = pd.read_csv('enronemail_1h.txt', sep='\\t', header=None, \n",
    "                    names=['id', 'spam', 'subject', 'body'],\n",
    "                    dtype = {'id':np.object, 'spam':int, 'subject':str, 'body':str})\n",
    "\n",
    "emails = emails.fillna('') # replace missing value with empty string\n",
    "emails['text'] = emails.subject + emails.body\n",
    "\n",
    "# Use CountVectorizer to extract tokens and extract the count of words in each email\n",
    "# Delimiters are: <spaces> , .\n",
    "vec = CountVectorizer(token_pattern=r\"([^\\s.,]+)\")\n",
    "\n",
    "X = vec.fit_transform(emails.text.tolist())\n",
    "y = np.array(emails.spam.tolist())\n",
    "\n",
    "# Use MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Get the prediction rate using the same data set\n",
    "sklearnErrRate = 1 - clf.score(X,y)\n",
    "print \"With sklearn MultinomialNB, error rate =\", sklearnErrRate\n",
    "\n",
    "# Display the two results as table\n",
    "result = pd.DataFrame(np.array([0.01, sklearnErrRate]), columns=['Error rate'], \n",
    "                      index=['Own hadoop code', 'sklearn MultinomialNB'])\n",
    "\n",
    "display(HTML(result.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn.** \n",
    "\n",
    "I could not see any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "16/01/26 18:31:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/26 18:31:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
