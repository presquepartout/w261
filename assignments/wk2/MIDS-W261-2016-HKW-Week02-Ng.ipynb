{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Patrick Ng  \n",
    "Email: patng@ischool.berkeley.edu  \n",
    "Class: W261-2  \n",
    "Week: 01  \n",
    "Date of submission: Jan 26, 2016\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2.1. Sort in Hadoop MapReduce\n",
    "Given as input: Records of the form < integer, “NA” >, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form < integer, “NA” > in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "Write code to generate N  random records of the form < integer, “NA” >. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting genrand.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile genrand.py\n",
    "#!/usr/bin/python\n",
    "import random\n",
    "import sys\n",
    "\n",
    "nums = 10000\n",
    "if len(sys.argv) > 1:\n",
    "    nums = int(sys.argv[1])\n",
    "\n",
    "random.seed(0)\n",
    "for i in range(nums):\n",
    "    print '< %d, \"NA\" >' % random.randint(-1000000, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x genrand.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# The regex which captures the integer from a line in the format < integer, \"NA\" >\n",
    "regex = re.compile(ur'\\<\\s*(-?\\d+)\\s*,\\s*\\\"NA\\\"\\s*\\>')\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    # Get the integer from the line\n",
    "    result = regex.findall(line)\n",
    "    if len(result) == 0:\n",
    "        # Cannot find any integer. Could be a corrupted input line.  Skip it.\n",
    "        continue\n",
    "    \n",
    "    # print the integer as the key of the output.  Absence of value means there is no value.\n",
    "    print result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    print '<%s, \"NA\">' % line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<688844, \"NA\">\r\n",
      "<515909, \"NA\">\r\n",
      "<-158857, \"NA\">\r\n",
      "<-482167, \"NA\">\r\n",
      "<22549, \"NA\">\r\n",
      "<-190132, \"NA\">\r\n",
      "<567597, \"NA\">\r\n",
      "<-393375, \"NA\">\r\n",
      "<-46806, \"NA\">\r\n",
      "<166764, \"NA\">\r\n",
      "<816226, \"NA\">\r\n",
      "<9374, \"NA\">\r\n",
      "<-436325, \"NA\">\r\n",
      "<511609, \"NA\">\r\n",
      "<236738, \"NA\">\r\n",
      "<-498988, \"NA\">\r\n",
      "<819493, \"NA\">\r\n",
      "<965571, \"NA\">\r\n",
      "<620435, \"NA\">\r\n",
      "<804332, \"NA\">\r\n"
     ]
    }
   ],
   "source": [
    "!python genrand.py 20 | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run it in hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-patrickng-resourcemanager-Patricks-MacBook-Pro.local.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-patrickng-nodemanager-Patricks-MacBook-Pro.local.out\n",
      "16/01/23 12:32:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-namenode-Patricks-MacBook-Pro.local.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-datanode-Patricks-MacBook-Pro.local.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-patrickng-secondarynamenode-Patricks-MacBook-Pro.local.out\n",
      "16/01/23 12:33:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random numbers, each in the range [-1000000, 1000000].\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"Generating random numbers, each in the range [-1000000, 1000000].\"\n",
    "!rm -f randomNums.txt\n",
    "!./genrand.py 10000 >> randomNums.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload randomNums.txt to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:20:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted randomNums.txt\n",
      "16/01/23 13:21:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -f randomNums.txt\n",
    "!hdfs dfs -put randomNums.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop streaming command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:21:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted sortRandomNums\n",
      "16/01/23 13:21:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r sortRandomNums\n",
    "!hadoop jar $HADOOP_INSTALL/share/hadoop/tools/lib/hadoop-*streaming*.jar -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -D mapred.text.key.comparator.options=\"-nr\" -mapper mapper.py -reducer reducer.py -input randomNums.txt -output sortRandomNums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:21:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 13:21:17 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "\n",
      "10 biggest numbers:\n",
      "<999806, \"NA\">\t\n",
      "<999764, \"NA\">\t\n",
      "<999727, \"NA\">\t\n",
      "<999663, \"NA\">\t\n",
      "<999371, \"NA\">\t\n",
      "<998888, \"NA\">\t\n",
      "<998841, \"NA\">\t\n",
      "<998388, \"NA\">\t\n",
      "<997707, \"NA\">\t\n",
      "<997613, \"NA\">\t\n",
      "\n",
      "10 smallest numbers:\n",
      "<-997715, \"NA\">\t\n",
      "<-997902, \"NA\">\t\n",
      "<-997975, \"NA\">\t\n",
      "<-998040, \"NA\">\t\n",
      "<-998770, \"NA\">\t\n",
      "<-998808, \"NA\">\t\n",
      "<-999519, \"NA\">\t\n",
      "<-999672, \"NA\">\t\n",
      "<-999732, \"NA\">\t\n",
      "<-999954, \"NA\">\t\n"
     ]
    }
   ],
   "source": [
    "!rm -f w2.1.result\n",
    "!hdfs dfs -get sortRandomNums/part-00000 w2.1.result\n",
    "!echo\n",
    "!echo \"10 biggest numbers:\"\n",
    "!head -n 10 w2.1.result\n",
    "!echo\n",
    "!echo \"10 smallest numbers:\"\n",
    "!tail -n 10 w2.1.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/23 12:23:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/23 12:23:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
