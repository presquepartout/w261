{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write some words to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo foo foo quux labs foo bar quux > WordCount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MrJob class for wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mr_wc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mr_wc.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRJobStep\n",
    "import re\n",
    " \n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    " \n",
    "class MRWordFreqCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is straightforward. Mapper outputs (word, 1) key value pairs, and then conbiner combines the sum locally. At last, Reducer sums them up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code in command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"bar\"\t1\n",
      "\"foo\"\t3\n",
      "\"labs\"\t1\n",
      "\"quux\"\t2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using configs in C:\\Anaconda\\mrjob.conf\n",
      "creating tmp directory c:\\users\\liang.dai\\appdata\\local\\temp\\WordCount.liang.dai.20150524.134758.767000\n",
      "writing to step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to c:\\users\\liang.dai\\appdata\\local\\temp\\WordCount.liang.dai.20150524.134758.767000\\step-0-mapper-sorted\n",
      "> sort 'c:\\users\\liang.dai\\appdata\\local\\temp\\WordCount.liang.dai.20150524.134758.767000\\step-0-mapper_part-00000'\n",
      "writing to step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving c:\\users\\liang.dai\\appdata\\local\\temp\\WordCount.liang.dai.20150524.134758.767000\\step-0-reducer_part-00000 -> c:\\users\\liang.dai\\appdata\\local\\temp\\WordCount.liang.dai.20150524.134758.767000\\output\\part-00000\n",
      "Streaming final output from c:\\users\\liang.dai\\appdata\\local\\temp\\WordCount.liang.dai.20150524.134758.767000\\output\n",
      "removing tmp directory c:\\users\\liang.dai\\appdata\\local\\temp\\WordCount.liang.dai.20150524.134758.767000\n"
     ]
    }
   ],
   "source": [
    "!python WordCount.py WordCount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code through python driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Reminder: You cannot use the programmatic runner functionality in the same file as your job class. That is because the file with the job class is sent to Hadoop to be run. Therefore, the job file cannot attempt to start the Hadoop job, or you would be recursively creating Hadoop jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use make_runner() to run an MRJob\n",
    "1. seperate driver from mapreduce jobs\n",
    "2. now we can run it within pythonnode book \n",
    "3. In python, typically one class is in each file. Each mrjob job is a seperate class, should be in a seperate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'bar', 1)\n",
      "(u'foo', 3)\n",
      "(u'labs', 1)\n",
      "(u'quux', 2)\n"
     ]
    }
   ],
   "source": [
    "from WordCount import MRWordFreqCount\n",
    "mr_job = MRWordFreqCount(args=['WordCount.txt'])\n",
    "with mr_job.make_runner() as runner: \n",
    "    runner.run()\n",
    "    # stream_output: get access of the output \n",
    "    for line in runner.stream_output():\n",
    "        print mr_job.parse_output_line(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
